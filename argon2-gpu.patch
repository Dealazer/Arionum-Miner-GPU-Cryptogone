diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3dd95ba..f5130e0 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1,4 +1,4 @@
-cmake_minimum_required(VERSION 3.7)
+cmake_minimum_required(VERSION 3.5)
 
 project(argon2-gpu CXX)
 set(BINARY_INSTALL_DIR /usr/local/bin)
@@ -28,9 +28,17 @@ if(CUDA_FOUND)
     )
 endif()
 
+FIND_PACKAGE(OpenCL)
+INCLUDE_DIRECTORIES(${OPENCL_INCLUDE_DIR})
+if (OPENCL_FOUND)
+    message("INFO: Using OPENCL version ${OpenCL_VERSION_MAJOR}.${OpenCL_VERSION_MINOR}")
+else()
+    message("INFO: OPENCL not found")
+endif()
+
 add_subdirectory(ext/argon2)
 
-add_library(argon2-gpu-common SHARED
+add_library(argon2-gpu-common STATIC
     lib/argon2-gpu-common/argon2params.cpp
     lib/argon2-gpu-common/blake2b.cpp
 )
@@ -44,7 +52,7 @@ target_include_directories(argon2-gpu-common PRIVATE
 )
 
 if(CUDA_FOUND)
-    cuda_add_library(argon2-cuda SHARED
+    cuda_add_library(argon2-cuda STATIC
         lib/argon2-cuda/device.cpp
         lib/argon2-cuda/globalcontext.cpp
         lib/argon2-cuda/programcontext.cpp
@@ -52,7 +60,7 @@ if(CUDA_FOUND)
         lib/argon2-cuda/kernels.cu
     )
 else()
-    add_library(argon2-cuda SHARED
+    add_library(argon2-cuda STATIC
         lib/argon2-cuda/nocuda.cpp
     )
 endif()
@@ -67,7 +75,7 @@ target_include_directories(argon2-cuda INTERFACE
 )
 target_link_libraries(argon2-cuda argon2-gpu-common)
 
-add_library(argon2-opencl SHARED
+add_library(argon2-opencl STATIC
     lib/argon2-opencl/device.cpp
     lib/argon2-opencl/globalcontext.cpp
     lib/argon2-opencl/kernelloader.cpp
@@ -84,56 +92,5 @@ target_include_directories(argon2-opencl PRIVATE
     lib/argon2-opencl
 )
 target_link_libraries(argon2-opencl
-    argon2-gpu-common -lOpenCL
-)
-
-add_executable(argon2-gpu-test
-    src/argon2-gpu-test/main.cpp
-    src/argon2-gpu-test/testcase.cpp
-)
-target_include_directories(argon2-gpu-test PRIVATE src/argon2-gpu-test)
-target_link_libraries(argon2-gpu-test
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_executable(argon2-gpu-bench
-    src/argon2-gpu-bench/cpuexecutive.cpp
-    src/argon2-gpu-bench/cudaexecutive.cpp
-    src/argon2-gpu-bench/openclexecutive.cpp
-    src/argon2-gpu-bench/benchmark.cpp
-    src/argon2-gpu-bench/main.cpp
-)
-target_include_directories(argon2-gpu-bench PRIVATE src/argon2-gpu-bench)
-target_link_libraries(argon2-gpu-bench
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_test(argon2-gpu-test-opencl argon2-gpu-test -m opencl)
-add_test(argon2-gpu-test-cuda argon2-gpu-test -m cuda)
-
-install(
-    TARGETS argon2-gpu-common argon2-opencl argon2-cuda
-    DESTINATION ${LIBRARY_INSTALL_DIR}
-)
-install(FILES
-    include/argon2-gpu-common/argon2-common.h
-    include/argon2-gpu-common/argon2params.h
-    include/argon2-opencl/cl.hpp
-    include/argon2-opencl/opencl.h
-    include/argon2-opencl/device.h
-    include/argon2-opencl/globalcontext.h
-    include/argon2-opencl/programcontext.h
-    include/argon2-opencl/processingunit.h
-    include/argon2-opencl/kernelrunner.h
-    include/argon2-cuda/cudaexception.h
-    include/argon2-cuda/kernels.h
-    include/argon2-cuda/device.h
-    include/argon2-cuda/globalcontext.h
-    include/argon2-cuda/programcontext.h
-    include/argon2-cuda/processingunit.h
-    DESTINATION ${INCLUDE_INSTALL_DIR}
-)
-install(
-    TARGETS argon2-gpu-bench argon2-gpu-test
-    DESTINATION ${BINARY_INSTALL_DIR}
+    argon2-gpu-common ${OpenCL_LIBRARY}
 )
diff --git a/data/kernels/argon2_kernel.cl b/data/kernels/argon2_kernel.cl
index fa5e12a..fe48f26 100644
--- a/data/kernels/argon2_kernel.cl
+++ b/data/kernels/argon2_kernel.cl
@@ -79,7 +79,7 @@ ulong u64_shuffle(ulong v, uint thread_src, uint thread,
     buf->lo[thread] = lo;
     buf->hi[thread] = hi;
 
-    barrier(CLK_LOCAL_MEM_FENCE);
+    barrier(CLK_LOCAL_MEM_FENCE); // may not be needed ...
 
     lo = buf->lo[thread_src];
     hi = buf->hi[thread_src];
@@ -645,6 +645,54 @@ __kernel void argon2_kernel_oneshot_precompute(
         mem_curr = mem_lane;
     }
 }
+
+struct index {
+    uint refSlot;
+    uint store;
+    uint storeSlot;
+};
+
+__kernel void argon2_kernel_oneshot_precomputedIndex(
+    __local struct u64_shuffle_buf *shuffle_bufs,   // 0
+    __global struct block_g *memory,                // 1
+    __global const struct index *indexs,            // 2
+    uint nSteps,                                    // 3
+    uint blocksPerBatch)                            // 4
+{
+    uint thread = get_local_id(0) % THREADS_PER_LANE; // [0-31]
+    uint warp = 0; // get_local_id(0) / THREADS_PER_LANE;
+    uint batch_id = get_global_id(1);                 // [0-nBatches-1]
+
+    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+
+    memory += (size_t)batch_id * blocksPerBatch;
+    
+    struct block_th prev, tmp;
+    load_block(&prev, memory + 1, thread);
+
+    for (uint i = 2; i < nSteps; ++i) {
+        uint ref_index = indexs->refSlot;
+        __global struct block_g *mem_ref = memory + ref_index;
+
+        load_block_xor(&prev, mem_ref, thread);
+        move_block(&tmp, &prev);
+
+        shuffle_block(&prev, thread, shuffle_buf);
+
+        xor_block(&prev, &tmp);
+
+        uint store = indexs->store;
+        if (store != 0) {
+            __global struct block_g *mem_curr = memory + indexs->storeSlot;
+            store_block(mem_curr, &prev, thread);
+        }
+
+        indexs++;
+    }
+
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
+}
+
 #endif /* ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID */
 
 void argon2_step(
diff --git a/include/argon2-cuda/cudaexception.h b/include/argon2-cuda/cudaexception.h
index ebc8460..92d44b6 100644
--- a/include/argon2-cuda/cudaexception.h
+++ b/include/argon2-cuda/cudaexception.h
@@ -6,6 +6,8 @@
 #endif
 
 #include <exception>
+#include <stdio.h>
+#include <iostream>
 
 namespace argon2 {
 namespace cuda {
@@ -27,6 +29,7 @@ public:
     static void check(cudaError_t res)
     {
         if (res != cudaSuccess) {
+            printf("CUDA exception => |%s|\n", cudaGetErrorString(res));
             throw CudaException(res);
         }
     }
diff --git a/include/argon2-cuda/kernels.h b/include/argon2-cuda/kernels.h
index 16418b4..203c473 100644
--- a/include/argon2-cuda/kernels.h
+++ b/include/argon2-cuda/kernels.h
@@ -5,12 +5,15 @@
 
 #include <cuda_runtime.h>
 #include <cstdint>
+#include "../../include/argon2-gpu-common/argon2-common.h"
 
 /* workaround weird CMake/CUDA bug: */
 #ifdef argon2
 #undef argon2
 #endif
 
+#define REUSE_BUFFERS (1)
+
 namespace argon2 {
 namespace cuda {
 
@@ -21,14 +24,26 @@ private:
     std::uint32_t passes, lanes, segmentBlocks;
     std::uint32_t batchSize;
     bool bySegment;
-    bool precompute;
+    
+    t_optParams optParams;
 
     cudaEvent_t start, end;
     cudaStream_t stream;
+
+#if REUSE_BUFFERS
+    size_t blocksBufferSize;
+    size_t refsBufferSize;
+    const uint32_t* lastPrecomputeAddress;
+#endif
+
     void *memory;
     void *refs;
-
     void precomputeRefs();
+    void initializeBuffers();
+    void freeBuffers();
+    
+    uint32_t getBlockCount() const;
+    size_t totalRefsSize() const;
 
     void runKernelSegment(std::uint32_t lanesPerBlock,
                           std::uint32_t jobsPerBlock,
@@ -37,25 +52,36 @@ private:
                           std::uint32_t jobsPerBlock);
 
 public:
+    KernelRunner(std::uint32_t type, std::uint32_t version,
+                 std::uint32_t passes, std::uint32_t lanes,
+                 std::uint32_t segmentBlocks, std::uint32_t batchSize,
+                 bool bySegment, t_optParams optParams);
+
+    ~KernelRunner();
+
     std::uint32_t getMinLanesPerBlock() const { return bySegment ? 1 : lanes; }
     std::uint32_t getMaxLanesPerBlock() const { return lanes; }
-
     std::uint32_t getMinJobsPerBlock() const { return 1; }
     std::uint32_t getMaxJobsPerBlock() const { return batchSize; }
-
     std::uint32_t getBatchSize() const { return batchSize; }
 
-    KernelRunner(std::uint32_t type, std::uint32_t version,
-                 std::uint32_t passes, std::uint32_t lanes,
-                 std::uint32_t segmentBlocks, std::uint32_t batchSize,
-                 bool bySegment, bool precompute);
-    ~KernelRunner();
-
     void writeInputMemory(std::uint32_t jobId, const void *buffer);
     void readOutputMemory(std::uint32_t jobId, void *buffer);
+    void syncStream();
+    bool streamOperationsComplete();
 
     void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
     float finish();
+
+    void reconfigureArgon(
+        std::uint32_t passes,
+        std::uint32_t lanes,
+        std::uint32_t segmentBlocks,
+        std::uint32_t newBatchSize,
+        const t_optParams &optParams);
+
+    size_t getMemoryUsage() const;
+    size_t getMemoryUsedPerBatch() const;
 };
 
 } // cuda
diff --git a/include/argon2-cuda/processingunit.h b/include/argon2-cuda/processingunit.h
index 2109154..c196f59 100644
--- a/include/argon2-cuda/processingunit.h
+++ b/include/argon2-cuda/processingunit.h
@@ -22,20 +22,31 @@ private:
     KernelRunner runner;
     std::uint32_t bestLanesPerBlock;
     std::uint32_t bestJobsPerBlock;
+    std::vector<uint8_t*> setPasswordBuffers;
 
 public:
     std::size_t getBatchSize() const { return runner.getBatchSize(); }
 
     ProcessingUnit(
             const ProgramContext *programContext, const Argon2Params *params,
-            const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false);
+            const Device *device, std::size_t batchSize, bool bySegment,
+            const t_optParams &optPrms);
 
     void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
+
+    void fetchResultAsync(std::size_t index, void *dest);
+    void syncStream();
+    bool streamOperationsComplete();
 
     void beginProcessing();
     void endProcessing();
+    void reconfigureArgon(
+        const Argon2Params *newParams,
+        std::uint32_t batchSize,
+        const t_optParams &optParams);
+
+    size_t getMemoryUsage() const;
+    size_t getMemoryUsedPerBatch() const;
 };
 
 } // namespace cuda
diff --git a/include/argon2-gpu-common/argon2-common.h b/include/argon2-gpu-common/argon2-common.h
index fbcf67c..dfc618f 100644
--- a/include/argon2-gpu-common/argon2-common.h
+++ b/include/argon2-gpu-common/argon2-common.h
@@ -1,6 +1,8 @@
 #ifndef ARGON2COMMON_H
 #define ARGON2COMMON_H
 
+#include <cstddef>
+
 namespace argon2 {
 
 enum {
@@ -21,6 +23,24 @@ enum Version {
     ARGON2_VERSION_13 = 0x13,
 };
 
+enum OPT_MODE {
+    BASELINE = 0,
+    PRECOMPUTE_SIMPLE = 1,
+    PRECOMPUTE = 2,
+};
+
+typedef struct OptParams {
+    OPT_MODE mode;
+    uint32_t customBlockCount;
+    const uint32_t* customIndex;
+    uint32_t customIndexNbSteps;
+
+    OptParams() : 
+        mode(BASELINE), customBlockCount(0), 
+        customIndex(nullptr), customIndexNbSteps(0) {
+    }
+}t_optParams;
+
 } // namespace argon2
 
 
diff --git a/include/argon2-opencl/cl.hpp b/include/argon2-opencl/cl.hpp
index ced34f5..c03a742 100644
--- a/include/argon2-opencl/cl.hpp
+++ b/include/argon2-opencl/cl.hpp
@@ -142,6 +142,14 @@
  * \endcode
  *
  */
+
+#ifdef _WIN32
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+#include <iostream>
+
 #ifndef CL_HPP_
 #define CL_HPP_
 
@@ -318,7 +326,6 @@ public:
 #define __ERR_STR(x) NULL
 #endif // __CL_ENABLE_EXCEPTIONS
 
-
 namespace detail
 {
 #if defined(__CL_ENABLE_EXCEPTIONS)
@@ -327,7 +334,8 @@ static inline cl_int errHandler (
     const char * errStr = NULL)
 {
     if (err != CL_SUCCESS) {
-        throw Error(err, errStr);
+        printf("OpenCL error: errCode=%d str=%s\n", err, errStr ? errStr: "none");
+	throw Error(err, errStr);
     }
     return err;
 }
diff --git a/include/argon2-opencl/kernelrunner.h b/include/argon2-opencl/kernelrunner.h
index a1bba8b..0c78134 100644
--- a/include/argon2-opencl/kernelrunner.h
+++ b/include/argon2-opencl/kernelrunner.h
@@ -4,6 +4,8 @@
 #include "programcontext.h"
 #include "argon2-gpu-common/argon2params.h"
 
+#define REUSE_BUFFERS (1)
+
 namespace argon2 {
 namespace opencl {
 
@@ -15,16 +17,28 @@ private:
 
     std::uint32_t batchSize;
     bool bySegment;
-    bool precompute;
 
-    cl::CommandQueue queue;
+    cl::CommandQueue *queue;
     cl::Kernel kernel;
     cl::Buffer memoryBuffer, refsBuffer;
-    cl::Event start, end;
 
-    std::size_t memorySize;
+    cl::Event end;
+
+    t_optParams optParams;
 
+#if REUSE_BUFFERS
+    size_t blocksBufferSize;
+    size_t refsBufferSize;
+    const uint32_t* lastPrecomputeAddress;
+#endif
+
+    void setKernelsArgs();
+    size_t totalRefsSize() const;
     void precomputeRefs();
+    void initializeBuffers();
+    void setupKernel(const Device *device);
+    void freeBuffers();
+
 
 public:
     std::uint32_t getMinLanesPerBlock() const
@@ -40,16 +54,30 @@ public:
 
     KernelRunner(const ProgramContext *programContext,
                  const Argon2Params *params, const Device *device,
-                 std::uint32_t batchSize, bool bySegment, bool precompute);
+                 std::uint32_t batchSize, bool bySegment, t_optParams opt);
 
-    void *mapInputMemory(std::uint32_t jobId);
-    void unmapInputMemory(void *memory);
-
-    void *mapOutputMemory(std::uint32_t jobId);
-    void unmapOutputMemory(void *memory);
+    void uploadToInputMemoryAsync(std::uint32_t jobId, const void* srcPtr);
+    void fetchOutputMemoryAsync(std::uint32_t jobId, void* dstPtr);
 
     void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
-    float finish();
+
+    void waitForResults();
+    bool resultsReady();
+    void insertEndEventAndFlush();
+
+    void reconfigureArgon(
+        const Device *device,
+        const Argon2Params *newParams,
+        std::uint32_t newBatchSize,
+        const t_optParams &newOptParams);
+    
+    size_t getMemoryUsage() const;
+    size_t getMemoryUsedPerBatch() const;
+    uint32_t getBlockCount() const;
+
+    cl::CommandQueue *getQueue() {
+        return queue;
+    }
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/processingunit.h b/include/argon2-opencl/processingunit.h
index 277272d..77906a6 100644
--- a/include/argon2-opencl/processingunit.h
+++ b/include/argon2-opencl/processingunit.h
@@ -19,19 +19,38 @@ private:
     std::uint32_t bestLanesPerBlock;
     std::uint32_t bestJobsPerBlock;
 
+    cl::Buffer setPasswordBuffer;
+    uint8_t* passwordsPtr;
+
+    cl::Buffer resultBuffer;
+    uint8_t* resultsPtr;
+
+    std::size_t getStartBlocksSize();
+
 public:
     std::size_t getBatchSize() const { return runner.getBatchSize(); }
 
     ProcessingUnit(
             const ProgramContext *programContext, const Argon2Params *params,
             const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false);
+            bool bySegment, const t_optParams& optPrms);
 
     void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
 
-    void beginProcessing();
-    void endProcessing();
+    void runKernelAsync();
+    void fetchResultAsync(std::size_t index);
+
+    void waitForResults();
+    bool resultsReady();
+
+    void reconfigureArgon(
+        const Argon2Params *newParams,
+        std::uint32_t batchSize,
+        const t_optParams &optParams);
+    
+    size_t getMemoryUsage() const;
+    size_t getMemoryUsedPerBatch() const;
+    uint8_t* getResultPtr(int jobId);
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/programcontext.h b/include/argon2-opencl/programcontext.h
index 45d7ab3..a7fd166 100644
--- a/include/argon2-opencl/programcontext.h
+++ b/include/argon2-opencl/programcontext.h
@@ -37,7 +37,7 @@ public:
     ProgramContext(
             const GlobalContext *globalContext,
             const std::vector<Device> &devices,
-            Type type, Version version, char *pathToKernel);
+            Type type, Version version, const char *pathToKernel);
 };
 
 } // namespace opencl
diff --git a/lib/argon2-cuda/globalcontext.cpp b/lib/argon2-cuda/globalcontext.cpp
index eb52e02..56040c9 100644
--- a/lib/argon2-cuda/globalcontext.cpp
+++ b/lib/argon2-cuda/globalcontext.cpp
@@ -25,7 +25,11 @@ GlobalContext::GlobalContext()
     : devices()
 {
     int count;
-    CudaException::check(cudaGetDeviceCount(&count));
+    cudaError_t res = cudaGetDeviceCount(&count);
+    if (res != cudaSuccess) {
+        std::cout << "Cannot get CUDA device count, aborting...." << std::endl;
+        exit(1);
+    }
 
     devices.reserve(count);
     for (int i = 0; i < count; i++) {
diff --git a/lib/argon2-cuda/kernels.cu b/lib/argon2-cuda/kernels.cu
index 79fb767..429c4d8 100644
--- a/lib/argon2-cuda/kernels.cu
+++ b/lib/argon2-cuda/kernels.cu
@@ -3,8 +3,11 @@
 #define __CUDACC__
 #endif
 
-#include "kernels.h"
-#include "cudaexception.h"
+#include <cuda_runtime.h>
+
+#include "../../include/argon2-cuda/kernels.h"
+#include "../../include/argon2-cuda/cudaexception.h"
+#include "../../../include/perfscope.h"
 
 #include <stdexcept>
 #ifndef NDEBUG
@@ -49,8 +52,8 @@ __device__ uint64_t u64_shuffle(uint64_t v, uint32_t thread)
 {
     uint32_t lo = u64_lo(v);
     uint32_t hi = u64_hi(v);
-    lo = __shfl(lo, thread);
-    hi = __shfl(hi, thread);
+    lo = __shfl_sync(0xFFFFFFFF, lo, thread);
+    hi = __shfl_sync(0xFFFFFFFF, hi, thread);
     return u64_build(hi, lo);
 }
 
@@ -269,6 +272,7 @@ __device__ void next_addresses(struct block_th *addr, struct block_th *tmp,
     xor_block(addr, tmp);
 }
 
+// ~= index_alpha in ref code ...
 __device__ void compute_ref_pos(
         uint32_t lanes, uint32_t segment_blocks,
         uint32_t pass, uint32_t lane, uint32_t slice, uint32_t offset,
@@ -309,6 +313,12 @@ struct ref {
     uint32_t ref_index;
 };
 
+struct index {
+    uint32_t refSlot;
+    uint32_t store;
+    uint32_t storeSlot;
+};
+
 /*
  * Refs hierarchy:
  * lanes -> passes -> slices -> blocks
@@ -415,6 +425,25 @@ __device__ void argon2_core(
     store_block(mem_curr, prev, thread);
 }
 
+template<uint32_t version>
+__device__ void argon2_core_precomputedIndex(
+    struct block_g *memory, struct block_g *mem_curr,
+    struct block_th *prev, struct block_th *tmp,
+    uint32_t thread, uint32_t ref_index)
+{
+    struct block_g *mem_ref = memory + ref_index;
+
+    load_block_xor(prev, mem_ref, thread);
+    move_block(tmp, prev);
+
+    shuffle_block(prev, thread);
+
+    xor_block(prev, tmp);
+
+    if (mem_curr != 0)
+        store_block(mem_curr, prev, thread);
+}
+
 template<uint32_t type, uint32_t version>
 __device__ void argon2_step_precompute(
         struct block_g *memory, struct block_g *mem_curr,
@@ -548,6 +577,33 @@ __global__ void argon2_kernel_oneshot_precompute(
     }
 }
 
+template<uint32_t type, uint32_t version>
+__global__ void argon2_kernel_oneshot_precomputedIndex(
+    struct block_g *memory, const struct index *indexs, 
+    uint32_t nSteps, uint32_t blocksPerBatch)
+{
+    struct block_th prev, tmp;
+    uint32_t batch_id = blockIdx.z; // nBatches
+    uint32_t thread = threadIdx.x; // 32
+
+    memory += (size_t)batch_id * blocksPerBatch;
+
+    load_block(&prev, memory + 1, thread);
+    for (uint32_t i = 2; i < nSteps; ++i) {
+        uint32_t store = indexs->store;
+        uint32_t storeSlot = indexs->storeSlot;
+        uint32_t ref_index = indexs->refSlot;
+
+        struct block_g *mem_curr = store ? (memory + storeSlot) : 0;
+        argon2_core_precomputedIndex<version>(
+            memory, mem_curr, &prev, &tmp, thread, ref_index);
+
+        indexs++;
+    }
+
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
+}
+
 template<uint32_t type, uint32_t version>
 __device__ void argon2_step(
         struct block_g *memory, struct block_g *mem_curr,
@@ -757,48 +813,162 @@ __global__ void argon2_kernel_oneshot(
     }
 }
 
+void cudaSafeFree(void* &ptr) {
+    if (ptr)
+        cudaFree(ptr);
+    ptr = nullptr;
+}
+
 KernelRunner::KernelRunner(uint32_t type, uint32_t version, uint32_t passes,
                            uint32_t lanes, uint32_t segmentBlocks,
-                           uint32_t batchSize, bool bySegment, bool precompute)
-    : type(type), version(version), passes(passes), lanes(lanes),
+                           uint32_t batchSize, bool bySegment, t_optParams opt)
+    : type(type), version(version), passes(passes), lanes(lanes), optParams(opt),
       segmentBlocks(segmentBlocks), batchSize(batchSize), bySegment(bySegment),
-      precompute(precompute), stream(nullptr), memory(nullptr),
+      stream(nullptr), memory(nullptr),
       refs(nullptr), start(nullptr), end(nullptr)
+#if REUSE_BUFFERS
+      , blocksBufferSize(0)
+      , refsBufferSize(0)
+      , lastPrecomputeAddress(nullptr)
+#endif
 {
-    // FIXME: check overflow:
-    size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE * batchSize;
+    CudaException::check(cudaStreamCreate(&stream));
 
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
-#endif
+    initializeBuffers();
+}
 
-    CudaException::check(cudaMalloc(&memory, memorySize));
+void KernelRunner::reconfigureArgon(
+    std::uint32_t newPasses,
+    std::uint32_t newLanes,
+    std::uint32_t newSegmentBlocks,
+    std::uint32_t newBatchSize,
+    const t_optParams &newOptParams)
+{
+    passes = newPasses;
+    lanes = newLanes;
+    segmentBlocks = newSegmentBlocks;
+    batchSize = newBatchSize;
+    optParams = newOptParams;
+    
+#if REUSE_BUFFERS
+    initializeBuffers();
+#else
+    freeBuffers();
+    initializeBuffers();
+#endif
+}
 
-    CudaException::check(cudaEventCreate(&start));
-    CudaException::check(cudaEventCreate(&end));
+uint32_t KernelRunner::getBlockCount() const {
+    if (optParams.customBlockCount)
+        return optParams.customBlockCount;
+    return lanes * segmentBlocks * ARGON2_SYNC_POINTS;
+}
 
-    CudaException::check(cudaStreamCreate(&stream));
+size_t totalBlocksSize(uint32_t nBlocks, uint32_t batchSize) {
+    return nBlocks * batchSize * ARGON2_BLOCK_SIZE;
+}
 
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
+size_t KernelRunner::totalRefsSize() const {
+    if ((type == ARGON2_I || type == ARGON2_ID) &&
+        optParams.mode == PRECOMPUTE_SIMPLE) {
         uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
+            type == ARGON2_ID
+            ? lanes * (ARGON2_SYNC_POINTS / 2)
+            : passes * lanes * ARGON2_SYNC_POINTS;
+        size_t refsSize = 
+            segments * segmentBlocks * sizeof(struct ref);
+        return refsSize;
+    }
+    else if (type == ARGON2_I &&
+        optParams.mode == PRECOMPUTE) {
+        return optParams.customIndexNbSteps * sizeof(struct index);
+    }
+    return 0;
+}
 
-        size_t refsSize = segments * segmentBlocks * sizeof(struct ref);
+void KernelRunner::initializeBuffers() {
+#if REUSE_BUFFERS
+    // blocks buffer
+    size_t blocksSize = totalBlocksSize(getBlockCount(), batchSize);
+    if (blocksBufferSize < blocksSize)
+    {
+        PERFSCOPE("Cuda realloc blocks buffer");
+        cudaSafeFree(memory);
+        CudaException::check(cudaMalloc(&memory, blocksSize));
+        blocksBufferSize = blocksSize;
+    }
 
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
+    // refs / index buffer
+    size_t refsSize = totalRefsSize();
+    if (refsSize && refsBufferSize < refsSize) {
+        {
+            PERFSCOPE("Cuda realloc refs buffer");
+            cudaSafeFree(refs);
+            CudaException::check(cudaMalloc(&refs, refsSize));
+            refsBufferSize = refsSize;
+        }
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
+            precomputeRefs();
+            CudaException::check(cudaStreamSynchronize(stream));
+        }
+    }
+
+    if (optParams.mode == PRECOMPUTE && optParams.customIndex != lastPrecomputeAddress) {
+        PERFSCOPE("Cuda memcpy index");
+        cudaMemcpy(refs, optParams.customIndex, refsSize, cudaMemcpyHostToDevice);
+        lastPrecomputeAddress = optParams.customIndex;
+    }
+#else
+    // blocks buffer
+    {
+        PERFSCOPE("Cuda alloc blocks buffer");
+        size_t blocksSize = totalBlocksSize(getBlockCount(), batchSize);
+        CudaException::check(cudaMalloc(&memory, blocksSize));
+    }
+    
+    // refs / index buffer
+    size_t refsSize = totalRefsSize();
+    if (refsSize) {
+        {
+            PERFSCOPE("Cuda alloc refs buffer");
+            CudaException::check(cudaMalloc(&refs, refsSize));
+        }
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
+            precomputeRefs();
+            CudaException::check(cudaStreamSynchronize(stream));
+        }
+        else if (optParams.mode == PRECOMPUTE) {
+            PERFSCOPE("Cuda memcpy index");
+            cudaMemcpy(refs, optParams.customIndex, refsSize, cudaMemcpyHostToDevice);
+        }
+    }
 #endif
+}
 
-        CudaException::check(cudaMalloc(&refs, refsSize));
+void KernelRunner::freeBuffers() {
+    PERFSCOPE("KernelRunner::freeBuffers");
+    cudaSafeFree(refs);
+    cudaSafeFree(memory);
+}
+
+KernelRunner::~KernelRunner()
+{
+    freeBuffers();
+    if (stream != nullptr) {
+        cudaStreamDestroy(stream);
+    }
+}
 
-        precomputeRefs();
-        CudaException::check(cudaStreamSynchronize(stream));
+size_t KernelRunner::getMemoryUsage() const {
+    size_t tot = totalBlocksSize(getBlockCount(), batchSize);
+    if (refs) {
+        tot += totalRefsSize();
     }
+    return tot;
+}
+
+size_t KernelRunner::getMemoryUsedPerBatch() const {
+    return getBlockCount() * ARGON2_BLOCK_SIZE;
 }
 
 void KernelRunner::precomputeRefs()
@@ -826,47 +996,42 @@ void KernelRunner::precomputeRefs()
     }
 }
 
-KernelRunner::~KernelRunner()
-{
-    if (start != nullptr) {
-        cudaEventDestroy(start);
-    }
-    if (end != nullptr) {
-        cudaEventDestroy(end);
-    }
-    if (stream != nullptr) {
-        cudaStreamDestroy(stream);
+void KernelRunner::syncStream() {
+    CudaException::check(cudaStreamSynchronize(stream));
+}
+
+bool KernelRunner::streamOperationsComplete() {
+    cudaError_t res = cudaStreamQuery(stream);
+    if (res == cudaSuccess) {
+        return true;
     }
-    if (memory != nullptr) {
-        cudaFree(memory);
+    else if (res == cudaErrorNotReady) {
+        return false;
     }
-    if (refs != nullptr) {
-        cudaFree(refs);
+    else {
+        CudaException::check(res);
+        return false;
     }
 }
 
 void KernelRunner::writeInputMemory(uint32_t jobId, const void *buffer)
 {
-    std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE;
+    std::size_t memorySize = getBlockCount() * ARGON2_BLOCK_SIZE;
     std::size_t size = static_cast<size_t>(lanes) * 2 * ARGON2_BLOCK_SIZE;
     std::size_t offset = memorySize * jobId;
     auto mem = static_cast<uint8_t *>(memory) + offset;
     CudaException::check(cudaMemcpyAsync(mem, buffer, size,
                                          cudaMemcpyHostToDevice, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
 }
 
 void KernelRunner::readOutputMemory(uint32_t jobId, void *buffer)
 {
-    std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE;
+    std::size_t memorySize = getBlockCount() * ARGON2_BLOCK_SIZE;
     std::size_t size = static_cast<size_t>(lanes) * ARGON2_BLOCK_SIZE;
     std::size_t offset = memorySize * (jobId + 1) - size;
     auto mem = static_cast<uint8_t *>(memory) + offset;
     CudaException::check(cudaMemcpyAsync(buffer, mem, size,
                                          cudaMemcpyDeviceToHost, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
 }
 
 void KernelRunner::runKernelSegment(uint32_t lanesPerBlock,
@@ -885,7 +1050,7 @@ void KernelRunner::runKernelSegment(uint32_t lanesPerBlock,
     dim3 blocks = dim3(1, lanes / lanesPerBlock, batchSize / jobsPerBlock);
     dim3 threads = dim3(THREADS_PER_LANE, lanesPerBlock, jobsPerBlock);
     if (type == ARGON2_I) {
-        if (precompute) {
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
             struct ref *refs = (struct ref *)this->refs;
             if (version == ARGON2_VERSION_10) {
                 argon2_kernel_segment_precompute<ARGON2_I, ARGON2_VERSION_10>
@@ -912,7 +1077,7 @@ void KernelRunner::runKernelSegment(uint32_t lanesPerBlock,
             }
         }
     } else if (type == ARGON2_ID) {
-        if (precompute) {
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
             struct ref *refs = (struct ref *)this->refs;
             if (version == ARGON2_VERSION_10) {
                 argon2_kernel_segment_precompute<ARGON2_ID, ARGON2_VERSION_10>
@@ -967,8 +1132,17 @@ void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
     struct block_g *memory_blocks = (struct block_g *)memory;
     dim3 blocks = dim3(1, 1, batchSize / jobsPerBlock);
     dim3 threads = dim3(THREADS_PER_LANE, lanes, jobsPerBlock);
-    if (type == ARGON2_I) {
-        if (precompute) {
+
+    if (type == ARGON2_I && optParams.mode > PRECOMPUTE_SIMPLE) {
+        struct index *indexs = (struct index *)this->refs;
+        uint32_t nSteps = ARGON2_SYNC_POINTS * segmentBlocks;
+        uint32_t blocksPerBatch = getBlockCount();
+        argon2_kernel_oneshot_precomputedIndex<ARGON2_I, ARGON2_VERSION_13>
+            << <blocks, threads, 0, stream >> >(
+                memory_blocks, indexs, nSteps, blocksPerBatch);
+    }
+    else if (type == ARGON2_I) {
+        if (optParams.mode== PRECOMPUTE_SIMPLE) {
             struct ref *refs = (struct ref *)this->refs;
             if (version == ARGON2_VERSION_10) {
                 argon2_kernel_oneshot_precompute<ARGON2_I, ARGON2_VERSION_10>
@@ -991,7 +1165,7 @@ void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
             }
         }
     } else if (type == ARGON2_ID) {
-        if (precompute) {
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
             struct ref *refs = (struct ref *)this->refs;
             if (version == ARGON2_VERSION_10) {
                 argon2_kernel_oneshot_precompute<ARGON2_ID, ARGON2_VERSION_10>
@@ -1028,8 +1202,6 @@ void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
 
 void KernelRunner::run(uint32_t lanesPerBlock, uint32_t jobsPerBlock)
 {
-    CudaException::check(cudaEventRecord(start, stream));
-
     if (bySegment) {
         for (uint32_t pass = 0; pass < passes; pass++) {
             for (uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
@@ -1041,18 +1213,11 @@ void KernelRunner::run(uint32_t lanesPerBlock, uint32_t jobsPerBlock)
     }
 
     CudaException::check(cudaGetLastError());
-
-    CudaException::check(cudaEventRecord(end, stream));
 }
 
 float KernelRunner::finish()
 {
-    CudaException::check(cudaStreamSynchronize(stream));
-
-    float time = 0.0;
-    CudaException::check(cudaEventElapsedTime(&time, start, end));
-    return time;
+    return 0.f;
 }
-
 } // cuda
 } // argon2
diff --git a/lib/argon2-cuda/processingunit.cpp b/lib/argon2-cuda/processingunit.cpp
index 7768427..e4da655 100644
--- a/lib/argon2-cuda/processingunit.cpp
+++ b/lib/argon2-cuda/processingunit.cpp
@@ -24,30 +24,47 @@ static bool isPowerOfTwo(std::uint32_t x)
     return (x & (x - 1)) == 0;
 }
 
+#pragma warning(disable:4267)
+#pragma warning(disable:4101)
+
 ProcessingUnit::ProcessingUnit(
         const ProgramContext *programContext, const Argon2Params *params,
         const Device *device, std::size_t batchSize, bool bySegment,
-        bool precomputeRefs)
+        const t_optParams& optPrms)
     : programContext(programContext), params(params), device(device),
       runner(programContext->getArgon2Type(),
              programContext->getArgon2Version(), params->getTimeCost(),
-             params->getLanes(), params->getSegmentBlocks(), batchSize,
-             bySegment, precomputeRefs),
+             params->getLanes(), params->getSegmentBlocks(),
+             batchSize, bySegment, optPrms),
       bestLanesPerBlock(runner.getMinLanesPerBlock()),
       bestJobsPerBlock(runner.getMinJobsPerBlock())
 {
+    // already done by caller, but let's still do it again just in case ...
+    // (it is done by the caller because the runner constructor also needs current device set !)
     setCudaDevice(device->getDeviceIndex());
 
+    // preallocate buffers used by ProcessingUnit::setPassword
+    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
+    for (int i = 0; i < batchSize; i++) {
+        uint8_t* ptrPinnedMem = nullptr;
+        cudaError_t status = cudaMallocHost((void**)&(ptrPinnedMem), size);
+        if (status != cudaSuccess) {
+            std::cout << "Error allocating pinned host memory" << std::endl;
+            exit(1);
+        }
+        setPasswordBuffers.push_back(ptrPinnedMem);
+    }
+
     /* pre-fill first blocks with pseudo-random data: */
     for (std::size_t i = 0; i < batchSize; i++) {
         setPassword(i, NULL, 0);
     }
-
+#if 0
     if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
             && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
+//#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
@@ -58,36 +75,36 @@ ProcessingUnit::ProcessingUnit(
                 runner.run(lpb, bestJobsPerBlock);
                 time = runner.finish();
             } catch(CudaException &ex) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
                 std::cerr << "[WARN]   CUDA error on " << lpb
                           << " lanes per block: " << ex.what() << std::endl;
-#endif
+//#endif
                 break;
             }
 
-#ifndef NDEBUG
+//#ifndef NDEBUG
             std::cerr << "[INFO]   " << lpb << " lanes per block: "
                       << time << " ms" << std::endl;
-#endif
+//#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestLanesPerBlock = lpb;
             }
         }
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Picked " << bestLanesPerBlock
                   << " lanes per block." << std::endl;
-#endif
+//#endif
     }
 
     /* Only tune jobs per block if we hit maximum lanes per block: */
     if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
             && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
             && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
+//#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
@@ -98,59 +115,85 @@ ProcessingUnit::ProcessingUnit(
                 runner.run(bestLanesPerBlock, jpb);
                 time = runner.finish();
             } catch(CudaException &ex) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
                 std::cerr << "[WARN]   CUDA error on " << jpb
                           << " jobs per block: " << ex.what() << std::endl;
-#endif
+//#endif
                 break;
             }
 
-#ifndef NDEBUG
+//#ifndef NDEBUG
             std::cerr << "[INFO]   " << jpb << " jobs per block: "
                       << time << " ms" << std::endl;
-#endif
+//#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestJobsPerBlock = jpb;
             }
         }
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Picked " << bestJobsPerBlock
                   << " jobs per block." << std::endl;
-#endif
+//#endif
     }
+#endif
 }
 
 void ProcessingUnit::setPassword(std::size_t index, const void *pw,
                                  std::size_t pwSize)
 {
-    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    params->fillFirstBlocks(buffer.get(), pw, pwSize,
+    params->fillFirstBlocks(setPasswordBuffers[index], pw, pwSize,
                             programContext->getArgon2Type(),
                             programContext->getArgon2Version());
-    runner.writeInputMemory(index, buffer.get());
+
+    runner.writeInputMemory(index, setPasswordBuffers[index]);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    std::size_t size = params->getLanes() * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    runner.readOutputMemory(index, buffer.get());
-    params->finalize(hash, buffer.get());
+void ProcessingUnit::fetchResultAsync(std::size_t index, void *dest) {
+    runner.readOutputMemory(index, dest);
 }
 
-void ProcessingUnit::beginProcessing()
-{
+void ProcessingUnit::syncStream() {
+    runner.syncStream();
+}
+
+bool ProcessingUnit::streamOperationsComplete() {
+    return runner.streamOperationsComplete();
+}
+
+void ProcessingUnit::beginProcessing() {
+    // we MUST set cuda device before launching a kernel
     setCudaDevice(device->getDeviceIndex());
     runner.run(bestLanesPerBlock, bestJobsPerBlock);
 }
 
-void ProcessingUnit::endProcessing()
-{
+void ProcessingUnit::endProcessing() {
     runner.finish();
 }
 
+void ProcessingUnit::reconfigureArgon(const Argon2Params *newParams, 
+                                      std::uint32_t batchSize,
+                                      const t_optParams &optParams) {
+    setCudaDevice(device->getDeviceIndex());
+    params = newParams;
+    runner.reconfigureArgon(
+        params->getTimeCost(),
+        params->getLanes(),
+        params->getSegmentBlocks(),
+        batchSize,
+        optParams);
+    bestLanesPerBlock = runner.getMinLanesPerBlock();
+    bestJobsPerBlock = runner.getMinJobsPerBlock();
+}
+
+size_t ProcessingUnit::getMemoryUsage() const {
+    return runner.getMemoryUsage();
+}
+
+size_t ProcessingUnit::getMemoryUsedPerBatch() const {
+    return runner.getMemoryUsedPerBatch();
+}
+
 } // namespace cuda
 } // namespace argon2
diff --git a/lib/argon2-gpu-common/argon2params.cpp b/lib/argon2-gpu-common/argon2params.cpp
index dd122d8..b74c11d 100644
--- a/lib/argon2-gpu-common/argon2params.cpp
+++ b/lib/argon2-gpu-common/argon2params.cpp
@@ -20,6 +20,8 @@ static void store32(void *dst, std::uint32_t v)
     *out++ = static_cast<std::uint8_t>(v);
 }
 
+#pragma warning(disable:4267)
+
 Argon2Params::Argon2Params(
         std::size_t outLen,
         const void *salt, std::size_t saltLen,
diff --git a/lib/argon2-gpu-common/blake2b.h b/lib/argon2-gpu-common/blake2b.h
index 094fb83..7f438ee 100644
--- a/lib/argon2-gpu-common/blake2b.h
+++ b/lib/argon2-gpu-common/blake2b.h
@@ -2,6 +2,7 @@
 #define ARGON2_BLAKE2B_H
 
 #include <cstdint>
+#include <cstddef>
 
 namespace argon2 {
 
diff --git a/lib/argon2-opencl/kernelloader.cpp b/lib/argon2-opencl/kernelloader.cpp
index 22949f3..62bb887 100644
--- a/lib/argon2-opencl/kernelloader.cpp
+++ b/lib/argon2-opencl/kernelloader.cpp
@@ -12,11 +12,16 @@ cl::Program KernelLoader::loadArgon2Program(
         const std::string &sourceDirectory,
         Type type, Version version, bool debug)
 {
-    std::string sourcePath = sourceDirectory + "/argon2_kernel.cl";
+    std::string sourcePath = sourceDirectory + "argon2_kernel.cl";
     std::string sourceText;
     std::stringstream buildOpts;
     {
         std::ifstream sourceFile { sourcePath };
+
+        if (!sourceFile.is_open()) {
+            throw std::logic_error(std::string("Cannot find kernel source: ") + sourcePath);
+        }
+
         sourceText = {
             std::istreambuf_iterator<char>(sourceFile),
             std::istreambuf_iterator<char>()
@@ -38,6 +43,7 @@ cl::Program KernelLoader::loadArgon2Program(
         for (cl::Device &device : context.getInfo<CL_CONTEXT_DEVICES>()) {
             std::cerr << "  Build log from device '" << device.getInfo<CL_DEVICE_NAME>() << "':" << std::endl;
             std::cerr << prog.getBuildInfo<CL_PROGRAM_BUILD_LOG>(device);
+            err;
         }
         throw;
     }
diff --git a/lib/argon2-opencl/kernelrunner.cpp b/lib/argon2-opencl/kernelrunner.cpp
index 9fe39b4..75d25f8 100644
--- a/lib/argon2-opencl/kernelrunner.cpp
+++ b/lib/argon2-opencl/kernelrunner.cpp
@@ -1,61 +1,93 @@
 #include "kernelrunner.h"
 
 #include <stdexcept>
-
-#ifndef NDEBUG
+#include <thread>
 #include <iostream>
-#endif
+#include <iomanip>
+#include <map>
+#include <sstream>
 
 #define THREADS_PER_LANE 32
 
+//#define SINGLE_QUEUE_PER_DEVICE
+//#define FLUSH_ALL
+//#define SKIP_MEM_TRANSFERS
+
+#include "../../include/perfscope.h"
+
 namespace argon2 {
 namespace opencl {
 
+#ifdef SINGLE_QUEUE_PER_DEVICE
+std::map<const Device*, cl::CommandQueue*> s_queues;
+#endif
+
 enum {
     ARGON2_REFS_PER_BLOCK = ARGON2_BLOCK_SIZE / (2 * sizeof(cl_uint)),
 };
 
-KernelRunner::KernelRunner(const ProgramContext *programContext,
-                           const Argon2Params *params, const Device *device,
-                           std::uint32_t batchSize, bool bySegment, bool precompute)
-    : programContext(programContext), params(params), batchSize(batchSize),
-      bySegment(bySegment), precompute(precompute),
-      memorySize(params->getMemorySize() * static_cast<std::size_t>(batchSize))
-{
+struct index {
+    uint32_t refSlot;
+    uint32_t store;
+    uint32_t storeSlot;
+};
+
+size_t KernelRunner::totalRefsSize() const {
     auto context = programContext->getContext();
     std::uint32_t passes = params->getTimeCost();
     std::uint32_t lanes = params->getLanes();
     std::uint32_t segmentBlocks = params->getSegmentBlocks();
 
-    queue = cl::CommandQueue(context, device->getCLDevice(),
-                             CL_QUEUE_PROFILING_ENABLE);
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
-#endif
-
-    memoryBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, memorySize);
-
     Type type = programContext->getArgon2Type();
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
+    if ((type == ARGON2_I || type == ARGON2_ID) && (optParams.mode == PRECOMPUTE_SIMPLE)) {
         std::uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
-
+            type == ARGON2_ID
+            ? lanes * (ARGON2_SYNC_POINTS / 2)
+            : passes * lanes * ARGON2_SYNC_POINTS;
         std::size_t refsSize = segments * segmentBlocks * sizeof(cl_uint) * 2;
+        return refsSize;
+    }
+    else if (type == ARGON2_I &&
+        optParams.mode == PRECOMPUTE) {
+        if (sizeof(struct index) != (3 * sizeof(cl_uint))) {
+            throw std::logic_error("Invalid index struct size");
+        }
+        return optParams.customIndexNbSteps * sizeof(struct index);
+    }
+    return 0;
+}
 
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
-#endif
+void KernelRunner::setKernelsArgs() {
+    std::uint32_t passes = params->getTimeCost();
+    std::uint32_t lanes = params->getLanes();
+    std::uint32_t segmentBlocks = params->getSegmentBlocks();
 
-        refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
+    kernel.setArg<cl::Buffer>(1, memoryBuffer);
+
+    if (optParams.mode == PRECOMPUTE) {
+        std::size_t shmemSize = THREADS_PER_LANE * sizeof(cl_uint) * 2;
+
+        kernel.setArg<cl::Buffer>(2, refsBuffer);
 
-        precomputeRefs();
+        uint32_t nSteps = ARGON2_SYNC_POINTS * segmentBlocks;
+
+        kernel.setArg<cl_uint>(3, nSteps);
+        kernel.setArg<cl_uint>(4, optParams.customBlockCount);
+    }
+    else if (optParams.mode == PRECOMPUTE_SIMPLE) {
+        kernel.setArg<cl::Buffer>(2, refsBuffer);
+        kernel.setArg<cl_uint>(3, passes);
+        kernel.setArg<cl_uint>(4, lanes);
+        kernel.setArg<cl_uint>(5, segmentBlocks);
+    }
+    else {
+        kernel.setArg<cl_uint>(2, passes);
+        kernel.setArg<cl_uint>(3, lanes);
+        kernel.setArg<cl_uint>(4, segmentBlocks);
     }
+}
 
+void KernelRunner::setupKernel(const Device *device) {
     static const char *KERNEL_NAMES[2][2] = {
         {
             "argon2_kernel_oneshot",
@@ -67,19 +99,161 @@ KernelRunner::KernelRunner(const ProgramContext *programContext,
         }
     };
 
-    kernel = cl::Kernel(programContext->getProgram(),
-                        KERNEL_NAMES[precompute][bySegment]);
-    kernel.setArg<cl::Buffer>(1, memoryBuffer);
-    if (precompute) {
-        kernel.setArg<cl::Buffer>(2, refsBuffer);
-        kernel.setArg<cl_uint>(3, passes);
-        kernel.setArg<cl_uint>(4, lanes);
-        kernel.setArg<cl_uint>(5, segmentBlocks);
-    } else {
-        kernel.setArg<cl_uint>(2, passes);
-        kernel.setArg<cl_uint>(3, lanes);
-        kernel.setArg<cl_uint>(4, segmentBlocks);
+    static const char *KERNEL_PRECOMPUTED_INDEX = "argon2_kernel_oneshot_precomputedIndex";
+
+    bool precompute = (optParams.mode == PRECOMPUTE_SIMPLE);
+    const char* kernelName = KERNEL_NAMES[precompute][bySegment];
+
+    if (optParams.mode == PRECOMPUTE) {
+        // check that we use argon2I
+        auto buildOptions =
+            programContext->getProgram().getBuildInfo<CL_PROGRAM_BUILD_OPTIONS>(device->getCLDevice());
+        std::ostringstream oss;
+        oss << "-DARGON2_TYPE=" << argon2::ARGON2_I;
+        if (buildOptions.find(oss.str()) == std::string::npos) {
+            throw std::logic_error("PRECOMPUTE only supported with ARGON2_I");
+        }
+
+        // check that we have only 1 lane, 1 pass etc.
+        if (bySegment || params->getLanes() > 1 || params->getTimeCost() > 1) {
+            throw std::logic_error("PRECOMPUTE only supported with 1 lane, 1 pass and oneshot mode");
+        }
+
+        // use custom kernel
+        kernelName = KERNEL_PRECOMPUTED_INDEX;
+    }
+
+    kernel = cl::Kernel(programContext->getProgram(), kernelName);
+
+    setKernelsArgs();
+}
+
+KernelRunner::KernelRunner(const ProgramContext *programContext,
+                           const Argon2Params *params, const Device *device,
+                           std::uint32_t batchSize, bool bySegment, t_optParams opt)
+    : programContext(programContext), params(params), batchSize(batchSize),
+      bySegment(bySegment), optParams(opt)
+#if REUSE_BUFFERS
+    , blocksBufferSize(0)
+    , refsBufferSize(0)
+    , lastPrecomputeAddress(nullptr)
+#endif
+{
+    auto context = programContext->getContext();
+    cl_command_queue_properties props = CL_QUEUE_PROFILING_ENABLE;
+#ifdef SINGLE_QUEUE_PER_DEVICE
+    auto it = s_queues.find(device);
+    if (it == s_queues.end()) {
+        s_queues.insert(
+            std::make_pair(
+                device,
+                new cl::CommandQueue(context, device->getCLDevice(), props)));
+    }
+    queue = s_queues[device];
+#else
+    queue = new cl::CommandQueue(context, device->getCLDevice(), props);
+#endif
+
+    initializeBuffers();
+  
+    setupKernel(device);
+}
+
+size_t totalBlocksSize(uint32_t nBlocks, uint32_t batchSize) {
+    return nBlocks * batchSize * ARGON2_BLOCK_SIZE;
+}
+
+void KernelRunner::initializeBuffers() {
+#if REUSE_BUFFERS
+    auto context = programContext->getContext();
+    
+    // blocks
+    size_t blocksSize = totalBlocksSize(getBlockCount(), batchSize);
+    if (blocksBufferSize < blocksSize) {
+        memoryBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, blocksSize);
+        blocksBufferSize = blocksSize;
+    }
+
+    // refs / index buffer
+    size_t refsSize = totalRefsSize();
+    if (refsSize && refsBufferSize < refsSize) {
+        refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
+        refsBufferSize = refsSize;
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
+            precomputeRefs();
+        }
+    }
+
+    if (optParams.mode == PRECOMPUTE && optParams.customIndex != lastPrecomputeAddress) {
+        bool blocking = false;
+        cl_int res = queue->enqueueWriteBuffer(
+            refsBuffer,
+            blocking,
+            0,
+            refsSize,
+            optParams.customIndex,
+            NULL,
+            NULL);
+        queue->finish();
+        lastPrecomputeAddress = optParams.customIndex;
+    }
+#else
+    // blocks
+    auto context = programContext->getContext();
+    size_t blocksSize = totalBlocksSize(getBlockCount(), batchSize);
+    memoryBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, blocksSize);
+
+    // refs
+    size_t refsSize = totalRefsSize();
+    if (refsSize) {
+        if (optParams.mode == PRECOMPUTE_SIMPLE) {
+            refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
+            precomputeRefs();
+        }
+        else if (optParams.mode == PRECOMPUTE) {
+            refsBuffer = cl::Buffer(context, CL_MEM_READ_ONLY, refsSize);
+            bool blocking = false;
+            cl_int res = queue->enqueueWriteBuffer(
+                refsBuffer,
+                blocking,
+                0,
+                refsSize,
+                optParams.customIndex,
+                NULL,
+                NULL);
+            queue->finish();
+        }
+    }
+#endif
+}
+
+void KernelRunner::freeBuffers() {
+    memoryBuffer = cl::Buffer();
+    refsBuffer = cl::Buffer();
+}
+
+void KernelRunner::reconfigureArgon(
+    const Device *device,
+    const Argon2Params *newParams, 
+    std::uint32_t newBatchSize,
+    const t_optParams &newOptParams) {
+    if (bySegment) {
+        printf("reconfigureArgon not supported with bySegment mode !\n");
+        exit(1);
     }
+
+    params = newParams;
+    batchSize = newBatchSize;
+    optParams = newOptParams;
+
+#if REUSE_BUFFERS
+    initializeBuffers();
+#else
+    freeBuffers();
+    initializeBuffers();
+#endif
+
+    setupKernel(device);
 }
 
 void KernelRunner::precomputeRefs()
@@ -98,6 +272,7 @@ void KernelRunner::precomputeRefs()
 
     cl::Kernel kernel = cl::Kernel(programContext->getProgram(),
                                    "argon2_precompute_kernel");
+
     kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
     kernel.setArg<cl::Buffer>(1, refsBuffer);
     kernel.setArg<cl_uint>(2, passes);
@@ -106,36 +281,62 @@ void KernelRunner::precomputeRefs()
 
     cl::NDRange globalRange { THREADS_PER_LANE * segments * segmentAddrBlocks };
     cl::NDRange localRange { THREADS_PER_LANE };
-    queue.enqueueNDRangeKernel(kernel, cl::NullRange, globalRange, localRange);
-    queue.finish();
+
+    queue->enqueueNDRangeKernel(kernel, cl::NullRange, globalRange, localRange);
+    queue->finish();
 }
 
-void *KernelRunner::mapInputMemory(std::uint32_t jobId)
-{
-    std::size_t memorySize = params->getMemorySize();
+void KernelRunner::uploadToInputMemoryAsync(std::uint32_t jobId, const void* srcPtr) {
     std::size_t mappedSize = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_WRITE,
-                                  memorySize * jobId, mappedSize);
+    std::size_t batchMemSize = getBlockCount() * ARGON2_BLOCK_SIZE;
+    bool blocking = false;
+    size_t offset = batchMemSize * jobId;
+    size_t size = mappedSize;
+#ifndef SKIP_MEM_TRANSFERS
+    {
+        PerfScope p("enqueueWriteBuffer");
+        cl_int res = queue->enqueueWriteBuffer(
+            memoryBuffer,
+            blocking,
+            offset,
+            size,
+            srcPtr,
+            NULL,
+            NULL);
+    }
+#endif
+#ifdef FLUSH_ALL
+    queue->flush();
+#endif
 }
 
-void KernelRunner::unmapInputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
+void KernelRunner::fetchOutputMemoryAsync(std::uint32_t jobId, void* dstPtr) {
+    std::size_t batchMemSize = getBlockCount() * ARGON2_BLOCK_SIZE;
+    std::size_t mappedSize = static_cast<std::size_t>(params->getLanes()) * ARGON2_BLOCK_SIZE;
+    std::size_t mappedOffset = batchMemSize * (jobId + 1) - mappedSize;
+    bool blocking = false;
+#ifndef SKIP_MEM_TRANSFERS
+    {
+        PerfScope p("enqueueReadBuffer");
+        cl_int res = queue->enqueueReadBuffer(
+            memoryBuffer,
+            blocking,
+            mappedOffset,
+            mappedSize,
+            dstPtr,
+            NULL,
+            NULL);
+    }
+#endif
+#ifdef FLUSH_ALL
+    queue->flush();
+#endif
 }
 
-void *KernelRunner::mapOutputMemory(std::uint32_t jobId)
-{
-    std::size_t memorySize = params->getMemorySize();
-    std::size_t mappedSize = static_cast<std::size_t>(params->getLanes())
-            * ARGON2_BLOCK_SIZE;
-    std::size_t mappedOffset = memorySize * (jobId + 1) - mappedSize;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_READ,
-                                  mappedOffset, mappedSize);
-}
+void KernelRunner::insertEndEventAndFlush() {
+    queue->enqueueMarker(&end);
 
-void KernelRunner::unmapOutputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
+    cl::detail::errHandler(queue->flush(), "KernelRunner::resultsReady, flushing queue");
 }
 
 void KernelRunner::run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock)
@@ -157,39 +358,72 @@ void KernelRunner::run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock)
         throw std::logic_error("Invalid jobsPerBlock!");
     }
 
+    // run the kernel
     cl::NDRange globalRange { THREADS_PER_LANE * lanes, batchSize };
     cl::NDRange localRange { THREADS_PER_LANE * lanesPerBlock, jobsPerBlock };
 
-    queue.enqueueMarker(&start);
-
-    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * jobsPerBlock
-            * sizeof(cl_uint) * 2;
+    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * jobsPerBlock * sizeof(cl_uint) * 2;
     kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
     if (bySegment) {
+        bool precomputeSimple = (optParams.mode == PRECOMPUTE_SIMPLE);
         for (std::uint32_t pass = 0; pass < passes; pass++) {
             for (std::uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
-                kernel.setArg<cl_uint>(precompute ? 6 : 5, pass);
-                kernel.setArg<cl_uint>(precompute ? 7 : 6, slice);
-                queue.enqueueNDRangeKernel(kernel, cl::NullRange,
+                kernel.setArg<cl_uint>(precomputeSimple ? 6 : 5, pass);
+                kernel.setArg<cl_uint>(precomputeSimple ? 7 : 6, slice);
+                queue->enqueueNDRangeKernel(kernel, cl::NullRange,
                                            globalRange, localRange);
             }
         }
     } else {
-        queue.enqueueNDRangeKernel(kernel, cl::NullRange,
-                                   globalRange, localRange);
+        {
+            PerfScope p("enqueueNDRangeKernel");
+            queue->enqueueNDRangeKernel(kernel, cl::NullRange,
+                globalRange, localRange);
+        }
     }
 
-    queue.enqueueMarker(&end);
+#ifdef FLUSH_ALL
+   queue->flush();
+#endif
 }
 
-float KernelRunner::finish()
-{
-    end.wait();
+void KernelRunner::waitForResults() {
+    {
+        PerfScope p("end.wait()");
+        end.wait();
+    }
+}
+
+bool KernelRunner::resultsReady() {
+    cl_int err = NULL;
+    auto status = end.getInfo<CL_EVENT_COMMAND_EXECUTION_STATUS>(&err);
+    cl::detail::errHandler(err, "KernelRunner::resultsReady");
+
+    if (status == CL_COMPLETE) {
+        return true;
+    }
+    else {
+        return false;
+    }
+}
+
+uint32_t KernelRunner::getBlockCount() const {
+    if (optParams.customBlockCount)
+        return optParams.customBlockCount;
+    return params->getLanes() * params->getSegmentBlocks() * ARGON2_SYNC_POINTS;
+}
+
+size_t KernelRunner::getMemoryUsage() const {
+    size_t tot = totalBlocksSize(getBlockCount(), batchSize);
 
-    cl_ulong nsStart = start.getProfilingInfo<CL_PROFILING_COMMAND_END>();
-    cl_ulong nsEnd   = end.getProfilingInfo<CL_PROFILING_COMMAND_END>();
+    if (refsBuffer() != NULL) {
+        tot += totalRefsSize();
+    }
+    return tot;
+}
 
-    return (nsEnd - nsStart) / (1000.0F * 1000.0F);
+size_t KernelRunner::getMemoryUsedPerBatch() const {
+    return getBlockCount() * ARGON2_BLOCK_SIZE;
 }
 
 } // namespace opencl
diff --git a/lib/argon2-opencl/processingunit.cpp b/lib/argon2-opencl/processingunit.cpp
index 598ced7..85eba81 100644
--- a/lib/argon2-opencl/processingunit.cpp
+++ b/lib/argon2-opencl/processingunit.cpp
@@ -13,126 +13,199 @@ static bool isPowerOfTwo(std::uint32_t x)
     return (x & (x - 1)) == 0;
 }
 
+#pragma warning(disable:4267)
+#pragma warning(disable:4101)
+
+std::size_t ProcessingUnit::getStartBlocksSize() {
+    return params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
+}
+
 ProcessingUnit::ProcessingUnit(
         const ProgramContext *programContext, const Argon2Params *params,
         const Device *device, std::size_t batchSize,
-        bool bySegment, bool precomputeRefs)
+        bool bySegment, const t_optParams& optPrms)
     : programContext(programContext), params(params), device(device),
-      runner(programContext, params, device, batchSize, bySegment,
-             precomputeRefs),
+      runner(programContext, params, device, batchSize, bySegment, optPrms),
       bestLanesPerBlock(runner.getMinLanesPerBlock()),
       bestJobsPerBlock(runner.getMinJobsPerBlock())
 {
+    // preallocate pinned mem for ProcessingUnit::setPassword
+    auto context = programContext->getContext();
+    std::size_t size = batchSize * getStartBlocksSize();
+    setPasswordBuffer = cl::Buffer(
+            context, 
+            CL_MEM_READ_ONLY | CL_MEM_ALLOC_HOST_PTR, 
+            size);
+    bool blocking = true;
+    auto queue = runner.getQueue();
+    passwordsPtr = (uint8_t*)queue->enqueueMapBuffer(
+        setPasswordBuffer,
+        blocking,
+        CL_MAP_WRITE, 0, size);
+
     /* pre-fill first blocks with pseudo-random data: */
     for (std::size_t i = 0; i < batchSize; i++) {
         setPassword(i, NULL, 0);
     }
 
+    // preallocate pinned mem used for getting results
+    size = batchSize * (params->getLanes() * ARGON2_BLOCK_SIZE);
+    resultBuffer = cl::Buffer(
+        context,
+        CL_MEM_WRITE_ONLY | CL_MEM_ALLOC_HOST_PTR,
+        size);
+    
+    resultsPtr = (uint8_t*)queue->enqueueMapBuffer(
+        resultBuffer,
+        blocking,
+        CL_MAP_READ, 0, size);
+
+
+#if 0
     if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
-            && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
+        && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
+        //#ifndef NDEBUG
+        std::cout << "[INFO] Tuning lanes per block..." << std::endl;
+        //#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
-             lpb *= 2)
+            lpb *= 2)
         {
             float time;
             try {
-                runner.run(lpb, bestJobsPerBlock);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   OpenCL error on " << lpb
-                          << " lanes per block: " << ex.what() << std::endl;
-#endif
+                //runner.run(lpb, bestJobsPerBlock);
+                //time = runner.finish();
+            }
+            catch (cl::Error &ex) {
+                //#ifndef NDEBUG
+                std::cout << "[WARN]   OpenCL error on " << lpb
+                    << " lanes per block: " << ex.what() << std::endl;
+                //#endif
                 break;
             }
 
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << lpb << " lanes per block: "
-                      << time << " ms" << std::endl;
-#endif
+            //#ifndef NDEBUG
+            std::cout << "[INFO]   " << lpb << " lanes per block: "
+                << time << " ms" << std::endl;
+            //#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestLanesPerBlock = lpb;
             }
         }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestLanesPerBlock
-                  << " lanes per block." << std::endl;
-#endif
+        //#ifndef NDEBUG
+        std::cout << "[INFO] Picked " << bestLanesPerBlock
+            << " lanes per block." << std::endl;
+        //#endif
     }
 
     /* Only tune jobs per block if we hit maximum lanes per block: */
     if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
-            && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
-            && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
+        && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
+        && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
+
+        //#ifndef NDEBUG
         std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
+        //#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
-             jpb *= 2)
+            jpb *= 2)
         {
             float time;
             try {
-                runner.run(bestLanesPerBlock, jpb);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
+                //runner.run(bestLanesPerBlock, jpb);
+                //time = runner.finish();
+            }
+            catch (cl::Error &ex) {
+                //#ifndef NDEBUG
                 std::cerr << "[WARN]   OpenCL error on " << jpb
-                          << " jobs per block: " << ex.what() << std::endl;
-#endif
+                    << " jobs per block: " << ex.what() << std::endl;
+                //#endif
                 break;
             }
 
-#ifndef NDEBUG
+            //#ifndef NDEBUG
             std::cerr << "[INFO]   " << jpb << " jobs per block: "
-                      << time << " ms" << std::endl;
-#endif
+                << time << " ms" << std::endl;
+            //#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestJobsPerBlock = jpb;
             }
         }
-#ifndef NDEBUG
+
+
+        //bestJobsPerBlock = 128;
+
+
+        //#ifndef NDEBUG
         std::cerr << "[INFO] Picked " << bestJobsPerBlock
-                  << " jobs per block." << std::endl;
-#endif
+            << " jobs per block." << std::endl;
+        //#endif
     }
+#endif
 }
 
 void ProcessingUnit::setPassword(std::size_t index, const void *pw,
                                  std::size_t pwSize)
 {
-    void *memory = runner.mapInputMemory(index);
-    params->fillFirstBlocks(memory, pw, pwSize,
+    uint8_t* pData = passwordsPtr + index * getStartBlocksSize();
+
+    params->fillFirstBlocks(pData, pw, pwSize,
                             programContext->getArgon2Type(),
                             programContext->getArgon2Version());
-    runner.unmapInputMemory(memory);
+
+    runner.uploadToInputMemoryAsync(index, pData);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    void *memory = runner.mapOutputMemory(index);
-    params->finalize(hash, memory);
-    runner.unmapOutputMemory(memory);
+void ProcessingUnit::fetchResultAsync(std::size_t index) {
+    std::size_t size = (params->getLanes() * ARGON2_BLOCK_SIZE);
+    std::size_t offset = index * size;
+    uint8_t* pData = resultsPtr + offset;
+    runner.fetchOutputMemoryAsync(index, pData);
+   
+    runner.insertEndEventAndFlush();
 }
 
-void ProcessingUnit::beginProcessing()
+void ProcessingUnit::runKernelAsync()
 {
     runner.run(bestLanesPerBlock, bestJobsPerBlock);
 }
 
-void ProcessingUnit::endProcessing()
-{
-    runner.finish();
+void ProcessingUnit::waitForResults() {
+    runner.waitForResults();
 }
 
+bool ProcessingUnit::resultsReady() {
+    return runner.resultsReady();
+}
+
+uint8_t* ProcessingUnit::getResultPtr(int jobId) {
+    return resultsPtr + jobId * (params->getLanes() * ARGON2_BLOCK_SIZE);
+}
+
+void ProcessingUnit::reconfigureArgon(
+    const Argon2Params *newParams,
+    std::uint32_t newBatchSize,
+    const t_optParams &newOptParams) {
+    params = newParams;
+    runner.reconfigureArgon(device, params, newBatchSize, newOptParams);
+    bestLanesPerBlock = runner.getMinLanesPerBlock();
+    bestJobsPerBlock = runner.getMinJobsPerBlock();
+}
+
+size_t ProcessingUnit::getMemoryUsage() const {
+    return runner.getMemoryUsage();
+}
+
+size_t ProcessingUnit::getMemoryUsedPerBatch() const {
+    return runner.getMemoryUsedPerBatch();
+}
+
+
 } // namespace opencl
 } // namespace argon2
diff --git a/lib/argon2-opencl/programcontext.cpp b/lib/argon2-opencl/programcontext.cpp
index 67c2dc1..d1e1dc4 100644
--- a/lib/argon2-opencl/programcontext.cpp
+++ b/lib/argon2-opencl/programcontext.cpp
@@ -8,7 +8,7 @@ namespace opencl {
 ProgramContext::ProgramContext(
         const GlobalContext *globalContext,
         const std::vector<Device> &devices,
-        Type type, Version version, char* pathToKernel)
+        Type type, Version version, const char* pathToKernel)
     : globalContext(globalContext), devices(), type(type), version(version)
 {
     this->devices.reserve(devices.size());
