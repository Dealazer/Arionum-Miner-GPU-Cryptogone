diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3dd95ba..f5130e0 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1,4 +1,4 @@
-cmake_minimum_required(VERSION 3.7)
+cmake_minimum_required(VERSION 3.5)
 
 project(argon2-gpu CXX)
 set(BINARY_INSTALL_DIR /usr/local/bin)
@@ -28,9 +28,17 @@ if(CUDA_FOUND)
     )
 endif()
 
+FIND_PACKAGE(OpenCL)
+INCLUDE_DIRECTORIES(${OPENCL_INCLUDE_DIR})
+if (OPENCL_FOUND)
+    message("INFO: Using OPENCL version ${OpenCL_VERSION_MAJOR}.${OpenCL_VERSION_MINOR}")
+else()
+    message("INFO: OPENCL not found")
+endif()
+
 add_subdirectory(ext/argon2)
 
-add_library(argon2-gpu-common SHARED
+add_library(argon2-gpu-common STATIC
     lib/argon2-gpu-common/argon2params.cpp
     lib/argon2-gpu-common/blake2b.cpp
 )
@@ -44,7 +52,7 @@ target_include_directories(argon2-gpu-common PRIVATE
 )
 
 if(CUDA_FOUND)
-    cuda_add_library(argon2-cuda SHARED
+    cuda_add_library(argon2-cuda STATIC
         lib/argon2-cuda/device.cpp
         lib/argon2-cuda/globalcontext.cpp
         lib/argon2-cuda/programcontext.cpp
@@ -52,7 +60,7 @@ if(CUDA_FOUND)
         lib/argon2-cuda/kernels.cu
     )
 else()
-    add_library(argon2-cuda SHARED
+    add_library(argon2-cuda STATIC
         lib/argon2-cuda/nocuda.cpp
     )
 endif()
@@ -67,7 +75,7 @@ target_include_directories(argon2-cuda INTERFACE
 )
 target_link_libraries(argon2-cuda argon2-gpu-common)
 
-add_library(argon2-opencl SHARED
+add_library(argon2-opencl STATIC
     lib/argon2-opencl/device.cpp
     lib/argon2-opencl/globalcontext.cpp
     lib/argon2-opencl/kernelloader.cpp
@@ -84,56 +92,5 @@ target_include_directories(argon2-opencl PRIVATE
     lib/argon2-opencl
 )
 target_link_libraries(argon2-opencl
-    argon2-gpu-common -lOpenCL
-)
-
-add_executable(argon2-gpu-test
-    src/argon2-gpu-test/main.cpp
-    src/argon2-gpu-test/testcase.cpp
-)
-target_include_directories(argon2-gpu-test PRIVATE src/argon2-gpu-test)
-target_link_libraries(argon2-gpu-test
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_executable(argon2-gpu-bench
-    src/argon2-gpu-bench/cpuexecutive.cpp
-    src/argon2-gpu-bench/cudaexecutive.cpp
-    src/argon2-gpu-bench/openclexecutive.cpp
-    src/argon2-gpu-bench/benchmark.cpp
-    src/argon2-gpu-bench/main.cpp
-)
-target_include_directories(argon2-gpu-bench PRIVATE src/argon2-gpu-bench)
-target_link_libraries(argon2-gpu-bench
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_test(argon2-gpu-test-opencl argon2-gpu-test -m opencl)
-add_test(argon2-gpu-test-cuda argon2-gpu-test -m cuda)
-
-install(
-    TARGETS argon2-gpu-common argon2-opencl argon2-cuda
-    DESTINATION ${LIBRARY_INSTALL_DIR}
-)
-install(FILES
-    include/argon2-gpu-common/argon2-common.h
-    include/argon2-gpu-common/argon2params.h
-    include/argon2-opencl/cl.hpp
-    include/argon2-opencl/opencl.h
-    include/argon2-opencl/device.h
-    include/argon2-opencl/globalcontext.h
-    include/argon2-opencl/programcontext.h
-    include/argon2-opencl/processingunit.h
-    include/argon2-opencl/kernelrunner.h
-    include/argon2-cuda/cudaexception.h
-    include/argon2-cuda/kernels.h
-    include/argon2-cuda/device.h
-    include/argon2-cuda/globalcontext.h
-    include/argon2-cuda/programcontext.h
-    include/argon2-cuda/processingunit.h
-    DESTINATION ${INCLUDE_INSTALL_DIR}
-)
-install(
-    TARGETS argon2-gpu-bench argon2-gpu-test
-    DESTINATION ${BINARY_INSTALL_DIR}
+    argon2-gpu-common ${OpenCL_LIBRARY}
 )
diff --git a/data/kernels/argon2_kernel.cl b/data/kernels/argon2_kernel.cl
index fa5e12a..870f561 100644
--- a/data/kernels/argon2_kernel.cl
+++ b/data/kernels/argon2_kernel.cl
@@ -79,7 +79,7 @@ ulong u64_shuffle(ulong v, uint thread_src, uint thread,
     buf->lo[thread] = lo;
     buf->hi[thread] = hi;
 
-    barrier(CLK_LOCAL_MEM_FENCE);
+//    barrier(CLK_LOCAL_MEM_FENCE);
 
     lo = buf->lo[thread_src];
     hi = buf->hi[thread_src];
@@ -413,238 +413,77 @@ struct ref {
     uint ref_index;
 };
 
-/*
- * Refs hierarchy:
- * lanes -> passes -> slices -> blocks
- */
-__kernel void argon2_precompute_kernel(
-        __local struct u64_shuffle_buf *shuffle_bufs, __global struct ref *refs,
-        uint passes, uint lanes, uint segment_blocks)
-{
-    uint block_id = get_global_id(0) / THREADS_PER_LANE;
-    uint warp = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
-
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
-
-    uint segment_addr_blocks = (segment_blocks + ARGON2_QWORDS_IN_BLOCK - 1)
-            / ARGON2_QWORDS_IN_BLOCK;
-    uint block = block_id % segment_addr_blocks;
-    uint segment = block_id / segment_addr_blocks;
-
-    uint slice, pass, lane;
-#if ARGON2_TYPE == ARGON2_ID
-    slice = segment % (ARGON2_SYNC_POINTS / 2);
-    lane = segment / (ARGON2_SYNC_POINTS / 2);
-    pass = 0;
-#else
-    uint pass_id;
-
-    slice = segment % ARGON2_SYNC_POINTS;
-    pass_id = segment / ARGON2_SYNC_POINTS;
-
-    pass = pass_id % passes;
-    lane = pass_id / passes;
-#endif
-
-    struct block_th addr, tmp;
-
-    uint thread_input;
-    switch (thread) {
-    case 0:
-        thread_input = pass;
-        break;
-    case 1:
-        thread_input = lane;
-        break;
-    case 2:
-        thread_input = slice;
-        break;
-    case 3:
-        thread_input = lanes * segment_blocks * ARGON2_SYNC_POINTS;
-        break;
-    case 4:
-        thread_input = passes;
-        break;
-    case 5:
-        thread_input = ARGON2_TYPE;
-        break;
-    case 6:
-        thread_input = block + 1;
-        break;
-    default:
-        thread_input = 0;
-        break;
-    }
-
-    next_addresses(&addr, &tmp, thread_input, thread, shuffle_buf);
-
-    refs += segment * segment_blocks;
-
-    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
-        uint pos = i * THREADS_PER_LANE + thread;
-        uint offset = block * ARGON2_QWORDS_IN_BLOCK + pos;
-        if (offset < segment_blocks) {
-            ulong v = block_th_get(&addr, i);
-            uint ref_index = u64_lo(v);
-            uint ref_lane  = u64_hi(v);
-
-            compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
-                            &ref_lane, &ref_index);
-
-            refs[offset].ref_index = ref_index;
-            refs[offset].ref_lane  = ref_lane;
-        }
-    }
-}
-
-void argon2_step_precompute(
-        __global struct block_g *memory, __global struct block_g *mem_curr,
-        struct block_th *prev, struct block_th *tmp,
-        __local struct u64_shuffle_buf *shuffle_buf,
-        __global const struct ref **refs,
-        uint lanes, uint segment_blocks, uint thread,
-        uint lane, uint pass, uint slice, uint offset)
-{
-    uint ref_index, ref_lane;
-    bool data_independent;
-#if ARGON2_TYPE == ARGON2_I
-    data_independent = true;
-#elif ARGON2_TYPE == ARGON2_ID
-    data_independent = pass == 0 && slice < ARGON2_SYNC_POINTS / 2;
-#else
-    data_independent = false;
-#endif
-    if (data_independent) {
-        ref_index = (*refs)->ref_index;
-        ref_lane = (*refs)->ref_lane;
-        (*refs)++;
-    } else {
-        ulong v = u64_shuffle(prev->a, 0, thread, shuffle_buf);
-        ref_index = u64_lo(v);
-        ref_lane  = u64_hi(v);
-
-        compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
-                        &ref_lane, &ref_index);
-    }
-
-    argon2_core(memory, mem_curr, prev, tmp, shuffle_buf, lanes, thread, pass,
-                ref_index, ref_lane);
-}
-
-__kernel void argon2_kernel_segment_precompute(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, __global const struct ref *refs,
-        uint passes, uint lanes, uint segment_blocks,
-        uint pass, uint slice)
-{
-    uint job_id = get_global_id(1);
-    uint lane   = get_global_id(0) / THREADS_PER_LANE;
-    uint warp   = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
-
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
-
-    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
-
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
-
-    struct block_th prev, tmp;
-
-    __global struct block_g *mem_segment =
-            memory + slice * segment_blocks * lanes + lane;
-    __global struct block_g *mem_prev, *mem_curr;
-    uint start_offset = 0;
-    if (pass == 0) {
-        if (slice == 0) {
-            mem_prev = mem_segment + 1 * lanes;
-            mem_curr = mem_segment + 2 * lanes;
-            start_offset = 2;
-        } else {
-            mem_prev = mem_segment - lanes;
-            mem_curr = mem_segment;
-        }
-    } else {
-        mem_prev = mem_segment + (slice == 0 ? lane_blocks * lanes : 0) - lanes;
-        mem_curr = mem_segment;
-    }
-
-    load_block(&prev, mem_prev, thread);
-
-#if ARGON2_TYPE == ARGON2_ID
-        if (pass == 0 && slice < ARGON2_SYNC_POINTS / 2) {
-            refs += lane * (lane_blocks / 2) + slice * segment_blocks;
-            refs += start_offset;
-        }
-#else
-        refs += (lane * passes + pass) * lane_blocks + slice * segment_blocks;
-        refs += start_offset;
-#endif
-
-    for (uint offset = start_offset; offset < segment_blocks; ++offset) {
-        argon2_step_precompute(
-                    memory, mem_curr, &prev, &tmp, shuffle_buf, &refs, lanes,
-                    segment_blocks, thread, lane, pass, slice, offset);
+struct index {
+    uint refSlot;
+    uint store;
+    uint storeSlot;
+};
 
-        mem_curr += lanes;
-    }
-}
-
-__kernel void argon2_kernel_oneshot_precompute(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, __global const struct ref *refs,
-        uint passes, uint lanes, uint segment_blocks)
-{
-    uint job_id = get_global_id(1);
-    uint lane   = get_global_id(0) / THREADS_PER_LANE;
-    uint warp   = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
+__kernel void argon2_kernel_oneshot_precomputedIndex(
+    __local struct u64_shuffle_buf *shuffle_bufs,   // 0
+    __global struct block_g *memory,              // 1
+    //uint memory_0_count,                            // 2
+    //__global struct block_g *memory_1,              // 3
+    //uint memory_1_count,                            // 4
+    //__global struct block_g *memory_2,              // 5
+    //uint memory_2_count,                            // 6
+    //__global struct block_g *memory_3,              // 7
+    //uint memory_3_count,                            // 8
+    __global const struct index *indexs,            // 9
+    uint nSteps,                                    // 10
+    uint blocksPerBatch)                            // 11
+{
+    uint thread = get_local_id(0) % THREADS_PER_LANE; // [0-31]
+    uint warp = 0; // get_local_id(0) / THREADS_PER_LANE;
+    int batch_id = get_global_id(1);
 
     __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
 
-    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
-
     /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
-
+ //   __global struct block_g *memory;
+
+ /*   if (batch_id>=0 && batch_id < memory_0_count)
+ */       memory = memory + (size_t)(batch_id * blocksPerBatch);
+    
+ /*   batch_id -= memory_0_count;
+    if (batch_id >= 0 && batch_id < memory_1_count)
+        memory = memory_1 + (size_t)(batch_id * blocksPerBatch);
+    
+    batch_id -= memory_1_count;
+    if (batch_id >= 0 && batch_id < memory_2_count)
+        memory = memory_2 + (size_t)(size_t)(batch_id * blocksPerBatch);
+    
+    batch_id -= memory_2_count;
+    if (batch_id >= 0 && batch_id < memory_3_count)
+        memory = memory_3 + (size_t)(size_t)(batch_id * blocksPerBatch);
+ */  
+    /* hash */
     struct block_th prev, tmp;
+    load_block(&prev, memory + 1, thread);
 
-    __global struct block_g *mem_lane = memory + lane;
-    __global struct block_g *mem_prev = mem_lane + 1 * lanes;
-    __global struct block_g *mem_curr = mem_lane + 2 * lanes;
+    for (uint i = 2; i < nSteps; ++i) {
+        uint ref_index = indexs->refSlot;
+        __global struct block_g *mem_ref = memory + ref_index;
 
-    load_block(&prev, mem_prev, thread);
-
-#if ARGON2_TYPE == ARGON2_ID
-    refs += lane * (lane_blocks / 2) + 2;
-#else
-    refs += lane * passes * lane_blocks + 2;
-#endif
-
-    uint skip = 2;
-    for (uint pass = 0; pass < passes; ++pass) {
-        for (uint slice = 0; slice < ARGON2_SYNC_POINTS; ++slice) {
-            for (uint offset = 0; offset < segment_blocks; ++offset) {
-                if (skip > 0) {
-                    --skip;
-                    continue;
-                }
+        load_block_xor(&prev, mem_ref, thread);
+        move_block(&tmp, &prev);
 
-                argon2_step_precompute(
-                            memory, mem_curr, &prev, &tmp, shuffle_buf, &refs,
-                            lanes, segment_blocks, thread,
-                            lane, pass, slice, offset);
+        shuffle_block(&prev, thread, shuffle_buf);
 
-                mem_curr += lanes;
-            }
+        xor_block(&prev, &tmp);
 
-            barrier(CLK_LOCAL_MEM_FENCE);
+        uint store = indexs->store;
+        if (store != 0) {
+            __global struct block_g *mem_curr = memory + indexs->storeSlot;
+            store_block(mem_curr, &prev, thread);
         }
 
-        mem_curr = mem_lane;
+        indexs++;
     }
+
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
 }
+
 #endif /* ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID */
 
 void argon2_step(
@@ -692,12 +531,14 @@ void argon2_step(
                 ref_index, ref_lane);
 }
 
-__kernel void argon2_kernel_segment(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, uint passes, uint lanes,
-        uint segment_blocks, uint pass, uint slice)
+__kernel void argon2_kernel_oneshot(
+        __local struct u64_shuffle_buf *shuffle_bufs,   // 0
+        __global struct block_g *memory,                // 1
+        uint passes,                                    // 2
+        uint lanes,                                     // 3
+        uint segment_blocks)                            // 4     
 {
-    uint job_id = get_global_id(1);
+    uint batch_id = get_global_id(1);
     uint lane   = get_global_id(0) / THREADS_PER_LANE;
     uint warp   = get_local_id(0) / THREADS_PER_LANE;
     uint thread = get_local_id(0) % THREADS_PER_LANE;
@@ -707,90 +548,27 @@ __kernel void argon2_kernel_segment(
     uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
 
     /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+#if 1
+    memory += (size_t)(batch_id * lanes * lane_blocks);
+#else
+    //__global struct block_g *memory;
+    if (batch_id >= 0 && batch_id < memory_0_count)
+        memory = memory + (size_t)(batch_id * lanes * lane_blocks);
 
-    struct block_th prev, addr, tmp;
-    uint thread_input;
+    batch_id -= memory_0_count;
+    if (batch_id >= 0 && batch_id < memory_1_count)
+        memory = memory_1 + (size_t)(batch_id * lanes * lane_blocks);
 
-#if ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID
-    switch (thread) {
-    case 0:
-        thread_input = pass;
-        break;
-    case 1:
-        thread_input = lane;
-        break;
-    case 2:
-        thread_input = slice;
-        break;
-    case 3:
-        thread_input = lanes * lane_blocks;
-        break;
-    case 4:
-        thread_input = passes;
-        break;
-    case 5:
-        thread_input = ARGON2_TYPE;
-        break;
-    default:
-        thread_input = 0;
-        break;
-    }
+    batch_id -= memory_1_count;
+    if (batch_id >= 0 && batch_id < memory_2_count)
+        memory = memory_2 + (size_t)(batch_id * lanes * lane_blocks);
 
-    if (pass == 0 && slice == 0 && segment_blocks > 2) {
-        if (thread == 6) {
-            ++thread_input;
-        }
-        next_addresses(&addr, &tmp, thread_input, thread, shuffle_buf);
-    }
+    batch_id -= memory_2_count;
+    if (batch_id >= 0 && batch_id < memory_3_count)
+        memory = memory_3 + (size_t)(batch_id * lanes * lane_blocks);
 #endif
 
-    __global struct block_g *mem_segment =
-            memory + slice * segment_blocks * lanes + lane;
-    __global struct block_g *mem_prev, *mem_curr;
-    uint start_offset = 0;
-    if (pass == 0) {
-        if (slice == 0) {
-            mem_prev = mem_segment + 1 * lanes;
-            mem_curr = mem_segment + 2 * lanes;
-            start_offset = 2;
-        } else {
-            mem_prev = mem_segment - lanes;
-            mem_curr = mem_segment;
-        }
-    } else {
-        mem_prev = mem_segment + (slice == 0 ? lane_blocks * lanes : 0) - lanes;
-        mem_curr = mem_segment;
-    }
-
-    load_block(&prev, mem_prev, thread);
-
-    for (uint offset = start_offset; offset < segment_blocks; ++offset) {
-        argon2_step(memory, mem_curr, &prev, &tmp, &addr, shuffle_buf,
-                    lanes, segment_blocks, thread, &thread_input,
-                    lane, pass, slice, offset);
-
-        mem_curr += lanes;
-    }
-}
-
-__kernel void argon2_kernel_oneshot(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, uint passes, uint lanes,
-        uint segment_blocks)
-{
-    uint job_id = get_global_id(1);
-    uint lane   = get_global_id(0) / THREADS_PER_LANE;
-    uint warp   = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
-
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
-
-    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
-
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
-
+    /* hash */
     struct block_th prev, addr, tmp;
     uint thread_input;
 
diff --git a/include/argon2-cuda/cudaexception.h b/include/argon2-cuda/cudaexception.h
index ebc8460..92d44b6 100644
--- a/include/argon2-cuda/cudaexception.h
+++ b/include/argon2-cuda/cudaexception.h
@@ -6,6 +6,8 @@
 #endif
 
 #include <exception>
+#include <stdio.h>
+#include <iostream>
 
 namespace argon2 {
 namespace cuda {
@@ -27,6 +29,7 @@ public:
     static void check(cudaError_t res)
     {
         if (res != cudaSuccess) {
+            printf("CUDA exception => |%s|\n", cudaGetErrorString(res));
             throw CudaException(res);
         }
     }
diff --git a/include/argon2-cuda/kernels.h b/include/argon2-cuda/kernels.h
index 16418b4..167d99c 100644
--- a/include/argon2-cuda/kernels.h
+++ b/include/argon2-cuda/kernels.h
@@ -5,12 +5,15 @@
 
 #include <cuda_runtime.h>
 #include <cstdint>
+#include "../../include/argon2-gpu-common/argon2-common.h"
 
 /* workaround weird CMake/CUDA bug: */
 #ifdef argon2
 #undef argon2
 #endif
 
+#define REUSE_BUFFERS (1)
+
 namespace argon2 {
 namespace cuda {
 
@@ -21,41 +24,53 @@ private:
     std::uint32_t passes, lanes, segmentBlocks;
     std::uint32_t batchSize;
     bool bySegment;
-    bool precompute;
+    
+    t_optParams optParams;
 
     cudaEvent_t start, end;
     cudaStream_t stream;
+
+#if REUSE_BUFFERS
+    size_t blocksBufferSize;
+    size_t refsBufferSize;
+    const uint32_t* lastPrecomputeAddress;
+#endif
+
     void *memory;
     void *refs;
+   
+    uint32_t getBlockCountPerHash() const;
 
-    void precomputeRefs();
-
-    void runKernelSegment(std::uint32_t lanesPerBlock,
-                          std::uint32_t jobsPerBlock,
-                          std::uint32_t pass, std::uint32_t slice);
-    void runKernelOneshot(std::uint32_t lanesPerBlock,
-                          std::uint32_t jobsPerBlock);
+    void runKernelOneshot(std::uint32_t lanesPerBlock);
 
 public:
+    KernelRunner(std::uint32_t type, std::uint32_t version,
+                 std::uint32_t passes, std::uint32_t lanes,
+                 std::uint32_t segmentBlocks, std::uint32_t batchSize,
+                 bool bySegment, t_optParams optParams);
+
+    ~KernelRunner();
+
     std::uint32_t getMinLanesPerBlock() const { return bySegment ? 1 : lanes; }
     std::uint32_t getMaxLanesPerBlock() const { return lanes; }
-
     std::uint32_t getMinJobsPerBlock() const { return 1; }
     std::uint32_t getMaxJobsPerBlock() const { return batchSize; }
-
     std::uint32_t getBatchSize() const { return batchSize; }
 
-    KernelRunner(std::uint32_t type, std::uint32_t version,
-                 std::uint32_t passes, std::uint32_t lanes,
-                 std::uint32_t segmentBlocks, std::uint32_t batchSize,
-                 bool bySegment, bool precompute);
-    ~KernelRunner();
-
     void writeInputMemory(std::uint32_t jobId, const void *buffer);
     void readOutputMemory(std::uint32_t jobId, void *buffer);
+    void syncStream();
+    bool streamOperationsComplete();
 
-    void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
+    void run(std::uint32_t lanesPerBlock);
     float finish();
+
+    void reconfigureArgon(
+        std::uint32_t passes,
+        std::uint32_t lanes,
+        std::uint32_t segmentBlocks,
+        std::uint32_t newBatchSize,
+        const t_optParams &optParams);
 };
 
 } // cuda
diff --git a/include/argon2-cuda/processingunit.h b/include/argon2-cuda/processingunit.h
index 2109154..c196f59 100644
--- a/include/argon2-cuda/processingunit.h
+++ b/include/argon2-cuda/processingunit.h
@@ -22,20 +22,31 @@ private:
     KernelRunner runner;
     std::uint32_t bestLanesPerBlock;
     std::uint32_t bestJobsPerBlock;
+    std::vector<uint8_t*> setPasswordBuffers;
 
 public:
     std::size_t getBatchSize() const { return runner.getBatchSize(); }
 
     ProcessingUnit(
             const ProgramContext *programContext, const Argon2Params *params,
-            const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false);
+            const Device *device, std::size_t batchSize, bool bySegment,
+            const t_optParams &optPrms);
 
     void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
+
+    void fetchResultAsync(std::size_t index, void *dest);
+    void syncStream();
+    bool streamOperationsComplete();
 
     void beginProcessing();
     void endProcessing();
+    void reconfigureArgon(
+        const Argon2Params *newParams,
+        std::uint32_t batchSize,
+        const t_optParams &optParams);
+
+    size_t getMemoryUsage() const;
+    size_t getMemoryUsedPerBatch() const;
 };
 
 } // namespace cuda
diff --git a/include/argon2-gpu-common/argon2-common.h b/include/argon2-gpu-common/argon2-common.h
index fbcf67c..fbf268b 100644
--- a/include/argon2-gpu-common/argon2-common.h
+++ b/include/argon2-gpu-common/argon2-common.h
@@ -1,6 +1,8 @@
 #ifndef ARGON2COMMON_H
 #define ARGON2COMMON_H
 
+#include <cstddef>
+
 namespace argon2 {
 
 enum {
@@ -21,6 +23,42 @@ enum Version {
     ARGON2_VERSION_13 = 0x13,
 };
 
+enum OPT_MODE {
+    BASELINE = 0,
+    PRECOMPUTE = 1
+};
+
+typedef struct OptParams {
+    OPT_MODE mode;
+    uint32_t customBlockCount;
+    const uint32_t* customIndex;
+    uint32_t customIndexNbSteps;
+
+    OptParams() : 
+        mode(BASELINE), customBlockCount(0), 
+        customIndex(nullptr), customIndexNbSteps(0) {
+    }
+}t_optParams;
+
+#define BLOCK_TYPE_COUNT (2)
+#define MAX_BLOCKS_BUFFERS (4)
+
+struct MemConfig {
+    size_t batchSizes[BLOCK_TYPE_COUNT][MAX_BLOCKS_BUFFERS];
+    uint32_t blocksBuffers[MAX_BLOCKS_BUFFERS];
+    size_t index;
+    size_t in;
+    size_t out;
+
+    size_t getTotalHashes(int blockType) const {
+        size_t total = 0;
+        for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+            total += batchSizes[blockType][i];
+        }
+        return total;
+    }
+};
+
 } // namespace argon2
 
 
diff --git a/include/argon2-gpu-common/argon2params.h b/include/argon2-gpu-common/argon2params.h
index bf2ca75..5756c1f 100644
--- a/include/argon2-gpu-common/argon2params.h
+++ b/include/argon2-gpu-common/argon2params.h
@@ -5,6 +5,8 @@
 
 #include "argon2-common.h"
 
+#define OPEN_CL_SKIP_MEM_TRANSFERS (0)
+
 namespace argon2 {
 
 class Argon2Params
diff --git a/include/argon2-opencl/cl.hpp b/include/argon2-opencl/cl.hpp
index ced34f5..2b228e6 100644
--- a/include/argon2-opencl/cl.hpp
+++ b/include/argon2-opencl/cl.hpp
@@ -142,6 +142,14 @@
  * \endcode
  *
  */
+
+#ifdef _WIN32
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+#include <iostream>
+
 #ifndef CL_HPP_
 #define CL_HPP_
 
@@ -318,7 +326,7 @@ public:
 #define __ERR_STR(x) NULL
 #endif // __CL_ENABLE_EXCEPTIONS
 
-
+extern bool s_logCLErrors;
 namespace detail
 {
 #if defined(__CL_ENABLE_EXCEPTIONS)
@@ -327,7 +335,9 @@ static inline cl_int errHandler (
     const char * errStr = NULL)
 {
     if (err != CL_SUCCESS) {
-        throw Error(err, errStr);
+        if (s_logCLErrors)
+            printf("OpenCL error: errCode=%d str=%s\n", err, errStr ? errStr: "none");
+	    throw Error(err, errStr);
     }
     return err;
 }
@@ -3155,6 +3165,8 @@ public:
         void* host_ptr = NULL,
         cl_int* err = NULL)
     {
+        printf("====> Create Buffer, size=%llu, flags=%x\n", size, (unsigned int)flags);
+
         cl_int error;
         object_ = ::clCreateBuffer(context(), flags, size, host_ptr, &error);
 
diff --git a/include/argon2-opencl/kernelrunner.h b/include/argon2-opencl/kernelrunner.h
index a1bba8b..69fc98e 100644
--- a/include/argon2-opencl/kernelrunner.h
+++ b/include/argon2-opencl/kernelrunner.h
@@ -3,53 +3,87 @@
 
 #include "programcontext.h"
 #include "argon2-gpu-common/argon2params.h"
+#include <vector>
 
 namespace argon2 {
 namespace opencl {
 
-class KernelRunner
+class DeviceRessources
 {
+public:
+    DeviceRessources() {};
+    DeviceRessources(const Device *device);
+
+    static DeviceRessources& get(const Device *device);
+    
+    cl::Buffer* addBuffer(cl_mem_flags flags, size_t size);
+    cl::Buffer* getBuffer(cl_mem_flags flags, size_t size);
+
+    cl::CommandQueue* addQueue();
+    cl::CommandQueue* getQueue(int index);
+
 private:
+    struct BufferInfo {
+        cl_mem_flags flags;
+        size_t size;
+        cl::Buffer buf;
+    };
+
+    std::vector<cl::CommandQueue> queues;
+    std::vector<BufferInfo> buffers;
+    cl::Device device;
+    cl_device_id id;
+    cl::Context context;
+};
+
+class KernelRunner
+{
+public:
     const ProgramContext *programContext;
     const Argon2Params *params;
+    const MemConfig &memConfig;
 
-    std::uint32_t batchSize;
-    bool bySegment;
-    bool precompute;
-
-    cl::CommandQueue queue;
+private:
+    int curBlockType;
+    t_optParams optParams;
     cl::Kernel kernel;
-    cl::Buffer memoryBuffer, refsBuffer;
-    cl::Event start, end;
-
-    std::size_t memorySize;
-
-    void precomputeRefs();
+    cl::Buffer blocksBuffers[MAX_BLOCKS_BUFFERS], indexBuffer;
+    cl::CommandQueue *queue;
+    cl::Event end;
+    
+    void setKernelsArgs();
+    void setupKernel(const Device *device);
+    uint32_t getBlockCountPerHash() const;
 
 public:
-    std::uint32_t getMinLanesPerBlock() const
-    {
-        return bySegment ? 1 : params->getLanes();
-    }
+    KernelRunner(
+        const Device *device,
+        const Argon2Params *params, 
+        const ProgramContext *programContext,
+        const MemConfig &mcfg,
+        t_optParams opt,
+        int curBlockType);
+
+    void reconfigureArgon(
+        const Device *device,
+        const Argon2Params *newParams,
+        const t_optParams &newOptParams,
+        int blockType);
+
+    void uploadToInputMemoryAsync(const void* srcPtr);
+    void fetchOutputMemoryAsync(uint8_t* dstPtr);
+    void insertEndEventAndFlush();
+    void run(std::uint32_t lanesPerBlock);
+    void waitForResults();
+    bool resultsReady();
+
+    std::uint32_t getMinLanesPerBlock() const { return params->getLanes(); }
     std::uint32_t getMaxLanesPerBlock() const { return params->getLanes(); }
-
     std::uint32_t getMinJobsPerBlock() const { return 1; }
-    std::uint32_t getMaxJobsPerBlock() const { return batchSize; }
-
-    std::uint32_t getBatchSize() const { return batchSize; }
-
-    KernelRunner(const ProgramContext *programContext,
-                 const Argon2Params *params, const Device *device,
-                 std::uint32_t batchSize, bool bySegment, bool precompute);
-
-    void *mapInputMemory(std::uint32_t jobId);
-    void unmapInputMemory(void *memory);
-
-    void *mapOutputMemory(std::uint32_t jobId);
-    void unmapOutputMemory(void *memory);
-
-    void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
-    float finish();
+    std::uint32_t getHashesPerRun() const;
+    cl::CommandQueue *getQueue() { return queue; }
+    int getCurrentBlockType() const { return curBlockType; };
+    std::size_t getStartBlocksSize();
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/processingunit.h b/include/argon2-opencl/processingunit.h
index 277272d..e96167c 100644
--- a/include/argon2-opencl/processingunit.h
+++ b/include/argon2-opencl/processingunit.h
@@ -11,27 +11,40 @@ namespace opencl {
 class ProcessingUnit
 {
 private:
-    const ProgramContext *programContext;
-    const Argon2Params *params;
     const Device *device;
 
     KernelRunner runner;
     std::uint32_t bestLanesPerBlock;
     std::uint32_t bestJobsPerBlock;
 
-public:
-    std::size_t getBatchSize() const { return runner.getBatchSize(); }
-
-    ProcessingUnit(
-            const ProgramContext *programContext, const Argon2Params *params,
-            const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false);
+    cl::Buffer inBuffer;
+    uint8_t* inPtr;
 
-    void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
+    cl::Buffer outBuffer;
+    uint8_t* outPtr;
 
-    void beginProcessing();
-    void endProcessing();
+public:
+    ProcessingUnit(
+            const Device *device,
+            const ProgramContext *programContext,
+            const Argon2Params *params,
+            const MemConfig &mcfg,
+            const t_optParams& optPrms,
+            int curBlockType);
+
+    void reconfigureArgon(
+        const Device *device,
+        const Argon2Params *newParams,
+        const t_optParams &newOptParams,
+        int blockType);
+
+    void uploadInputDataAsync(const std::vector<std::string>& bases);
+    void runKernelAsync();
+    void waitForResults();
+    bool resultsReady();
+    void fetchResultsAsync();
+    
+    uint8_t* getResultPtr(int jobId);
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/programcontext.h b/include/argon2-opencl/programcontext.h
index 45d7ab3..a7fd166 100644
--- a/include/argon2-opencl/programcontext.h
+++ b/include/argon2-opencl/programcontext.h
@@ -37,7 +37,7 @@ public:
     ProgramContext(
             const GlobalContext *globalContext,
             const std::vector<Device> &devices,
-            Type type, Version version, char *pathToKernel);
+            Type type, Version version, const char *pathToKernel);
 };
 
 } // namespace opencl
diff --git a/lib/argon2-cuda/globalcontext.cpp b/lib/argon2-cuda/globalcontext.cpp
index eb52e02..56040c9 100644
--- a/lib/argon2-cuda/globalcontext.cpp
+++ b/lib/argon2-cuda/globalcontext.cpp
@@ -25,7 +25,11 @@ GlobalContext::GlobalContext()
     : devices()
 {
     int count;
-    CudaException::check(cudaGetDeviceCount(&count));
+    cudaError_t res = cudaGetDeviceCount(&count);
+    if (res != cudaSuccess) {
+        std::cout << "Cannot get CUDA device count, aborting...." << std::endl;
+        exit(1);
+    }
 
     devices.reserve(count);
     for (int i = 0; i < count; i++) {
diff --git a/lib/argon2-cuda/kernels.cu b/lib/argon2-cuda/kernels.cu
index 79fb767..cdeac97 100644
--- a/lib/argon2-cuda/kernels.cu
+++ b/lib/argon2-cuda/kernels.cu
@@ -3,8 +3,11 @@
 #define __CUDACC__
 #endif
 
-#include "kernels.h"
-#include "cudaexception.h"
+#include <cuda_runtime.h>
+
+#include "../../include/argon2-cuda/kernels.h"
+#include "../../include/argon2-cuda/cudaexception.h"
+#include "../../../include/perfscope.h"
 
 #include <stdexcept>
 #ifndef NDEBUG
@@ -49,8 +52,8 @@ __device__ uint64_t u64_shuffle(uint64_t v, uint32_t thread)
 {
     uint32_t lo = u64_lo(v);
     uint32_t hi = u64_hi(v);
-    lo = __shfl(lo, thread);
-    hi = __shfl(hi, thread);
+    lo = __shfl_sync(0xFFFFFFFF, lo, thread);
+    hi = __shfl_sync(0xFFFFFFFF, hi, thread);
     return u64_build(hi, lo);
 }
 
@@ -269,6 +272,7 @@ __device__ void next_addresses(struct block_th *addr, struct block_th *tmp,
     xor_block(addr, tmp);
 }
 
+// ~= index_alpha in ref code ...
 __device__ void compute_ref_pos(
         uint32_t lanes, uint32_t segment_blocks,
         uint32_t pass, uint32_t lane, uint32_t slice, uint32_t offset,
@@ -309,6 +313,12 @@ struct ref {
     uint32_t ref_index;
 };
 
+struct index {
+    uint32_t refSlot;
+    uint32_t store;
+    uint32_t storeSlot;
+};
+
 /*
  * Refs hierarchy:
  * lanes -> passes -> slices -> blocks
@@ -415,6 +425,25 @@ __device__ void argon2_core(
     store_block(mem_curr, prev, thread);
 }
 
+template<uint32_t version>
+__device__ void argon2_core_precomputedIndex(
+    struct block_g *memory, struct block_g *mem_curr,
+    struct block_th *prev, struct block_th *tmp,
+    uint32_t thread, uint32_t ref_index)
+{
+    struct block_g *mem_ref = memory + ref_index;
+
+    load_block_xor(prev, mem_ref, thread);
+    move_block(tmp, prev);
+
+    shuffle_block(prev, thread);
+
+    xor_block(prev, tmp);
+
+    if (mem_curr != 0)
+        store_block(mem_curr, prev, thread);
+}
+
 template<uint32_t type, uint32_t version>
 __device__ void argon2_step_precompute(
         struct block_g *memory, struct block_g *mem_curr,
@@ -548,6 +577,33 @@ __global__ void argon2_kernel_oneshot_precompute(
     }
 }
 
+template<uint32_t type, uint32_t version>
+__global__ void argon2_kernel_oneshot_precomputedIndex(
+    struct block_g *memory, const struct index *indexs, 
+    uint32_t nSteps, uint32_t blocksPerBatch)
+{
+    struct block_th prev, tmp;
+    uint32_t batch_id = blockIdx.z; // nBatches
+    uint32_t thread = threadIdx.x; // 32
+
+    memory += (size_t)batch_id * blocksPerBatch;
+
+    load_block(&prev, memory + 1, thread);
+    for (uint32_t i = 2; i < nSteps; ++i) {
+        uint32_t store = indexs->store;
+        uint32_t storeSlot = indexs->storeSlot;
+        uint32_t ref_index = indexs->refSlot;
+
+        struct block_g *mem_curr = store ? (memory + storeSlot) : 0;
+        argon2_core_precomputedIndex<version>(
+            memory, mem_curr, &prev, &tmp, thread, ref_index);
+
+        indexs++;
+    }
+
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
+}
+
 template<uint32_t type, uint32_t version>
 __device__ void argon2_step(
         struct block_g *memory, struct block_g *mem_curr,
@@ -757,261 +813,259 @@ __global__ void argon2_kernel_oneshot(
     }
 }
 
+void cudaSafeFree(void* &ptr) {
+    if (ptr)
+        cudaFree(ptr);
+    ptr = nullptr;
+}
+
 KernelRunner::KernelRunner(uint32_t type, uint32_t version, uint32_t passes,
                            uint32_t lanes, uint32_t segmentBlocks,
-                           uint32_t batchSize, bool bySegment, bool precompute)
-    : type(type), version(version), passes(passes), lanes(lanes),
+                           uint32_t batchSize, bool bySegment, t_optParams opt)
+    : type(type), version(version), passes(passes), lanes(lanes), optParams(opt),
       segmentBlocks(segmentBlocks), batchSize(batchSize), bySegment(bySegment),
-      precompute(precompute), stream(nullptr), memory(nullptr),
+      stream(nullptr), memory(nullptr),
       refs(nullptr), start(nullptr), end(nullptr)
-{
-    // FIXME: check overflow:
-    size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE * batchSize;
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
+#if REUSE_BUFFERS
+      , blocksBufferSize(0)
+      , refsBufferSize(0)
+      , lastPrecomputeAddress(nullptr)
 #endif
-
-    CudaException::check(cudaMalloc(&memory, memorySize));
-
-    CudaException::check(cudaEventCreate(&start));
-    CudaException::check(cudaEventCreate(&end));
-
+{
     CudaException::check(cudaStreamCreate(&stream));
 
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
-        uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
-
-        size_t refsSize = segments * segmentBlocks * sizeof(struct ref);
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
-#endif
-
-        CudaException::check(cudaMalloc(&refs, refsSize));
-
-        precomputeRefs();
-        CudaException::check(cudaStreamSynchronize(stream));
-    }
+    initializeBuffers();
 }
 
-void KernelRunner::precomputeRefs()
+void KernelRunner::reconfigureArgon(
+    std::uint32_t newPasses,
+    std::uint32_t newLanes,
+    std::uint32_t newSegmentBlocks,
+    std::uint32_t newBatchSize,
+    const t_optParams &newOptParams)
 {
-    struct ref *refs = (struct ref *)this->refs;
+    passes = newPasses;
+    lanes = newLanes;
+    segmentBlocks = newSegmentBlocks;
+    batchSize = newBatchSize;
+    optParams = newOptParams;
+    
+#if REUSE_BUFFERS
+    initializeBuffers();
+#else
+    freeBuffers();
+    initializeBuffers();
+#endif
+}
 
-    uint32_t segmentAddrBlocks = (segmentBlocks + ARGON2_QWORDS_IN_BLOCK - 1)
-            / ARGON2_QWORDS_IN_BLOCK;
-    uint32_t segments =
-            type == ARGON2_ID
-            ? lanes * (ARGON2_SYNC_POINTS / 2)
-            : passes * lanes * ARGON2_SYNC_POINTS;
-
-    dim3 blocks = dim3(1, segments * segmentAddrBlocks);
-    dim3 threads = dim3(THREADS_PER_LANE);
-
-    if (type == ARGON2_I) {
-        argon2_precompute_kernel<ARGON2_I>
-            <<<blocks, threads, 0, stream>>>(
-                refs, passes, lanes, segmentBlocks);
-    } else {
-        argon2_precompute_kernel<ARGON2_ID>
-            <<<blocks, threads, 0, stream>>>(
-                refs, passes, lanes, segmentBlocks);
-    }
+uint32_t KernelRunner::getBlockCount() const {
+    if (optParams.customBlockCount)
+        return optParams.customBlockCount;
+    return lanes * segmentBlocks * ARGON2_SYNC_POINTS;
 }
 
+//size_t totalBlocksSize(uint32_t nBlocks, uint32_t batchSize) {
+//    return nBlocks * batchSize * ARGON2_BLOCK_SIZE;
+//}
+
+//size_t KernelRunner::totalRefsSize() const {
+//    if ((type == ARGON2_I || type == ARGON2_ID) &&
+//        optParams.mode == PRECOMPUTE_SIMPLE) {
+//        uint32_t segments =
+//            type == ARGON2_ID
+//            ? lanes * (ARGON2_SYNC_POINTS / 2)
+//            : passes * lanes * ARGON2_SYNC_POINTS;
+//        size_t refsSize = 
+//            segments * segmentBlocks * sizeof(struct ref);
+//        return refsSize;
+//    }
+//    else if (type == ARGON2_I &&
+//        optParams.mode == PRECOMPUTE) {
+//        return optParams.customIndexNbSteps * sizeof(struct index);
+//    }
+//    return 0;
+//}
+
+//void KernelRunner::initializeBuffers() {
+//#if REUSE_BUFFERS
+//    // blocks buffer
+//    size_t blocksSize = totalBlocksSize(getBlockCount(), batchSize);
+//    if (blocksBufferSize < blocksSize)
+//    {
+//        PERFSCOPE("Cuda realloc blocks buffer");
+//        cudaSafeFree(memory);
+//        CudaException::check(cudaMalloc(&memory, blocksSize));
+//        blocksBufferSize = blocksSize;
+//    }
+//
+//    // refs / index buffer
+//    size_t refsSize = totalRefsSize();
+//    if (refsSize && refsBufferSize < refsSize) {
+//        {
+//            PERFSCOPE("Cuda realloc refs buffer");
+//            cudaSafeFree(refs);
+//            CudaException::check(cudaMalloc(&refs, refsSize));
+//            refsBufferSize = refsSize;
+//        }
+//        if (optParams.mode == PRECOMPUTE_SIMPLE) {
+//            precomputeRefs();
+//            CudaException::check(cudaStreamSynchronize(stream));
+//        }
+//    }
+//
+//    if (optParams.mode == PRECOMPUTE && optParams.customIndex != lastPrecomputeAddress) {
+//        PERFSCOPE("Cuda memcpy index");
+//        cudaMemcpy(refs, optParams.customIndex, refsSize, cudaMemcpyHostToDevice);
+//        lastPrecomputeAddress = optParams.customIndex;
+//    }
+//#else
+//    // blocks buffer
+//    {
+//        PERFSCOPE("Cuda alloc blocks buffer");
+//        size_t blocksSize = totalBlocksSize(getBlockCount(), batchSize);
+//        CudaException::check(cudaMalloc(&memory, blocksSize));
+//    }
+//    
+//    // refs / index buffer
+//    size_t refsSize = totalRefsSize();
+//    if (refsSize) {
+//        {
+//            PERFSCOPE("Cuda alloc refs buffer");
+//            CudaException::check(cudaMalloc(&refs, refsSize));
+//        }
+//        if (optParams.mode == PRECOMPUTE_SIMPLE) {
+//            precomputeRefs();
+//            CudaException::check(cudaStreamSynchronize(stream));
+//        }
+//        else if (optParams.mode == PRECOMPUTE) {
+//            PERFSCOPE("Cuda memcpy index");
+//            cudaMemcpy(refs, optParams.customIndex, refsSize, cudaMemcpyHostToDevice);
+//        }
+//    }
+//#endif
+//}
+//
 KernelRunner::~KernelRunner()
 {
-    if (start != nullptr) {
-        cudaEventDestroy(start);
-    }
-    if (end != nullptr) {
-        cudaEventDestroy(end);
-    }
+    cudaSafeFree(refs);
+    cudaSafeFree(memory);
     if (stream != nullptr) {
         cudaStreamDestroy(stream);
     }
-    if (memory != nullptr) {
-        cudaFree(memory);
+}
+
+//size_t KernelRunner::getMemoryUsage() const {
+//    size_t tot = totalBlocksSize(getBlockCount(), batchSize);
+//    if (refs) {
+//        tot += totalRefsSize();
+//    }
+//    return tot;
+//}
+
+//size_t KernelRunner::getMemoryUsedPerBatch() const {
+//    return getBlockCount() * ARGON2_BLOCK_SIZE;
+//}
+
+//void KernelRunner::precomputeRefs()
+//{
+//    struct ref *refs = (struct ref *)this->refs;
+//
+//    uint32_t segmentAddrBlocks = (segmentBlocks + ARGON2_QWORDS_IN_BLOCK - 1)
+//            / ARGON2_QWORDS_IN_BLOCK;
+//    uint32_t segments =
+//            type == ARGON2_ID
+//            ? lanes * (ARGON2_SYNC_POINTS / 2)
+//            : passes * lanes * ARGON2_SYNC_POINTS;
+//
+//    dim3 blocks = dim3(1, segments * segmentAddrBlocks);
+//    dim3 threads = dim3(THREADS_PER_LANE);
+//
+//    if (type == ARGON2_I) {
+//        argon2_precompute_kernel<ARGON2_I>
+//            <<<blocks, threads, 0, stream>>>(
+//                refs, passes, lanes, segmentBlocks);
+//    } else {
+//        argon2_precompute_kernel<ARGON2_ID>
+//            <<<blocks, threads, 0, stream>>>(
+//                refs, passes, lanes, segmentBlocks);
+//    }
+//}
+
+void KernelRunner::syncStream() {
+    CudaException::check(cudaStreamSynchronize(stream));
+}
+
+bool KernelRunner::streamOperationsComplete() {
+    cudaError_t res = cudaStreamQuery(stream);
+    if (res == cudaSuccess) {
+        return true;
     }
-    if (refs != nullptr) {
-        cudaFree(refs);
+    else if (res == cudaErrorNotReady) {
+        return false;
+    }
+    else {
+        CudaException::check(res);
+        return false;
     }
 }
 
 void KernelRunner::writeInputMemory(uint32_t jobId, const void *buffer)
 {
-    std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE;
+    std::size_t memorySize = getBlockCount() * ARGON2_BLOCK_SIZE;
     std::size_t size = static_cast<size_t>(lanes) * 2 * ARGON2_BLOCK_SIZE;
     std::size_t offset = memorySize * jobId;
     auto mem = static_cast<uint8_t *>(memory) + offset;
     CudaException::check(cudaMemcpyAsync(mem, buffer, size,
                                          cudaMemcpyHostToDevice, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
 }
 
 void KernelRunner::readOutputMemory(uint32_t jobId, void *buffer)
 {
-    std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE;
+    std::size_t memorySize = getBlockCount() * ARGON2_BLOCK_SIZE;
     std::size_t size = static_cast<size_t>(lanes) * ARGON2_BLOCK_SIZE;
     std::size_t offset = memorySize * (jobId + 1) - size;
     auto mem = static_cast<uint8_t *>(memory) + offset;
     CudaException::check(cudaMemcpyAsync(buffer, mem, size,
                                          cudaMemcpyDeviceToHost, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
 }
 
-void KernelRunner::runKernelSegment(uint32_t lanesPerBlock,
-                                    uint32_t jobsPerBlock,
-                                    uint32_t pass, uint32_t slice)
+void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock)
 {
-    if (lanesPerBlock > lanes || lanes % lanesPerBlock != 0) {
+    if (lanesPerBlock != lanes) {
         throw std::logic_error("Invalid lanesPerBlock!");
     }
 
-    if (jobsPerBlock > batchSize || batchSize % jobsPerBlock != 0) {
-        throw std::logic_error("Invalid jobsPerBlock!");
-    }
-
     struct block_g *memory_blocks = (struct block_g *)memory;
-    dim3 blocks = dim3(1, lanes / lanesPerBlock, batchSize / jobsPerBlock);
-    dim3 threads = dim3(THREADS_PER_LANE, lanesPerBlock, jobsPerBlock);
-    if (type == ARGON2_I) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment_precompute<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment_precompute<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        }
-    } else if (type == ARGON2_ID) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment_precompute<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment_precompute<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        }
-    } else {
+    dim3 blocks = dim3(1, 1, batchSize);
+    dim3 threads = dim3(THREADS_PER_LANE, lanes, 1);
+
+    if (type == ARGON2_I && optParams.mode == PRECOMPUTE) {
+        struct index *indexs = (struct index *)this->refs;
+        uint32_t nSteps = ARGON2_SYNC_POINTS * segmentBlocks;
+        uint32_t blocksPerBatch = getBlockCount();
+        argon2_kernel_oneshot_precomputedIndex<ARGON2_I, ARGON2_VERSION_13>
+            << <blocks, threads, 0, stream >> >(
+                memory_blocks, indexs, nSteps, blocksPerBatch);
+    }
+    else if (type == ARGON2_I) {
         if (version == ARGON2_VERSION_10) {
-            argon2_kernel_segment<ARGON2_D, ARGON2_VERSION_10>
+            argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_10>
                     <<<blocks, threads, 0, stream>>>(
-                        memory_blocks, passes, lanes, segmentBlocks,
-                        pass, slice);
+                        memory_blocks, passes, lanes, segmentBlocks);
         } else {
-            argon2_kernel_segment<ARGON2_D, ARGON2_VERSION_13>
+            argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_13>
                     <<<blocks, threads, 0, stream>>>(
-                        memory_blocks, passes, lanes, segmentBlocks,
-                        pass, slice);
-        }
-    }
-}
-
-void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
-                                    uint32_t jobsPerBlock)
-{
-    if (lanesPerBlock != lanes) {
-        throw std::logic_error("Invalid lanesPerBlock!");
-    }
-
-    if (jobsPerBlock > batchSize || batchSize % jobsPerBlock != 0) {
-        throw std::logic_error("Invalid jobsPerBlock!");
-    }
-
-    struct block_g *memory_blocks = (struct block_g *)memory;
-    dim3 blocks = dim3(1, 1, batchSize / jobsPerBlock);
-    dim3 threads = dim3(THREADS_PER_LANE, lanes, jobsPerBlock);
-    if (type == ARGON2_I) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot_precompute<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot_precompute<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            }
+                        memory_blocks, passes, lanes, segmentBlocks);
         }
     } else if (type == ARGON2_ID) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot_precompute<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot_precompute<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            }
+        if (version == ARGON2_VERSION_10) {
+            argon2_kernel_oneshot<ARGON2_ID, ARGON2_VERSION_10>
+                    <<<blocks, threads, 0, stream>>>(
+                        memory_blocks, passes, lanes, segmentBlocks);
         } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            }
+            argon2_kernel_oneshot<ARGON2_ID, ARGON2_VERSION_13>
+                    <<<blocks, threads, 0, stream>>>(
+                        memory_blocks, passes, lanes, segmentBlocks);
         }
     } else {
         if (version == ARGON2_VERSION_10) {
@@ -1026,33 +1080,15 @@ void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
     }
 }
 
-void KernelRunner::run(uint32_t lanesPerBlock, uint32_t jobsPerBlock)
+void KernelRunner::run(uint32_t lanesPerBlock)
 {
-    CudaException::check(cudaEventRecord(start, stream));
-
-    if (bySegment) {
-        for (uint32_t pass = 0; pass < passes; pass++) {
-            for (uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
-                runKernelSegment(lanesPerBlock, jobsPerBlock, pass, slice);
-            }
-        }
-    } else {
-        runKernelOneshot(lanesPerBlock, jobsPerBlock);
-    }
-
+    runKernelOneshot(lanesPerBlock);
     CudaException::check(cudaGetLastError());
-
-    CudaException::check(cudaEventRecord(end, stream));
 }
 
 float KernelRunner::finish()
 {
-    CudaException::check(cudaStreamSynchronize(stream));
-
-    float time = 0.0;
-    CudaException::check(cudaEventElapsedTime(&time, start, end));
-    return time;
+    return 0.f;
 }
-
 } // cuda
 } // argon2
diff --git a/lib/argon2-cuda/processingunit.cpp b/lib/argon2-cuda/processingunit.cpp
index 7768427..4159848 100644
--- a/lib/argon2-cuda/processingunit.cpp
+++ b/lib/argon2-cuda/processingunit.cpp
@@ -24,133 +24,95 @@ static bool isPowerOfTwo(std::uint32_t x)
     return (x & (x - 1)) == 0;
 }
 
+#pragma warning(disable:4267)
+#pragma warning(disable:4101)
+
 ProcessingUnit::ProcessingUnit(
         const ProgramContext *programContext, const Argon2Params *params,
         const Device *device, std::size_t batchSize, bool bySegment,
-        bool precomputeRefs)
+        const t_optParams& optPrms)
     : programContext(programContext), params(params), device(device),
       runner(programContext->getArgon2Type(),
              programContext->getArgon2Version(), params->getTimeCost(),
-             params->getLanes(), params->getSegmentBlocks(), batchSize,
-             bySegment, precomputeRefs),
+             params->getLanes(), params->getSegmentBlocks(),
+             batchSize, bySegment, optPrms),
       bestLanesPerBlock(runner.getMinLanesPerBlock()),
-      bestJobsPerBlock(runner.getMinJobsPerBlock())
 {
+    // already done by caller, but let's still do it again just in case ...
+    // (it is done by the caller because the runner constructor also needs current device set !)
     setCudaDevice(device->getDeviceIndex());
 
-    /* pre-fill first blocks with pseudo-random data: */
-    for (std::size_t i = 0; i < batchSize; i++) {
-        setPassword(i, NULL, 0);
-    }
-
-    if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
-            && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
-             lpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(lpb, bestJobsPerBlock);
-                time = runner.finish();
-            } catch(CudaException &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   CUDA error on " << lpb
-                          << " lanes per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
-
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << lpb << " lanes per block: "
-                      << time << " ms" << std::endl;
-#endif
-
-            if (time < bestTime) {
-                bestTime = time;
-                bestLanesPerBlock = lpb;
-            }
+    // preallocate buffers used by ProcessingUnit::setPassword
+    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
+    for (int i = 0; i < batchSize; i++) {
+        uint8_t* ptrPinnedMem = nullptr;
+        cudaError_t status = cudaMallocHost((void**)&(ptrPinnedMem), size);
+        if (status != cudaSuccess) {
+            std::cout << "Error allocating pinned host memory" << std::endl;
+            exit(1);
         }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestLanesPerBlock
-                  << " lanes per block." << std::endl;
-#endif
+        setPasswordBuffers.push_back(ptrPinnedMem);
     }
 
-    /* Only tune jobs per block if we hit maximum lanes per block: */
-    if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
-            && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
-            && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
-             jpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(bestLanesPerBlock, jpb);
-                time = runner.finish();
-            } catch(CudaException &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   CUDA error on " << jpb
-                          << " jobs per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
-
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << jpb << " jobs per block: "
-                      << time << " ms" << std::endl;
-#endif
-
-            if (time < bestTime) {
-                bestTime = time;
-                bestJobsPerBlock = jpb;
-            }
-        }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestJobsPerBlock
-                  << " jobs per block." << std::endl;
-#endif
+    /* pre-fill first blocks with pseudo-random data: */
+    for (std::size_t i = 0; i < batchSize; i++) {
+        setPassword(i, NULL, 0);
     }
 }
 
 void ProcessingUnit::setPassword(std::size_t index, const void *pw,
                                  std::size_t pwSize)
 {
-    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    params->fillFirstBlocks(buffer.get(), pw, pwSize,
+    params->fillFirstBlocks(setPasswordBuffers[index], pw, pwSize,
                             programContext->getArgon2Type(),
                             programContext->getArgon2Version());
-    runner.writeInputMemory(index, buffer.get());
+
+    runner.writeInputMemory(index, setPasswordBuffers[index]);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    std::size_t size = params->getLanes() * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    runner.readOutputMemory(index, buffer.get());
-    params->finalize(hash, buffer.get());
+void ProcessingUnit::fetchResultAsync(std::size_t index, void *dest) {
+    runner.readOutputMemory(index, dest);
 }
 
-void ProcessingUnit::beginProcessing()
-{
+void ProcessingUnit::syncStream() {
+    runner.syncStream();
+}
+
+bool ProcessingUnit::streamOperationsComplete() {
+    return runner.streamOperationsComplete();
+}
+
+void ProcessingUnit::beginProcessing() {
+    // we MUST set cuda device before launching a kernel
     setCudaDevice(device->getDeviceIndex());
-    runner.run(bestLanesPerBlock, bestJobsPerBlock);
+    runner.run(bestLanesPerBlock);
 }
 
-void ProcessingUnit::endProcessing()
-{
+void ProcessingUnit::endProcessing() {
     runner.finish();
 }
 
+void ProcessingUnit::reconfigureArgon(const Argon2Params *newParams, 
+                                      std::uint32_t batchSize,
+                                      const t_optParams &optParams) {
+    setCudaDevice(device->getDeviceIndex());
+    params = newParams;
+    runner.reconfigureArgon(
+        params->getTimeCost(),
+        params->getLanes(),
+        params->getSegmentBlocks(),
+        batchSize,
+        optParams);
+    bestLanesPerBlock = runner.getMinLanesPerBlock();
+}
+
+size_t ProcessingUnit::getMemoryUsage() const {
+    return runner.getMemoryUsage();
+}
+
+size_t ProcessingUnit::getMemoryUsedPerBatch() const {
+    return runner.getMemoryUsedPerBatch();
+}
+
 } // namespace cuda
 } // namespace argon2
diff --git a/lib/argon2-gpu-common/argon2params.cpp b/lib/argon2-gpu-common/argon2params.cpp
index dd122d8..b74c11d 100644
--- a/lib/argon2-gpu-common/argon2params.cpp
+++ b/lib/argon2-gpu-common/argon2params.cpp
@@ -20,6 +20,8 @@ static void store32(void *dst, std::uint32_t v)
     *out++ = static_cast<std::uint8_t>(v);
 }
 
+#pragma warning(disable:4267)
+
 Argon2Params::Argon2Params(
         std::size_t outLen,
         const void *salt, std::size_t saltLen,
diff --git a/lib/argon2-gpu-common/blake2b.h b/lib/argon2-gpu-common/blake2b.h
index 094fb83..7f438ee 100644
--- a/lib/argon2-gpu-common/blake2b.h
+++ b/lib/argon2-gpu-common/blake2b.h
@@ -2,6 +2,7 @@
 #define ARGON2_BLAKE2B_H
 
 #include <cstdint>
+#include <cstddef>
 
 namespace argon2 {
 
diff --git a/lib/argon2-opencl/kernelloader.cpp b/lib/argon2-opencl/kernelloader.cpp
index 22949f3..62bb887 100644
--- a/lib/argon2-opencl/kernelloader.cpp
+++ b/lib/argon2-opencl/kernelloader.cpp
@@ -12,11 +12,16 @@ cl::Program KernelLoader::loadArgon2Program(
         const std::string &sourceDirectory,
         Type type, Version version, bool debug)
 {
-    std::string sourcePath = sourceDirectory + "/argon2_kernel.cl";
+    std::string sourcePath = sourceDirectory + "argon2_kernel.cl";
     std::string sourceText;
     std::stringstream buildOpts;
     {
         std::ifstream sourceFile { sourcePath };
+
+        if (!sourceFile.is_open()) {
+            throw std::logic_error(std::string("Cannot find kernel source: ") + sourcePath);
+        }
+
         sourceText = {
             std::istreambuf_iterator<char>(sourceFile),
             std::istreambuf_iterator<char>()
@@ -38,6 +43,7 @@ cl::Program KernelLoader::loadArgon2Program(
         for (cl::Device &device : context.getInfo<CL_CONTEXT_DEVICES>()) {
             std::cerr << "  Build log from device '" << device.getInfo<CL_DEVICE_NAME>() << "':" << std::endl;
             std::cerr << prog.getBuildInfo<CL_PROGRAM_BUILD_LOG>(device);
+            err;
         }
         throw;
     }
diff --git a/lib/argon2-opencl/kernelrunner.cpp b/lib/argon2-opencl/kernelrunner.cpp
index 9fe39b4..70df280 100644
--- a/lib/argon2-opencl/kernelrunner.cpp
+++ b/lib/argon2-opencl/kernelrunner.cpp
@@ -1,195 +1,358 @@
 #include "kernelrunner.h"
 
 #include <stdexcept>
-
-#ifndef NDEBUG
+#include <thread>
 #include <iostream>
-#endif
+#include <iomanip>
+#include <map>
+#include <sstream>
 
 #define THREADS_PER_LANE 32
 
+//#define FLUSH_ALL
+
+#include "../../include/perfscope.h"
+
 namespace argon2 {
 namespace opencl {
 
-enum {
-    ARGON2_REFS_PER_BLOCK = ARGON2_BLOCK_SIZE / (2 * sizeof(cl_uint)),
-};
+uint32_t KernelRunner::getBlockCountPerHash() const {
+    if (optParams.customBlockCount)
+        return optParams.customBlockCount;
+    return params->getLanes() * params->getSegmentBlocks() * ARGON2_SYNC_POINTS;
+}
+
+std::uint32_t KernelRunner::getHashesPerRun() const {
+    return (std::uint32_t)memConfig.getTotalHashes(curBlockType);
+}
 
-KernelRunner::KernelRunner(const ProgramContext *programContext,
-                           const Argon2Params *params, const Device *device,
-                           std::uint32_t batchSize, bool bySegment, bool precompute)
-    : programContext(programContext), params(params), batchSize(batchSize),
-      bySegment(bySegment), precompute(precompute),
-      memorySize(params->getMemorySize() * static_cast<std::size_t>(batchSize))
+KernelRunner::KernelRunner(
+        const Device *device,
+        const Argon2Params *params,
+        const ProgramContext *programContext,
+        const MemConfig &mCfg,
+        t_optParams opt,
+        int curBlockType) :
+    memConfig(mCfg),
+    programContext(programContext), 
+    params(params), 
+    optParams(opt),
+    curBlockType(curBlockType)
 {
     auto context = programContext->getContext();
+    cl_command_queue_properties props = CL_QUEUE_PROFILING_ENABLE;
+    queue = new cl::CommandQueue(context, device->getCLDevice(), props);
+
+    // allocate blocks buffer
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto size = memConfig.blocksBuffers[i];
+        if (size)
+            blocksBuffers[i] = cl::Buffer(context, CL_MEM_READ_WRITE, size);
+    }
+
+    // allocate & fill index buffer
+    //auto indexSize = memConfig.index;
+    //if (indexSize) {
+    //    indexBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, indexSize);
+    //    bool blocking = false;
+    //    cl_int res = queue->enqueueWriteBuffer(
+    //        indexBuffer,
+    //        blocking,
+    //        0,
+    //        indexSize,
+    //        optParams.customIndex,
+    //        NULL,
+    //        NULL);
+    //    queue->finish();
+    //}
+
+    setupKernel(device);
+}
+
+void KernelRunner::reconfigureArgon(
+    const Device *device,
+    const Argon2Params *newParams, 
+    const t_optParams &newOptParams,
+    int blockType) {
+    PerfScope p("reconfigureArgon");
+    curBlockType = blockType;
+    params = newParams;
+    optParams = newOptParams;
+    setupKernel(device);
+}
+
+void KernelRunner::setKernelsArgs() {
+    PerfScope p("setKernelsArgs");
+
     std::uint32_t passes = params->getTimeCost();
     std::uint32_t lanes = params->getLanes();
     std::uint32_t segmentBlocks = params->getSegmentBlocks();
 
-    queue = cl::CommandQueue(context, device->getCLDevice(),
-                             CL_QUEUE_PROFILING_ENABLE);
+#if 1
+    kernel.setArg<cl::Buffer>(1, blocksBuffers[0]);
+#else
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        kernel.setArg<cl::Buffer>(i * 2 + 1, blocksBuffers[i]);
+        kernel.setArg<cl_uint>(i * 2 + 2, (cl_uint)memConfig.batchSizes[curBlockType][i]);
+    }
+#endif
+
+    if (optParams.mode == PRECOMPUTE) {
+        kernel.setArg<cl::Buffer>(9, indexBuffer);
 
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
+        uint32_t nSteps = ARGON2_SYNC_POINTS * segmentBlocks;
+        kernel.setArg<cl_uint>(10, nSteps);
+        kernel.setArg<cl_uint>(11, optParams.customBlockCount);
+    }
+    else {
+#if 1
+        kernel.setArg<cl_uint>(2, passes);
+        kernel.setArg<cl_uint>(3, lanes);
+        kernel.setArg<cl_uint>(4, segmentBlocks);
+#else
+        kernel.setArg<cl_uint>(9, passes);
+        kernel.setArg<cl_uint>(10, lanes);
+        kernel.setArg<cl_uint>(11, segmentBlocks);
 #endif
+    }
+}
 
-    memoryBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, memorySize);
+void KernelRunner::setupKernel(const Device *device) {
+    PerfScope p("setupKernel");
 
-    Type type = programContext->getArgon2Type();
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
-        std::uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
+    static const char *KERNEL_NAMES[2] = {
+        "argon2_kernel_oneshot",
+        "argon2_kernel_oneshot_precomputedIndex"
+    };
+    const char* kernelName =
+        KERNEL_NAMES[(optParams.mode == PRECOMPUTE)];
+
+    if (optParams.mode == PRECOMPUTE) {
+        // check that we use argon2I
+        auto buildOptions =
+            programContext->getProgram().getBuildInfo<CL_PROGRAM_BUILD_OPTIONS>(device->getCLDevice());
+        std::ostringstream oss;
+        oss << "-DARGON2_TYPE=" << argon2::ARGON2_I;
+        if (buildOptions.find(oss.str()) == std::string::npos) {
+            throw std::logic_error("PRECOMPUTE only supported with ARGON2_I");
+        }
 
-        std::size_t refsSize = segments * segmentBlocks * sizeof(cl_uint) * 2;
+        // check that we have only 1 lane, 1 pass etc.
+        if (params->getLanes() > 1 || params->getTimeCost() > 1) {
+            throw std::logic_error("PRECOMPUTE only supported with 1 lane, 1 pass and oneshot mode");
+        }
+    }
 
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
-#endif
+    kernel = cl::Kernel(programContext->getProgram(), kernelName);
 
-        refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
+    setKernelsArgs();
+}
 
-        precomputeRefs();
+std::size_t KernelRunner::getStartBlocksSize() {
+    return params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
+}
+  
+void KernelRunner::uploadToInputMemoryAsync(const void* srcPtr) {
+#if (!OPEN_CL_SKIP_MEM_TRANSFERS)
+    bool blocking = false;
+    std::size_t startBlocksSize = getStartBlocksSize();
+    std::size_t batchMemSize = getBlockCountPerHash() * ARGON2_BLOCK_SIZE;
+    uint8_t* pBlocks = (uint8_t*)srcPtr;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nHashes = memConfig.batchSizes[curBlockType][i];
+        for (int j = 0; j < nHashes; j++) {
+            size_t offset = batchMemSize * j;
+            {
+                PerfScope p("enqueueWriteBuffer (uploadToInputMemoryAsync)");
+                cl_int res = queue->enqueueWriteBuffer(
+                    blocksBuffers[i],
+                    blocking,
+                    offset,
+                    startBlocksSize,
+                    pBlocks,
+                    NULL,
+                    NULL);
+            }
+            pBlocks += startBlocksSize;
+        }
     }
+#ifdef FLUSH_ALL
+    queue->flush();
+#endif
+#endif // SKIP_MEM_TRANSFERS
+}
 
-    static const char *KERNEL_NAMES[2][2] = {
-        {
-            "argon2_kernel_oneshot",
-            "argon2_kernel_segment",
-        },
-        {
-            "argon2_kernel_oneshot_precompute",
-            "argon2_kernel_segment_precompute",
+void KernelRunner::fetchOutputMemoryAsync(uint8_t* dstPtr) {
+#if (!OPEN_CL_SKIP_MEM_TRANSFERS)
+    std::size_t batchMemSize = getBlockCountPerHash() * ARGON2_BLOCK_SIZE;
+    std::size_t resultSize = static_cast<std::size_t>(params->getLanes()) * ARGON2_BLOCK_SIZE;
+    bool blocking = false;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nHashes = memConfig.batchSizes[curBlockType][i];
+        for (int j = 0; j < nHashes; j++) {
+            std::size_t offset = batchMemSize * (j + 1) - resultSize;
+            {
+                PerfScope p("enqueueReadBuffer (fetchOutputMemoryAsync)");
+                cl_int res = queue->enqueueReadBuffer(
+                    blocksBuffers[i],
+                    blocking,
+                    offset,
+                    resultSize,
+                    dstPtr,
+                    NULL,
+                    NULL);
+            }
+            dstPtr += resultSize;
         }
-    };
+    }
+#ifdef FLUSH_ALL
+    queue->flush();
+#endif
+#endif // SKIP_MEM_TRANSFERS
+}
 
-    kernel = cl::Kernel(programContext->getProgram(),
-                        KERNEL_NAMES[precompute][bySegment]);
-    kernel.setArg<cl::Buffer>(1, memoryBuffer);
-    if (precompute) {
-        kernel.setArg<cl::Buffer>(2, refsBuffer);
-        kernel.setArg<cl_uint>(3, passes);
-        kernel.setArg<cl_uint>(4, lanes);
-        kernel.setArg<cl_uint>(5, segmentBlocks);
-    } else {
-        kernel.setArg<cl_uint>(2, passes);
-        kernel.setArg<cl_uint>(3, lanes);
-        kernel.setArg<cl_uint>(4, segmentBlocks);
+void KernelRunner::insertEndEventAndFlush() {
+    {
+        PerfScope p("insertEndEventAndFlush");
+        queue->enqueueMarker(&end);
+        cl::detail::errHandler(queue->flush(), "KernelRunner::resultsReady, flushing queue");
     }
 }
 
-void KernelRunner::precomputeRefs()
+void KernelRunner::run(std::uint32_t lanesPerBlock)
 {
-    std::uint32_t passes = params->getTimeCost();
     std::uint32_t lanes = params->getLanes();
-    std::uint32_t segmentBlocks = params->getSegmentBlocks();
-    std::uint32_t segmentAddrBlocks =
-            (segmentBlocks + ARGON2_REFS_PER_BLOCK - 1)
-            / ARGON2_REFS_PER_BLOCK;
-    std::uint32_t segments = programContext->getArgon2Type() == ARGON2_ID
-            ? lanes * (ARGON2_SYNC_POINTS / 2)
-            : passes * lanes * ARGON2_SYNC_POINTS;
+    std::uint32_t passes = params->getTimeCost();
 
-    std::size_t shmemSize = THREADS_PER_LANE * sizeof(cl_uint) * 2;
+    if (lanesPerBlock != lanes) {
+        throw std::logic_error("Invalid lanesPerBlock!");
+    }
+
+    cl::NDRange globalRange { THREADS_PER_LANE * lanes, memConfig.getTotalHashes(curBlockType) };
+    cl::NDRange localRange { THREADS_PER_LANE * lanesPerBlock, 1 };
+
+    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * sizeof(cl_uint) * 2;
+
+    //std::cout << "lanesPerBlock: " << lanesPerBlock << std::endl;
+    //std::cout << "jobsPerBlock: " << 1 << std::endl;
+    //std::cout << "shmemSize: " << shmemSize << std::endl;
+
+//#if 0
+//    std::cout << "lanesPerBlock = " << lanesPerBlock << std::endl;
+//    std::cout << "shmemSize     = " << shmemSize << std::endl;
+//
+//    std::cout 
+//        << "globalRange   = { " 
+//        << globalRange[0] << ", " << globalRange[1] 
+//        << " }" << std::endl;
+//
+//    std::cout
+//        << "localRange    = { "
+//        << localRange[0] << ", " << localRange[1]
+//        << " }" << std::endl;
+//#endif
 
-    cl::Kernel kernel = cl::Kernel(programContext->getProgram(),
-                                   "argon2_precompute_kernel");
     kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
-    kernel.setArg<cl::Buffer>(1, refsBuffer);
-    kernel.setArg<cl_uint>(2, passes);
-    kernel.setArg<cl_uint>(3, lanes);
-    kernel.setArg<cl_uint>(4, segmentBlocks);
+    {
+        PerfScope p("enqueueNDRangeKernel");
+        queue->enqueueNDRangeKernel(kernel, cl::NullRange,
+            globalRange, localRange);
+    }
 
-    cl::NDRange globalRange { THREADS_PER_LANE * segments * segmentAddrBlocks };
-    cl::NDRange localRange { THREADS_PER_LANE };
-    queue.enqueueNDRangeKernel(kernel, cl::NullRange, globalRange, localRange);
-    queue.finish();
+#ifdef FLUSH_ALL
+   queue->flush();
+#endif
 }
 
-void *KernelRunner::mapInputMemory(std::uint32_t jobId)
-{
-    std::size_t memorySize = params->getMemorySize();
-    std::size_t mappedSize = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_WRITE,
-                                  memorySize * jobId, mappedSize);
+void KernelRunner::waitForResults() {
+    {
+        PerfScope p("end.wait()");
+        end.wait();
+    }
 }
 
-void KernelRunner::unmapInputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
-}
+bool KernelRunner::resultsReady() {
+    cl_int err = NULL;
+    auto status = end.getInfo<CL_EVENT_COMMAND_EXECUTION_STATUS>(&err);
+    cl::detail::errHandler(err, "KernelRunner::resultsReady");
 
-void *KernelRunner::mapOutputMemory(std::uint32_t jobId)
-{
-    std::size_t memorySize = params->getMemorySize();
-    std::size_t mappedSize = static_cast<std::size_t>(params->getLanes())
-            * ARGON2_BLOCK_SIZE;
-    std::size_t mappedOffset = memorySize * (jobId + 1) - mappedSize;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_READ,
-                                  mappedOffset, mappedSize);
+    if (status == CL_COMPLETE) {
+        return true;
+    }
+    else {
+        return false;
+    }
 }
 
-void KernelRunner::unmapOutputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
+DeviceRessources::DeviceRessources(const Device *pDevice) {
+    id = pDevice->getCLDevice()();
+    device = pDevice->getCLDevice();
+    context = cl::Context(device);
 }
 
-void KernelRunner::run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock)
-{
-    std::uint32_t lanes = params->getLanes();
-    std::uint32_t passes = params->getTimeCost();
+std::map<cl_device_id, DeviceRessources> s_devicesRessources;
 
-    if (bySegment) {
-        if (lanesPerBlock > lanes || lanes % lanesPerBlock != 0) {
-            throw std::logic_error("Invalid lanesPerBlock!");
-        }
-    } else {
-        if (lanesPerBlock != lanes) {
-            throw std::logic_error("Invalid lanesPerBlock!");
-        }
+DeviceRessources& DeviceRessources::get(const Device *pDevice) {
+    cl_device_id cl_device = pDevice->getCLDevice()();
+    auto it = s_devicesRessources.find(cl_device);
+    if (it == s_devicesRessources.end()) {
+        s_devicesRessources.insert(
+            std::make_pair(cl_device, DeviceRessources(pDevice)));
     }
+    return s_devicesRessources[cl_device];
+}
 
-    if (jobsPerBlock > batchSize || batchSize % jobsPerBlock != 0) {
-        throw std::logic_error("Invalid jobsPerBlock!");
-    }
+cl::CommandQueue* DeviceRessources::addQueue() {
+    queues.push_back(cl::CommandQueue(context, device));
+    return &queues.back();
+}
 
-    cl::NDRange globalRange { THREADS_PER_LANE * lanes, batchSize };
-    cl::NDRange localRange { THREADS_PER_LANE * lanesPerBlock, jobsPerBlock };
+cl::CommandQueue* DeviceRessources::getQueue(int index) {
+    if (index < queues.size())
+        return &(queues[index]);
+    return nullptr;
+}
 
-    queue.enqueueMarker(&start);
+cl::Buffer* DeviceRessources::addBuffer(cl_mem_flags flags, size_t size) {
 
-    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * jobsPerBlock
-            * sizeof(cl_uint) * 2;
-    kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
-    if (bySegment) {
-        for (std::uint32_t pass = 0; pass < passes; pass++) {
-            for (std::uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
-                kernel.setArg<cl_uint>(precompute ? 6 : 5, pass);
-                kernel.setArg<cl_uint>(precompute ? 7 : 6, slice);
-                queue.enqueueNDRangeKernel(kernel, cl::NullRange,
-                                           globalRange, localRange);
-            }
-        }
-    } else {
-        queue.enqueueNDRangeKernel(kernel, cl::NullRange,
-                                   globalRange, localRange);
+    BufferInfo b;
+    b.size = size;
+    b.flags = flags;
+
+    // get a device queue
+    auto pQueue = getQueue(0);
+    if (!pQueue) {
+        pQueue = addQueue();
+        if (!pQueue)
+            throw std::logic_error("DeviceRessources::addBuffer: cannot get queue");
     }
 
-    queue.enqueueMarker(&end);
-}
+    try {
+        // allocate buffer
+        b.buf = cl::Buffer(context, flags, size);
+        if (b.buf() == 0)
+            return nullptr;
 
-float KernelRunner::finish()
-{
-    end.wait();
+        // write one byte to it (to make sure that allocation succeeded)
+        uint8_t dummy = 0xFF;
+        pQueue->enqueueWriteBuffer(b.buf, true, size - 1, 1, &dummy);
+    }
+    catch (const cl::Error) {
+        return nullptr;
+    }
 
-    cl_ulong nsStart = start.getProfilingInfo<CL_PROFILING_COMMAND_END>();
-    cl_ulong nsEnd   = end.getProfilingInfo<CL_PROFILING_COMMAND_END>();
+    buffers.push_back(b);
+    return &(buffers.back().buf);
+}
 
-    return (nsEnd - nsStart) / (1000.0F * 1000.0F);
+cl::Buffer* DeviceRessources::getBuffer(
+    cl_mem_flags flags,
+    size_t size) {
+    for (auto &it : buffers) {
+        if (it.size == size && it.flags == flags)
+            return &(it.buf);
+    }
+    return nullptr;
 }
 
 } // namespace opencl
diff --git a/lib/argon2-opencl/processingunit.cpp b/lib/argon2-opencl/processingunit.cpp
index 598ced7..38b5620 100644
--- a/lib/argon2-opencl/processingunit.cpp
+++ b/lib/argon2-opencl/processingunit.cpp
@@ -8,130 +8,113 @@
 namespace argon2 {
 namespace opencl {
 
-static bool isPowerOfTwo(std::uint32_t x)
-{
-    return (x & (x - 1)) == 0;
-}
+#pragma warning(disable:4267)
+#pragma warning(disable:4101)
 
 ProcessingUnit::ProcessingUnit(
-        const ProgramContext *programContext, const Argon2Params *params,
-        const Device *device, std::size_t batchSize,
-        bool bySegment, bool precomputeRefs)
-    : programContext(programContext), params(params), device(device),
-      runner(programContext, params, device, batchSize, bySegment,
-             precomputeRefs),
-      bestLanesPerBlock(runner.getMinLanesPerBlock()),
-      bestJobsPerBlock(runner.getMinJobsPerBlock())
+    const Device *device,
+    const ProgramContext *programContext,
+    const Argon2Params *params,
+    const MemConfig &mcfg,
+    const t_optParams& optPrms,
+    int curBlockType)
+    : device(device),
+      runner(device, params, programContext, mcfg, optPrms, curBlockType),
+      bestLanesPerBlock(runner.getMinLanesPerBlock())
 {
-    /* pre-fill first blocks with pseudo-random data: */
-    for (std::size_t i = 0; i < batchSize; i++) {
-        setPassword(i, NULL, 0);
-    }
-
-    if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
-            && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
-             lpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(lpb, bestJobsPerBlock);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   OpenCL error on " << lpb
-                          << " lanes per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
+    // preallocate pinned mem for kernel inputs
+    auto context = programContext->getContext();
+    std::size_t inSize = mcfg.in;
+    inBuffer = cl::Buffer(
+            context,
+            CL_MEM_READ_ONLY | CL_MEM_ALLOC_HOST_PTR, 
+            inSize);
+    auto queue = runner.getQueue();
+    inPtr = (uint8_t*)queue->enqueueMapBuffer(
+        inBuffer,
+        true,
+        CL_MAP_WRITE, 0, inSize);
+
+    // preallocate pinned mem for kernel outputs
+    std::size_t outSize = mcfg.out;
+    outBuffer = cl::Buffer(
+        context,
+        CL_MEM_WRITE_ONLY | CL_MEM_ALLOC_HOST_PTR,
+        outSize);
+    outPtr = (uint8_t*)queue->enqueueMapBuffer(
+        outBuffer,
+        true,
+        CL_MAP_READ, 0, outSize);
+}
 
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << lpb << " lanes per block: "
-                      << time << " ms" << std::endl;
-#endif
+//#define SKIP_MEM_TRANSFERS
 
-            if (time < bestTime) {
-                bestTime = time;
-                bestLanesPerBlock = lpb;
-            }
+void ProcessingUnit::uploadInputDataAsync(const std::vector<std::string>& bases)
+{
+#ifndef SKIP_MEM_TRANSFERS
+    std::vector<std::string>::const_iterator it = bases.begin();
+    auto blockType = runner.getCurrentBlockType();
+    uint8_t* pIn = inPtr;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nHashes = runner.memConfig.batchSizes[blockType][i];
+        for (int j = 0; j < nHashes; j++) {
+            runner.params->fillFirstBlocks(
+                pIn,
+                it->data(),
+                it->length(),
+                runner.programContext->getArgon2Type(),
+                runner.programContext->getArgon2Version());
+            pIn += runner.getStartBlocksSize();
+            it++;
         }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestLanesPerBlock
-                  << " lanes per block." << std::endl;
-#endif
     }
-
-    /* Only tune jobs per block if we hit maximum lanes per block: */
-    if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
-            && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
-            && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
-             jpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(bestLanesPerBlock, jpb);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   OpenCL error on " << jpb
-                          << " jobs per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
-
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << jpb << " jobs per block: "
-                      << time << " ms" << std::endl;
+    runner.uploadToInputMemoryAsync(inPtr);
 #endif
+}
 
-            if (time < bestTime) {
-                bestTime = time;
-                bestJobsPerBlock = jpb;
-            }
-        }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestJobsPerBlock
-                  << " jobs per block." << std::endl;
+void ProcessingUnit::fetchResultsAsync() {
+#ifndef SKIP_MEM_TRANSFERS
+    runner.fetchOutputMemoryAsync(outPtr);
 #endif
-    }
+    runner.insertEndEventAndFlush();
 }
 
-void ProcessingUnit::setPassword(std::size_t index, const void *pw,
-                                 std::size_t pwSize)
+void ProcessingUnit::runKernelAsync()
 {
-    void *memory = runner.mapInputMemory(index);
-    params->fillFirstBlocks(memory, pw, pwSize,
-                            programContext->getArgon2Type(),
-                            programContext->getArgon2Version());
-    runner.unmapInputMemory(memory);
+    runner.run(bestLanesPerBlock);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    void *memory = runner.mapOutputMemory(index);
-    params->finalize(hash, memory);
-    runner.unmapOutputMemory(memory);
+void ProcessingUnit::waitForResults() {
+    runner.waitForResults();
 }
 
-void ProcessingUnit::beginProcessing()
-{
-    runner.run(bestLanesPerBlock, bestJobsPerBlock);
+bool ProcessingUnit::resultsReady() {
+    return runner.resultsReady();
+}
+
+uint8_t* ProcessingUnit::getResultPtr(int jobId) {
+    return outPtr + jobId * (runner.params->getLanes() * ARGON2_BLOCK_SIZE);
 }
 
-void ProcessingUnit::endProcessing()
+using namespace std;
+
+void ProcessingUnit::reconfigureArgon(
+    const Device *device,
+    const Argon2Params *newParams,
+    const t_optParams &newOptParams,
+    int blockType)
 {
-    runner.finish();
+    runner.reconfigureArgon(device, newParams, newOptParams, blockType);
+    bestLanesPerBlock = runner.getMinLanesPerBlock();
+
+    cout << "reconfigureArgon("
+        << newParams->getTimeCost() << ", "
+        << newParams->getMemoryCost() << ", "
+        << newParams->getLanes() << ", "
+        << "customBlockCount=" << newOptParams.customBlockCount << ", "
+        << "customIndex=" << static_cast<const void*>(newOptParams.customIndex) << ", "
+        << "bestLanesPerBlock=" << bestLanesPerBlock
+        << ")" << endl;
 }
 
 } // namespace opencl
diff --git a/lib/argon2-opencl/programcontext.cpp b/lib/argon2-opencl/programcontext.cpp
index 67c2dc1..d1e1dc4 100644
--- a/lib/argon2-opencl/programcontext.cpp
+++ b/lib/argon2-opencl/programcontext.cpp
@@ -8,7 +8,7 @@ namespace opencl {
 ProgramContext::ProgramContext(
         const GlobalContext *globalContext,
         const std::vector<Device> &devices,
-        Type type, Version version, char* pathToKernel)
+        Type type, Version version, const char* pathToKernel)
     : globalContext(globalContext), devices(), type(type), version(version)
 {
     this->devices.reserve(devices.size());
