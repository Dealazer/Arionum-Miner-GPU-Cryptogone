diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3dd95ba..15fb786 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -28,9 +28,17 @@ if(CUDA_FOUND)
     )
 endif()
 
+FIND_PACKAGE(OpenCL)
+INCLUDE_DIRECTORIES(${OPENCL_INCLUDE_DIR})
+if (OPENCL_FOUND)
+    message("INFO: Using OPENCL version ${OpenCL_VERSION_MAJOR}.${OpenCL_VERSION_MINOR}")
+else()
+    message("INFO: OPENCL not found")
+endif()
+
 add_subdirectory(ext/argon2)
 
-add_library(argon2-gpu-common SHARED
+add_library(argon2-gpu-common STATIC
     lib/argon2-gpu-common/argon2params.cpp
     lib/argon2-gpu-common/blake2b.cpp
 )
@@ -44,7 +52,7 @@ target_include_directories(argon2-gpu-common PRIVATE
 )
 
 if(CUDA_FOUND)
-    cuda_add_library(argon2-cuda SHARED
+    cuda_add_library(argon2-cuda STATIC
         lib/argon2-cuda/device.cpp
         lib/argon2-cuda/globalcontext.cpp
         lib/argon2-cuda/programcontext.cpp
@@ -52,7 +60,7 @@ if(CUDA_FOUND)
         lib/argon2-cuda/kernels.cu
     )
 else()
-    add_library(argon2-cuda SHARED
+    add_library(argon2-cuda STATIC
         lib/argon2-cuda/nocuda.cpp
     )
 endif()
@@ -67,7 +75,7 @@ target_include_directories(argon2-cuda INTERFACE
 )
 target_link_libraries(argon2-cuda argon2-gpu-common)
 
-add_library(argon2-opencl SHARED
+add_library(argon2-opencl STATIC
     lib/argon2-opencl/device.cpp
     lib/argon2-opencl/globalcontext.cpp
     lib/argon2-opencl/kernelloader.cpp
@@ -84,56 +92,5 @@ target_include_directories(argon2-opencl PRIVATE
     lib/argon2-opencl
 )
 target_link_libraries(argon2-opencl
-    argon2-gpu-common -lOpenCL
-)
-
-add_executable(argon2-gpu-test
-    src/argon2-gpu-test/main.cpp
-    src/argon2-gpu-test/testcase.cpp
-)
-target_include_directories(argon2-gpu-test PRIVATE src/argon2-gpu-test)
-target_link_libraries(argon2-gpu-test
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_executable(argon2-gpu-bench
-    src/argon2-gpu-bench/cpuexecutive.cpp
-    src/argon2-gpu-bench/cudaexecutive.cpp
-    src/argon2-gpu-bench/openclexecutive.cpp
-    src/argon2-gpu-bench/benchmark.cpp
-    src/argon2-gpu-bench/main.cpp
-)
-target_include_directories(argon2-gpu-bench PRIVATE src/argon2-gpu-bench)
-target_link_libraries(argon2-gpu-bench
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_test(argon2-gpu-test-opencl argon2-gpu-test -m opencl)
-add_test(argon2-gpu-test-cuda argon2-gpu-test -m cuda)
-
-install(
-    TARGETS argon2-gpu-common argon2-opencl argon2-cuda
-    DESTINATION ${LIBRARY_INSTALL_DIR}
-)
-install(FILES
-    include/argon2-gpu-common/argon2-common.h
-    include/argon2-gpu-common/argon2params.h
-    include/argon2-opencl/cl.hpp
-    include/argon2-opencl/opencl.h
-    include/argon2-opencl/device.h
-    include/argon2-opencl/globalcontext.h
-    include/argon2-opencl/programcontext.h
-    include/argon2-opencl/processingunit.h
-    include/argon2-opencl/kernelrunner.h
-    include/argon2-cuda/cudaexception.h
-    include/argon2-cuda/kernels.h
-    include/argon2-cuda/device.h
-    include/argon2-cuda/globalcontext.h
-    include/argon2-cuda/programcontext.h
-    include/argon2-cuda/processingunit.h
-    DESTINATION ${INCLUDE_INSTALL_DIR}
-)
-install(
-    TARGETS argon2-gpu-bench argon2-gpu-test
-    DESTINATION ${BINARY_INSTALL_DIR}
+    argon2-gpu-common ${OpenCL_LIBRARY}
 )
diff --git a/data/kernels/argon2_kernel_2id.cl b/data/kernels/argon2_kernel_2id.cl
index e69de29..594df7b 100644
--- a/data/kernels/argon2_kernel_2id.cl
+++ b/data/kernels/argon2_kernel_2id.cl
@@ -0,0 +1,686 @@
+/* C compatibility For dumb IDEs: */
+#ifndef __OPENCL_VERSION__
+#ifndef __cplusplus
+typedef int bool;
+#endif
+typedef unsigned char uchar;
+typedef unsigned short ushort;
+typedef unsigned int uint;
+typedef unsigned long ulong;
+typedef unsigned long size_t;
+typedef long ptrdiff_t;
+typedef size_t uintptr_t;
+typedef ptrdiff_t intptr_t;
+#ifndef __kernel
+#define __kernel
+#endif
+#ifndef __global
+#define __global
+#endif
+#ifndef __private
+#define __private
+#endif
+#ifndef __local
+#define __local
+#endif
+#ifndef __constant
+#define __constant const
+#endif
+#endif /* __OPENCL_VERSION__ */
+
+#define ARGON2_D  0
+#define ARGON2_I  1
+
+#define ARGON2_VERSION_10 0x10
+#define ARGON2_VERSION_13 0x13
+
+#define ARGON2_BLOCK_SIZE 1024
+#define ARGON2_QWORDS_IN_BLOCK (ARGON2_BLOCK_SIZE / 8)
+#define ARGON2_SYNC_POINTS 4
+
+#define THREADS_PER_LANE 32
+#define QWORDS_PER_THREAD (ARGON2_QWORDS_IN_BLOCK / 32)
+
+#ifndef ARGON2_TYPE
+#define ARGON2_TYPE ARGON2_I
+#endif
+
+ulong u64_build(uint hi, uint lo)
+{
+    return upsample(hi, lo);
+}
+
+uint u64_lo(ulong x)
+{
+    return (uint)x;
+}
+
+uint u64_hi(ulong x)
+{
+    return (uint)(x >> 32);
+}
+
+struct u64_shuffle_buf {
+    uint lo[THREADS_PER_LANE];
+    uint hi[THREADS_PER_LANE];
+};
+
+ulong u64_shuffle(ulong v, uint thread_src, uint thread,
+                  __local struct u64_shuffle_buf *buf)
+{
+    uint lo = u64_lo(v);
+    uint hi = u64_hi(v);
+
+    buf->lo[thread] = lo;
+    buf->hi[thread] = hi;
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    lo = buf->lo[thread_src];
+    hi = buf->hi[thread_src];
+
+    return u64_build(hi, lo);
+}
+
+struct block_g {
+    ulong data[ARGON2_QWORDS_IN_BLOCK];
+};
+
+struct block_th {
+    ulong a, b, c, d;
+};
+
+ulong cmpeq_mask(uint test, uint ref)
+{
+    uint x = -(uint)(test == ref);
+    return u64_build(x, x);
+}
+
+ulong block_th_get(const struct block_th *b, uint idx)
+{
+    ulong res = 0;
+    res ^= cmpeq_mask(idx, 0) & b->a;
+    res ^= cmpeq_mask(idx, 1) & b->b;
+    res ^= cmpeq_mask(idx, 2) & b->c;
+    res ^= cmpeq_mask(idx, 3) & b->d;
+    return res;
+}
+
+void block_th_set(struct block_th *b, uint idx, ulong v)
+{
+    b->a ^= cmpeq_mask(idx, 0) & (v ^ b->a);
+    b->b ^= cmpeq_mask(idx, 1) & (v ^ b->b);
+    b->c ^= cmpeq_mask(idx, 2) & (v ^ b->c);
+    b->d ^= cmpeq_mask(idx, 3) & (v ^ b->d);
+}
+
+void move_block(struct block_th *dst, const struct block_th *src)
+{
+    *dst = *src;
+}
+
+void xor_block(struct block_th *dst, const struct block_th *src)
+{
+    dst->a ^= src->a;
+    dst->b ^= src->b;
+    dst->c ^= src->c;
+    dst->d ^= src->d;
+}
+
+void load_block(struct block_th *dst, __global const struct block_g *src,
+                uint thread)
+{
+    dst->a = src->data[0 * THREADS_PER_LANE + thread];
+    dst->b = src->data[1 * THREADS_PER_LANE + thread];
+    dst->c = src->data[2 * THREADS_PER_LANE + thread];
+    dst->d = src->data[3 * THREADS_PER_LANE + thread];
+}
+
+void load_block_xor(struct block_th *dst, __global const struct block_g *src,
+                    uint thread)
+{
+    dst->a ^= src->data[0 * THREADS_PER_LANE + thread];
+    dst->b ^= src->data[1 * THREADS_PER_LANE + thread];
+    dst->c ^= src->data[2 * THREADS_PER_LANE + thread];
+    dst->d ^= src->data[3 * THREADS_PER_LANE + thread];
+}
+
+void store_block(__global struct block_g *dst, const struct block_th *src,
+                 uint thread)
+{
+    dst->data[0 * THREADS_PER_LANE + thread] = src->a;
+    dst->data[1 * THREADS_PER_LANE + thread] = src->b;
+    dst->data[2 * THREADS_PER_LANE + thread] = src->c;
+    dst->data[3 * THREADS_PER_LANE + thread] = src->d;
+}
+
+#ifdef cl_amd_media_ops
+#pragma OPENCL EXTENSION cl_amd_media_ops : enable
+
+ulong rotr64(ulong x, ulong n)
+{
+    uint lo = u64_lo(x);
+    uint hi = u64_hi(x);
+    uint r_lo, r_hi;
+    if (n < 32) {
+        r_lo = amd_bitalign(hi, lo, (uint)n);
+        r_hi = amd_bitalign(lo, hi, (uint)n);
+    } else {
+        r_lo = amd_bitalign(lo, hi, (uint)n - 32);
+        r_hi = amd_bitalign(hi, lo, (uint)n - 32);
+    }
+    return u64_build(r_hi, r_lo);
+}
+#else
+ulong rotr64(ulong x, ulong n)
+{
+    return rotate(x, 64 - n);
+}
+#endif
+
+ulong f(ulong x, ulong y)
+{
+    uint xlo = u64_lo(x);
+    uint ylo = u64_lo(y);
+    return x + y + 2 * u64_build(mul_hi(xlo, ylo), xlo * ylo);
+}
+
+void g(struct block_th *block)
+{
+    ulong a, b, c, d;
+    a = block->a;
+    b = block->b;
+    c = block->c;
+    d = block->d;
+
+    a = f(a, b);
+    d = rotr64(d ^ a, 32);
+    c = f(c, d);
+    b = rotr64(b ^ c, 24);
+    a = f(a, b);
+    d = rotr64(d ^ a, 16);
+    c = f(c, d);
+    b = rotr64(b ^ c, 63);
+
+    block->a = a;
+    block->b = b;
+    block->c = c;
+    block->d = d;
+}
+
+uint apply_shuffle_shift1(uint thread, uint idx)
+{
+    return (thread & 0x1c) | ((thread + idx) & 0x3);
+}
+
+uint apply_shuffle_unshift1(uint thread, uint idx)
+{
+    idx = (QWORDS_PER_THREAD - idx) % QWORDS_PER_THREAD;
+
+    return apply_shuffle_shift1(thread, idx);
+}
+
+uint apply_shuffle_shift2(uint thread, uint idx)
+{
+    uint lo = (thread & 0x1) | ((thread & 0x10) >> 3);
+    lo = (lo + idx) & 0x3;
+    return ((lo & 0x2) << 3) | (thread & 0xe) | (lo & 0x1);
+}
+
+uint apply_shuffle_unshift2(uint thread, uint idx)
+{
+    idx = (QWORDS_PER_THREAD - idx) % QWORDS_PER_THREAD;
+
+    return apply_shuffle_shift2(thread, idx);
+}
+
+void shuffle_shift1(struct block_th *block, uint thread,
+                    __local struct u64_shuffle_buf *buf)
+{
+    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
+        uint src_thr = apply_shuffle_shift1(thread, i);
+
+        ulong v = block_th_get(block, i);
+        v = u64_shuffle(v, src_thr, thread, buf);
+        block_th_set(block, i, v);
+    }
+}
+
+void shuffle_unshift1(struct block_th *block, uint thread,
+                      __local struct u64_shuffle_buf *buf)
+{
+    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
+        uint src_thr = apply_shuffle_unshift1(thread, i);
+
+        ulong v = block_th_get(block, i);
+        v = u64_shuffle(v, src_thr, thread, buf);
+        block_th_set(block, i, v);
+    }
+}
+
+void shuffle_shift2(struct block_th *block, uint thread,
+                    __local struct u64_shuffle_buf *buf)
+{
+    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
+        uint src_thr = apply_shuffle_shift2(thread, i);
+
+        ulong v = block_th_get(block, i);
+        v = u64_shuffle(v, src_thr, thread, buf);
+        block_th_set(block, i, v);
+    }
+}
+
+void shuffle_unshift2(struct block_th *block, uint thread,
+                      __local struct u64_shuffle_buf *buf)
+{
+    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
+        uint src_thr = apply_shuffle_unshift2(thread, i);
+
+        ulong v = block_th_get(block, i);
+        v = u64_shuffle(v, src_thr, thread, buf);
+        block_th_set(block, i, v);
+    }
+}
+
+void transpose(struct block_th *block, uint thread,
+               __local struct u64_shuffle_buf *buf)
+{
+    uint thread_group = (thread & 0x0C) >> 2;
+    for (uint i = 1; i < QWORDS_PER_THREAD; i++) {
+        uint thr = (i << 2) ^ thread;
+        uint idx = thread_group ^ i;
+
+        ulong v = block_th_get(block, idx);
+        v = u64_shuffle(v, thr, thread, buf);
+        block_th_set(block, idx, v);
+    }
+}
+
+void shuffle_block(struct block_th *block, uint thread,
+                   __local struct u64_shuffle_buf *buf)
+{
+    transpose(block, thread, buf);
+
+    g(block);
+
+    shuffle_shift1(block, thread, buf);
+
+    g(block);
+
+    shuffle_unshift1(block, thread, buf);
+    transpose(block, thread, buf);
+
+    g(block);
+
+    shuffle_shift2(block, thread, buf);
+
+    g(block);
+
+    shuffle_unshift2(block, thread, buf);
+}
+
+void compute_ref_pos(uint lanes, uint segment_blocks,
+                     uint pass, uint lane, uint slice, uint offset,
+                     uint *ref_lane, uint *ref_index)
+{
+    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+
+    *ref_lane = *ref_lane % lanes;
+
+    uint base;
+    if (pass != 0) {
+        base = lane_blocks - segment_blocks;
+    } else {
+        if (slice == 0) {
+            *ref_lane = lane;
+        }
+        base = slice * segment_blocks;
+    }
+
+    uint ref_area_size = base + offset - 1;
+    if (*ref_lane != lane) {
+        ref_area_size = min(ref_area_size, base);
+    }
+
+    *ref_index = mul_hi(*ref_index, *ref_index);
+    *ref_index = ref_area_size - 1 - mul_hi(ref_area_size, *ref_index);
+
+    if (pass != 0 && slice != ARGON2_SYNC_POINTS - 1) {
+        *ref_index += (slice + 1) * segment_blocks;
+        if (*ref_index >= lane_blocks) {
+            *ref_index -= lane_blocks;
+        }
+    }
+}
+
+void argon2_core(
+        __global struct block_g *memory, __global struct block_g *mem_curr,
+        struct block_th *prev, struct block_th *tmp,
+        __local struct u64_shuffle_buf *shuffle_buf, uint lanes,
+        uint thread, uint pass, uint ref_index, uint ref_lane)
+{
+    __global struct block_g *mem_ref;
+    mem_ref = memory + ref_index * lanes + ref_lane;
+
+    if (pass != 0) {
+        load_block(tmp, mem_curr, thread);
+        load_block_xor(prev, mem_ref, thread);
+        xor_block(tmp, prev);
+    } else {
+        load_block_xor(prev, mem_ref, thread);
+        move_block(tmp, prev);
+    }
+
+    shuffle_block(prev, thread, shuffle_buf);
+
+    xor_block(prev, tmp);
+
+	// STORE mem_curr !
+    store_block(mem_curr, prev, thread);
+}
+
+void next_addresses(struct block_th *addr, struct block_th *tmp,
+                    uint thread_input, uint thread,
+                    __local struct u64_shuffle_buf *buf)
+{
+    addr->a = u64_build(0, thread_input);
+    addr->b = 0;
+    addr->c = 0;
+    addr->d = 0;
+
+    shuffle_block(addr, thread, buf);
+
+    addr->a ^= u64_build(0, thread_input);
+    move_block(tmp, addr);
+
+    shuffle_block(addr, thread, buf);
+
+    xor_block(addr, tmp);
+}
+
+struct ref {
+    uint ref_lane;
+    uint ref_index;
+};
+
+/*
+ * Refs hierarchy:
+ * lanes -> passes -> slices -> blocks
+ */
+__kernel void argon2_precompute_kernel(
+        __local struct u64_shuffle_buf *shuffle_bufs, __global struct ref *refs,
+        uint passes, uint lanes, uint segment_blocks)
+{
+    uint block_id = get_global_id(0) / THREADS_PER_LANE;
+    uint warp = get_local_id(0) / THREADS_PER_LANE;
+    uint thread = get_local_id(0) % THREADS_PER_LANE;
+
+    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+
+    uint segment_addr_blocks = (segment_blocks + ARGON2_QWORDS_IN_BLOCK - 1)
+            / ARGON2_QWORDS_IN_BLOCK;
+    uint block = block_id % segment_addr_blocks;
+    uint segment = block_id / segment_addr_blocks;
+
+    uint slice, pass, lane;
+    uint pass_id;
+
+    slice = segment % ARGON2_SYNC_POINTS;
+    pass_id = segment / ARGON2_SYNC_POINTS;
+
+    pass = pass_id % passes;
+    lane = pass_id / passes;
+
+    struct block_th addr, tmp;
+
+    uint thread_input;
+    switch (thread) {
+    case 0:
+        thread_input = pass;
+        break;
+    case 1:
+        thread_input = lane;
+        break;
+    case 2:
+        thread_input = slice;
+        break;
+    case 3:
+        thread_input = lanes * segment_blocks * ARGON2_SYNC_POINTS;
+        break;
+    case 4:
+        thread_input = passes;
+        break;
+    case 5:
+        thread_input = ARGON2_TYPE;
+        break;
+    case 6:
+        thread_input = block + 1;
+        break;
+    default:
+        thread_input = 0;
+        break;
+    }
+
+    next_addresses(&addr, &tmp, thread_input, thread, shuffle_buf);
+
+    refs += segment * segment_blocks;
+
+    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
+        uint pos = i * THREADS_PER_LANE + thread;
+        uint offset = block * ARGON2_QWORDS_IN_BLOCK + pos;
+        if (offset < segment_blocks) {
+            ulong v = block_th_get(&addr, i);
+            uint ref_index = u64_lo(v);
+            uint ref_lane  = u64_hi(v);
+
+            compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
+                            &ref_lane, &ref_index);
+
+            refs[offset].ref_index = ref_index;
+            refs[offset].ref_lane  = ref_lane;
+        }
+    }
+}
+
+void argon2_step_precompute(
+        __global struct block_g *memory, __global struct block_g *mem_curr,
+        struct block_th *prev, struct block_th *tmp,
+        __local struct u64_shuffle_buf *shuffle_buf,
+        __global const struct ref **refs,
+        uint lanes, uint segment_blocks, uint thread,
+        uint lane, uint pass, uint slice, uint offset)
+{
+    uint ref_index, ref_lane;
+    ref_index = (*refs)->ref_index;
+    ref_lane = (*refs)->ref_lane;
+    (*refs)++;
+    argon2_core(memory, mem_curr, prev, tmp, shuffle_buf, lanes, thread, pass,
+                ref_index, ref_lane);
+}
+
+__kernel void argon2_kernel_oneshot_precompute(
+        __local struct u64_shuffle_buf *shuffle_bufs,
+        __global struct block_g *memory, __global const struct ref *refs,
+        uint passes, uint lanes, uint segment_blocks)
+{
+    uint job_id = get_global_id(1);
+    uint lane   = get_global_id(0) / THREADS_PER_LANE;
+    uint warp   = get_local_id(0) / THREADS_PER_LANE;
+    uint thread = get_local_id(0) % THREADS_PER_LANE;
+
+    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+
+    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+
+    /* select job's memory region: */
+    memory += (size_t)job_id * lanes * lane_blocks;
+
+    struct block_th prev, tmp;
+
+    __global struct block_g *mem_lane = memory + lane;
+    __global struct block_g *mem_prev = mem_lane + 1 * lanes;
+    __global struct block_g *mem_curr = mem_lane + 2 * lanes;
+
+    load_block(&prev, mem_prev, thread);
+
+    refs += lane * passes * lane_blocks + 2;
+
+    uint skip = 2;
+    for (uint pass = 0; pass < passes; ++pass) {
+        for (uint slice = 0; slice < ARGON2_SYNC_POINTS; ++slice) {
+            for (uint offset = 0; offset < segment_blocks; ++offset) {
+                if (skip > 0) {
+                    --skip;
+                    continue;
+                }
+
+                // can make a argon2_step_precompute first pass here ...
+                // some things are done differently in first pass ...
+
+                argon2_step_precompute(
+                            memory, mem_curr, &prev, &tmp, shuffle_buf, &refs,
+                            lanes, segment_blocks, thread,
+                            lane, pass, slice, offset);
+
+                mem_curr += lanes;
+            }
+
+            barrier(CLK_LOCAL_MEM_FENCE);
+        }
+
+        mem_curr = mem_lane;
+    }
+}
+
+void argon2_step(
+        __global struct block_g *memory, __global struct block_g *mem_curr,
+        struct block_th *prev, struct block_th *tmp, struct block_th *addr,
+        __local struct u64_shuffle_buf *shuffle_buf,
+        uint lanes, uint segment_blocks, uint thread, uint *thread_input,
+        uint lane, uint pass, uint slice, uint offset)
+{
+    uint ref_index, ref_lane;
+    bool data_independent;
+#if ARGON2_TYPE == ARGON2_I
+    data_independent = true;
+#endif
+    if (data_independent) {
+        uint addr_index = offset % ARGON2_QWORDS_IN_BLOCK;
+        if (addr_index == 0) {
+            if (thread == 6) {
+                ++*thread_input;
+            }
+            next_addresses(addr, tmp, *thread_input, thread, shuffle_buf);
+        }
+
+        uint thr = addr_index % THREADS_PER_LANE;
+        uint idx = addr_index / THREADS_PER_LANE;
+
+        ulong v = block_th_get(addr, idx);
+        v = u64_shuffle(v, thr, thread, shuffle_buf);
+        ref_index = u64_lo(v);
+        ref_lane  = u64_hi(v);
+    } else {
+        ulong v = u64_shuffle(prev->a, 0, thread, shuffle_buf);
+        ref_index = u64_lo(v);
+        ref_lane  = u64_hi(v);
+    }
+
+    compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
+                    &ref_lane, &ref_index);
+
+    argon2_core(memory, mem_curr, prev, tmp, shuffle_buf, lanes, thread, pass,
+                ref_index, ref_lane);
+}
+
+__kernel void argon2_kernel_oneshot(
+        __local struct u64_shuffle_buf *shuffle_bufs,
+        __global struct block_g *memory, uint passes, uint lanes,
+        uint segment_blocks)
+{
+    uint job_id = get_global_id(1);
+    uint lane   = get_global_id(0) / THREADS_PER_LANE;
+    uint warp   = get_local_id(0) / THREADS_PER_LANE;
+    uint thread = get_local_id(0) % THREADS_PER_LANE;
+
+    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+
+    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+
+    /* select job's memory region: */
+    memory += (size_t)job_id * lanes * lane_blocks;
+
+    struct block_th prev, addr, tmp;
+    uint thread_input;
+
+#if ARGON2_TYPE == ARGON2_I
+    switch (thread) {
+    case 1:
+        thread_input = lane;
+        break;
+    case 3:
+        thread_input = lanes * lane_blocks;
+        break;
+    case 4:
+        thread_input = passes;
+        break;
+    case 5:
+        thread_input = ARGON2_TYPE;
+        break;
+    default:
+        thread_input = 0;
+        break;
+    }
+
+    if (segment_blocks > 2) {
+        if (thread == 6) {
+            ++thread_input;
+        }
+        next_addresses(&addr, &tmp, thread_input, thread, shuffle_buf);
+    }
+#endif
+
+    __global struct block_g *mem_lane = memory + lane;
+    __global struct block_g *mem_prev = mem_lane + 1 * lanes;
+    __global struct block_g *mem_curr = mem_lane + 2 * lanes;
+
+    load_block(&prev, mem_prev, thread);
+
+    uint skip = 2;
+    for (uint pass = 0; pass < passes; ++pass) {
+        for (uint slice = 0; slice < ARGON2_SYNC_POINTS; ++slice) {
+            for (uint offset = 0; offset < segment_blocks; ++offset) {
+                if (skip > 0) {
+                    --skip;
+                    continue;
+                }
+
+                argon2_step(memory, mem_curr, &prev, &tmp, &addr, shuffle_buf,
+                            lanes, segment_blocks, thread, &thread_input,
+                            lane, pass, slice, offset);
+
+                mem_curr += lanes;
+            }
+
+            barrier(CLK_LOCAL_MEM_FENCE);
+
+#if ARGON2_TYPE == ARGON2_I
+            if (thread == 2) {
+                ++thread_input;
+            }
+            if (thread == 6) {
+                thread_input = 0;
+            }
+#endif
+        }
+#if ARGON2_TYPE == ARGON2_I
+        if (thread == 0) {
+            ++thread_input;
+        }
+        if (thread == 2) {
+            thread_input = 0;
+        }
+#endif
+        mem_curr = mem_lane;
+    }
+}
diff --git a/include/argon2-cuda/cudaexception.h b/include/argon2-cuda/cudaexception.h
index ebc8460..92d44b6 100644
--- a/include/argon2-cuda/cudaexception.h
+++ b/include/argon2-cuda/cudaexception.h
@@ -6,6 +6,8 @@
 #endif
 
 #include <exception>
+#include <stdio.h>
+#include <iostream>
 
 namespace argon2 {
 namespace cuda {
@@ -27,6 +29,7 @@ public:
     static void check(cudaError_t res)
     {
         if (res != cudaSuccess) {
+            printf("CUDA exception => |%s|\n", cudaGetErrorString(res));
             throw CudaException(res);
         }
     }
diff --git a/include/argon2-cuda/kernels.h b/include/argon2-cuda/kernels.h
index 16418b4..b332b7c 100644
--- a/include/argon2-cuda/kernels.h
+++ b/include/argon2-cuda/kernels.h
@@ -53,9 +53,17 @@ public:
 
     void writeInputMemory(std::uint32_t jobId, const void *buffer);
     void readOutputMemory(std::uint32_t jobId, void *buffer);
+    void syncStream();
+    bool streamOperationsComplete();
 
     void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
     float finish();
+
+    void reconfigureArgon(
+        std::uint32_t passes,
+        std::uint32_t lanes,
+        std::uint32_t segmentBlocks,
+        std::uint32_t newBatchSize);
 };
 
 } // cuda
diff --git a/include/argon2-cuda/processingunit.h b/include/argon2-cuda/processingunit.h
index 2109154..0a65604 100644
--- a/include/argon2-cuda/processingunit.h
+++ b/include/argon2-cuda/processingunit.h
@@ -22,6 +22,7 @@ private:
     KernelRunner runner;
     std::uint32_t bestLanesPerBlock;
     std::uint32_t bestJobsPerBlock;
+    std::vector<uint8_t*> setPasswordBuffers;
 
 public:
     std::size_t getBatchSize() const { return runner.getBatchSize(); }
@@ -32,10 +33,14 @@ public:
             bool bySegment = true, bool precomputeRefs = false);
 
     void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
+
+    void fetchResultAsync(std::size_t index, void *dest);
+    void syncStream();
+    bool streamOperationsComplete();
 
     void beginProcessing();
     void endProcessing();
+    void reconfigureArgon(const Argon2Params *newParams, std::uint32_t batchSize);
 };
 
 } // namespace cuda
diff --git a/include/argon2-gpu-common/argon2-common.h b/include/argon2-gpu-common/argon2-common.h
index fbcf67c..7a849bb 100644
--- a/include/argon2-gpu-common/argon2-common.h
+++ b/include/argon2-gpu-common/argon2-common.h
@@ -1,6 +1,8 @@
 #ifndef ARGON2COMMON_H
 #define ARGON2COMMON_H
 
+#include <cstddef>
+
 namespace argon2 {
 
 enum {
diff --git a/include/argon2-opencl/cl.hpp b/include/argon2-opencl/cl.hpp
index ced34f5..c03a742 100644
--- a/include/argon2-opencl/cl.hpp
+++ b/include/argon2-opencl/cl.hpp
@@ -142,6 +142,14 @@
  * \endcode
  *
  */
+
+#ifdef _WIN32
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+#include <iostream>
+
 #ifndef CL_HPP_
 #define CL_HPP_
 
@@ -318,7 +326,6 @@ public:
 #define __ERR_STR(x) NULL
 #endif // __CL_ENABLE_EXCEPTIONS
 
-
 namespace detail
 {
 #if defined(__CL_ENABLE_EXCEPTIONS)
@@ -327,7 +334,8 @@ static inline cl_int errHandler (
     const char * errStr = NULL)
 {
     if (err != CL_SUCCESS) {
-        throw Error(err, errStr);
+        printf("OpenCL error: errCode=%d str=%s\n", err, errStr ? errStr: "none");
+	throw Error(err, errStr);
     }
     return err;
 }
diff --git a/include/argon2-opencl/kernelrunner.h b/include/argon2-opencl/kernelrunner.h
index a1bba8b..b985e2f 100644
--- a/include/argon2-opencl/kernelrunner.h
+++ b/include/argon2-opencl/kernelrunner.h
@@ -17,13 +17,15 @@ private:
     bool bySegment;
     bool precompute;
 
-    cl::CommandQueue queue;
+    cl::CommandQueue *queue;
     cl::Kernel kernel;
     cl::Buffer memoryBuffer, refsBuffer;
-    cl::Event start, end;
+    cl::Event end;
 
     std::size_t memorySize;
 
+    void setKernelsArgs();
+    void allocateAndPrecomputeRefs();
     void precomputeRefs();
 
 public:
@@ -42,14 +44,15 @@ public:
                  const Argon2Params *params, const Device *device,
                  std::uint32_t batchSize, bool bySegment, bool precompute);
 
-    void *mapInputMemory(std::uint32_t jobId);
-    void unmapInputMemory(void *memory);
-
-    void *mapOutputMemory(std::uint32_t jobId);
-    void unmapOutputMemory(void *memory);
+    void uploadToInputMemoryAsync(std::uint32_t jobId, const void* srcPtr);
+    void fetchOutputMemoryAsync(std::uint32_t jobId, void* dstPtr);
 
     void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
-    float finish();
+
+    void waitForResults();
+    bool resultsReady();
+
+    void reconfigureArgon(const Argon2Params *newParams, std::uint32_t batchSize);
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/processingunit.h b/include/argon2-opencl/processingunit.h
index 277272d..1d61324 100644
--- a/include/argon2-opencl/processingunit.h
+++ b/include/argon2-opencl/processingunit.h
@@ -18,6 +18,7 @@ private:
     KernelRunner runner;
     std::uint32_t bestLanesPerBlock;
     std::uint32_t bestJobsPerBlock;
+    std::vector<uint8_t*> setPasswordBuffers;
 
 public:
     std::size_t getBatchSize() const { return runner.getBatchSize(); }
@@ -28,10 +29,14 @@ public:
             bool bySegment = true, bool precomputeRefs = false);
 
     void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
 
-    void beginProcessing();
-    void endProcessing();
+    void runKernelAsync();
+    void fetchResultAsync(std::size_t index, void *dest);
+
+    void waitForResults();
+    bool resultsReady();
+
+    void reconfigureArgon(const Argon2Params *newParams, std::uint32_t batchSize);
 };
 
 } // namespace opencl
diff --git a/lib/argon2-cuda/globalcontext.cpp b/lib/argon2-cuda/globalcontext.cpp
index eb52e02..56040c9 100644
--- a/lib/argon2-cuda/globalcontext.cpp
+++ b/lib/argon2-cuda/globalcontext.cpp
@@ -25,7 +25,11 @@ GlobalContext::GlobalContext()
     : devices()
 {
     int count;
-    CudaException::check(cudaGetDeviceCount(&count));
+    cudaError_t res = cudaGetDeviceCount(&count);
+    if (res != cudaSuccess) {
+        std::cout << "Cannot get CUDA device count, aborting...." << std::endl;
+        exit(1);
+    }
 
     devices.reserve(count);
     for (int i = 0; i < count; i++) {
diff --git a/lib/argon2-cuda/kernels.cu b/lib/argon2-cuda/kernels.cu
index 79fb767..9d343ed 100644
--- a/lib/argon2-cuda/kernels.cu
+++ b/lib/argon2-cuda/kernels.cu
@@ -3,9 +3,14 @@
 #define __CUDACC__
 #endif
 
+#include <cuda_runtime.h>
+
 #include "kernels.h"
 #include "cudaexception.h"
 
+#include <iostream>
+#include <iomanip>
+
 #include <stdexcept>
 #ifndef NDEBUG
 #include <iostream>
@@ -49,8 +54,8 @@ __device__ uint64_t u64_shuffle(uint64_t v, uint32_t thread)
 {
     uint32_t lo = u64_lo(v);
     uint32_t hi = u64_hi(v);
-    lo = __shfl(lo, thread);
-    hi = __shfl(hi, thread);
+    lo = __shfl_sync(0xFFFFFFFF, lo, thread);
+    hi = __shfl_sync(0xFFFFFFFF, hi, thread);
     return u64_build(hi, lo);
 }
 
@@ -776,9 +781,6 @@ KernelRunner::KernelRunner(uint32_t type, uint32_t version, uint32_t passes,
 
     CudaException::check(cudaMalloc(&memory, memorySize));
 
-    CudaException::check(cudaEventCreate(&start));
-    CudaException::check(cudaEventCreate(&end));
-
     CudaException::check(cudaStreamCreate(&stream));
 
     if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
@@ -828,12 +830,6 @@ void KernelRunner::precomputeRefs()
 
 KernelRunner::~KernelRunner()
 {
-    if (start != nullptr) {
-        cudaEventDestroy(start);
-    }
-    if (end != nullptr) {
-        cudaEventDestroy(end);
-    }
     if (stream != nullptr) {
         cudaStreamDestroy(stream);
     }
@@ -845,6 +841,24 @@ KernelRunner::~KernelRunner()
     }
 }
 
+void KernelRunner::syncStream() {
+    CudaException::check(cudaStreamSynchronize(stream));
+}
+
+bool KernelRunner::streamOperationsComplete() {
+    cudaError_t res = cudaStreamQuery(stream);
+    if (res == cudaSuccess) {
+        return true;
+    }
+    else if (res == cudaErrorNotReady) {
+        return false;
+    }
+    else {
+        CudaException::check(res);
+        return false;
+    }
+}
+
 void KernelRunner::writeInputMemory(uint32_t jobId, const void *buffer)
 {
     std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
@@ -854,7 +868,6 @@ void KernelRunner::writeInputMemory(uint32_t jobId, const void *buffer)
     auto mem = static_cast<uint8_t *>(memory) + offset;
     CudaException::check(cudaMemcpyAsync(mem, buffer, size,
                                          cudaMemcpyHostToDevice, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
 }
 
 void KernelRunner::readOutputMemory(uint32_t jobId, void *buffer)
@@ -866,7 +879,6 @@ void KernelRunner::readOutputMemory(uint32_t jobId, void *buffer)
     auto mem = static_cast<uint8_t *>(memory) + offset;
     CudaException::check(cudaMemcpyAsync(buffer, mem, size,
                                          cudaMemcpyDeviceToHost, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
 }
 
 void KernelRunner::runKernelSegment(uint32_t lanesPerBlock,
@@ -1028,8 +1040,6 @@ void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
 
 void KernelRunner::run(uint32_t lanesPerBlock, uint32_t jobsPerBlock)
 {
-    CudaException::check(cudaEventRecord(start, stream));
-
     if (bySegment) {
         for (uint32_t pass = 0; pass < passes; pass++) {
             for (uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
@@ -1041,17 +1051,28 @@ void KernelRunner::run(uint32_t lanesPerBlock, uint32_t jobsPerBlock)
     }
 
     CudaException::check(cudaGetLastError());
-
-    CudaException::check(cudaEventRecord(end, stream));
 }
 
 float KernelRunner::finish()
 {
-    CudaException::check(cudaStreamSynchronize(stream));
+    return 0.f;
+}
+
+void KernelRunner::reconfigureArgon(
+    std::uint32_t newPasses,
+    std::uint32_t newLanes,
+    std::uint32_t newSegmentBlocks,
+    std::uint32_t newBatchSize)
+{
+    if (precompute || bySegment) {
+        printf("reconfigureArgon not supported with precompute/bySegment modes !\n");
+        exit(1);
+    }
 
-    float time = 0.0;
-    CudaException::check(cudaEventElapsedTime(&time, start, end));
-    return time;
+    passes = newPasses;
+    lanes = newLanes;
+    segmentBlocks = newSegmentBlocks;
+    batchSize = newBatchSize;
 }
 
 } // cuda
diff --git a/lib/argon2-cuda/processingunit.cpp b/lib/argon2-cuda/processingunit.cpp
index 7768427..9f62cc8 100644
--- a/lib/argon2-cuda/processingunit.cpp
+++ b/lib/argon2-cuda/processingunit.cpp
@@ -24,6 +24,9 @@ static bool isPowerOfTwo(std::uint32_t x)
     return (x & (x - 1)) == 0;
 }
 
+#pragma warning(disable:4267)
+#pragma warning(disable:4101)
+
 ProcessingUnit::ProcessingUnit(
         const ProgramContext *programContext, const Argon2Params *params,
         const Device *device, std::size_t batchSize, bool bySegment,
@@ -36,18 +39,33 @@ ProcessingUnit::ProcessingUnit(
       bestLanesPerBlock(runner.getMinLanesPerBlock()),
       bestJobsPerBlock(runner.getMinJobsPerBlock())
 {
+    // already done by caller, but let's still do it again just in case ...
+    // (it is done by the caller because the runner constructor also needs current device set !)
     setCudaDevice(device->getDeviceIndex());
 
+    // preallocate buffers used by ProcessingUnit::setPassword
+    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
+    for (int i = 0; i < batchSize; i++) {
+        uint8_t* ptrPinnedMem = nullptr;
+        cudaError_t status = cudaMallocHost((void**)&(ptrPinnedMem), size);
+        if (status != cudaSuccess) {
+            std::cout << "Error allocating pinned host memory" << std::endl;
+            exit(1);
+        }
+        setPasswordBuffers.push_back(ptrPinnedMem);
+    }
+
     /* pre-fill first blocks with pseudo-random data: */
     for (std::size_t i = 0; i < batchSize; i++) {
         setPassword(i, NULL, 0);
     }
 
+#if 0
     if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
             && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
+//#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
@@ -58,36 +76,36 @@ ProcessingUnit::ProcessingUnit(
                 runner.run(lpb, bestJobsPerBlock);
                 time = runner.finish();
             } catch(CudaException &ex) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
                 std::cerr << "[WARN]   CUDA error on " << lpb
                           << " lanes per block: " << ex.what() << std::endl;
-#endif
+//#endif
                 break;
             }
 
-#ifndef NDEBUG
+//#ifndef NDEBUG
             std::cerr << "[INFO]   " << lpb << " lanes per block: "
                       << time << " ms" << std::endl;
-#endif
+//#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestLanesPerBlock = lpb;
             }
         }
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Picked " << bestLanesPerBlock
                   << " lanes per block." << std::endl;
-#endif
+//#endif
     }
 
     /* Only tune jobs per block if we hit maximum lanes per block: */
     if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
             && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
             && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
+//#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
@@ -98,51 +116,56 @@ ProcessingUnit::ProcessingUnit(
                 runner.run(bestLanesPerBlock, jpb);
                 time = runner.finish();
             } catch(CudaException &ex) {
-#ifndef NDEBUG
+//#ifndef NDEBUG
                 std::cerr << "[WARN]   CUDA error on " << jpb
                           << " jobs per block: " << ex.what() << std::endl;
-#endif
+//#endif
                 break;
             }
 
-#ifndef NDEBUG
+//#ifndef NDEBUG
             std::cerr << "[INFO]   " << jpb << " jobs per block: "
                       << time << " ms" << std::endl;
-#endif
+//#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestJobsPerBlock = jpb;
             }
         }
-#ifndef NDEBUG
+//#ifndef NDEBUG
         std::cerr << "[INFO] Picked " << bestJobsPerBlock
                   << " jobs per block." << std::endl;
-#endif
+//#endif
     }
+#endif
 }
 
 void ProcessingUnit::setPassword(std::size_t index, const void *pw,
                                  std::size_t pwSize)
 {
-    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    params->fillFirstBlocks(buffer.get(), pw, pwSize,
+    params->fillFirstBlocks(setPasswordBuffers[index], pw, pwSize,
                             programContext->getArgon2Type(),
                             programContext->getArgon2Version());
-    runner.writeInputMemory(index, buffer.get());
+
+    runner.writeInputMemory(index, setPasswordBuffers[index]);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    std::size_t size = params->getLanes() * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    runner.readOutputMemory(index, buffer.get());
-    params->finalize(hash, buffer.get());
+void ProcessingUnit::fetchResultAsync(std::size_t index, void *dest) {
+    runner.readOutputMemory(index, dest);
+}
+
+void ProcessingUnit::syncStream() {
+    runner.syncStream();
+}
+
+bool ProcessingUnit::streamOperationsComplete() {
+    return runner.streamOperationsComplete();
 }
 
 void ProcessingUnit::beginProcessing()
 {
+    // we MUST set cuda device before launching a kernel
     setCudaDevice(device->getDeviceIndex());
     runner.run(bestLanesPerBlock, bestJobsPerBlock);
 }
@@ -152,5 +175,15 @@ void ProcessingUnit::endProcessing()
     runner.finish();
 }
 
+void ProcessingUnit::reconfigureArgon(const Argon2Params *newParams, std::uint32_t batchSize) {
+    params = newParams;
+    runner.reconfigureArgon(
+        params->getTimeCost(),
+        params->getLanes(),
+        params->getSegmentBlocks(),
+        batchSize);
+    bestLanesPerBlock = runner.getMinLanesPerBlock();
+    bestJobsPerBlock = runner.getMinJobsPerBlock();
+}
 } // namespace cuda
 } // namespace argon2
diff --git a/lib/argon2-gpu-common/argon2params.cpp b/lib/argon2-gpu-common/argon2params.cpp
index dd122d8..b74c11d 100644
--- a/lib/argon2-gpu-common/argon2params.cpp
+++ b/lib/argon2-gpu-common/argon2params.cpp
@@ -20,6 +20,8 @@ static void store32(void *dst, std::uint32_t v)
     *out++ = static_cast<std::uint8_t>(v);
 }
 
+#pragma warning(disable:4267)
+
 Argon2Params::Argon2Params(
         std::size_t outLen,
         const void *salt, std::size_t saltLen,
diff --git a/lib/argon2-gpu-common/blake2b.h b/lib/argon2-gpu-common/blake2b.h
index 094fb83..7f438ee 100644
--- a/lib/argon2-gpu-common/blake2b.h
+++ b/lib/argon2-gpu-common/blake2b.h
@@ -2,6 +2,7 @@
 #define ARGON2_BLAKE2B_H
 
 #include <cstdint>
+#include <cstddef>
 
 namespace argon2 {
 
diff --git a/lib/argon2-opencl/kernelloader.cpp b/lib/argon2-opencl/kernelloader.cpp
index 22949f3..927a44d 100644
--- a/lib/argon2-opencl/kernelloader.cpp
+++ b/lib/argon2-opencl/kernelloader.cpp
@@ -38,6 +38,7 @@ cl::Program KernelLoader::loadArgon2Program(
         for (cl::Device &device : context.getInfo<CL_CONTEXT_DEVICES>()) {
             std::cerr << "  Build log from device '" << device.getInfo<CL_DEVICE_NAME>() << "':" << std::endl;
             std::cerr << prog.getBuildInfo<CL_PROGRAM_BUILD_LOG>(device);
+			err;
         }
         throw;
     }
diff --git a/lib/argon2-opencl/kernelrunner.cpp b/lib/argon2-opencl/kernelrunner.cpp
index 9fe39b4..46a3f51 100644
--- a/lib/argon2-opencl/kernelrunner.cpp
+++ b/lib/argon2-opencl/kernelrunner.cpp
@@ -1,20 +1,68 @@
 #include "kernelrunner.h"
 
 #include <stdexcept>
-
-#ifndef NDEBUG
+#include <thread>
 #include <iostream>
-#endif
+#include <iomanip>
+#include <map>
 
 #define THREADS_PER_LANE 32
 
+//#define SINGLE_QUEUE_PER_DEVICE
+//#define FLUSH_ALL
+//#define SKIP_MEM_TRANSFERS
+
+#include "../../include/perfscope.h"
+
 namespace argon2 {
 namespace opencl {
 
+#ifdef SINGLE_QUEUE_PER_DEVICE
+std::map<const Device*, cl::CommandQueue*> s_queues;
+#endif
+
 enum {
     ARGON2_REFS_PER_BLOCK = ARGON2_BLOCK_SIZE / (2 * sizeof(cl_uint)),
 };
 
+void KernelRunner::allocateAndPrecomputeRefs() {
+    auto context = programContext->getContext();
+    std::uint32_t passes = params->getTimeCost();
+    std::uint32_t lanes = params->getLanes();
+    std::uint32_t segmentBlocks = params->getSegmentBlocks();
+
+    Type type = programContext->getArgon2Type();
+    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
+        std::uint32_t segments =
+            type == ARGON2_ID
+            ? lanes * (ARGON2_SYNC_POINTS / 2)
+            : passes * lanes * ARGON2_SYNC_POINTS;
+        std::size_t refsSize = segments * segmentBlocks * sizeof(cl_uint) * 2;
+        std::cout << "[INFO] Allocating " << std::fixed << std::setprecision(2) << (double)refsSize / (1024.0 * 1024.0) << " MB for refs..." << std::endl;
+        refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
+        precomputeRefs();
+    }
+}
+
+void KernelRunner::setKernelsArgs() {
+    std::uint32_t passes = params->getTimeCost();
+    std::uint32_t lanes = params->getLanes();
+    std::uint32_t segmentBlocks = params->getSegmentBlocks();
+
+    kernel.setArg<cl::Buffer>(1, memoryBuffer);
+    if (precompute) {
+        kernel.setArg<cl::Buffer>(2, refsBuffer);
+        kernel.setArg<cl_uint>(3, passes);
+        kernel.setArg<cl_uint>(4, lanes);
+        kernel.setArg<cl_uint>(5, segmentBlocks);
+    }
+    else {
+        kernel.setArg<cl_uint>(2, passes);
+        kernel.setArg<cl_uint>(3, lanes);
+        kernel.setArg<cl_uint>(4, segmentBlocks);
+    }
+}
+
 KernelRunner::KernelRunner(const ProgramContext *programContext,
                            const Argon2Params *params, const Device *device,
                            std::uint32_t batchSize, bool bySegment, bool precompute)
@@ -27,35 +75,24 @@ KernelRunner::KernelRunner(const ProgramContext *programContext,
     std::uint32_t lanes = params->getLanes();
     std::uint32_t segmentBlocks = params->getSegmentBlocks();
 
-    queue = cl::CommandQueue(context, device->getCLDevice(),
-                             CL_QUEUE_PROFILING_ENABLE);
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
+    cl_command_queue_properties props = CL_QUEUE_PROFILING_ENABLE;
+#ifdef SINGLE_QUEUE_PER_DEVICE
+    auto it = s_queues.find(device);
+    if (it == s_queues.end()) {
+        s_queues.insert(
+            std::make_pair(
+                device,
+                new cl::CommandQueue(context, device->getCLDevice(), props)));
+    }
+    queue = s_queues[device];
+#else
+    queue = new cl::CommandQueue(context, device->getCLDevice(), props);
 #endif
 
     memoryBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, memorySize);
 
-    Type type = programContext->getArgon2Type();
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
-        std::uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
-
-        std::size_t refsSize = segments * segmentBlocks * sizeof(cl_uint) * 2;
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
-#endif
-
-        refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
-
-        precomputeRefs();
-    }
-
+    allocateAndPrecomputeRefs();
+   
     static const char *KERNEL_NAMES[2][2] = {
         {
             "argon2_kernel_oneshot",
@@ -69,17 +106,23 @@ KernelRunner::KernelRunner(const ProgramContext *programContext,
 
     kernel = cl::Kernel(programContext->getProgram(),
                         KERNEL_NAMES[precompute][bySegment]);
-    kernel.setArg<cl::Buffer>(1, memoryBuffer);
-    if (precompute) {
-        kernel.setArg<cl::Buffer>(2, refsBuffer);
-        kernel.setArg<cl_uint>(3, passes);
-        kernel.setArg<cl_uint>(4, lanes);
-        kernel.setArg<cl_uint>(5, segmentBlocks);
-    } else {
-        kernel.setArg<cl_uint>(2, passes);
-        kernel.setArg<cl_uint>(3, lanes);
-        kernel.setArg<cl_uint>(4, segmentBlocks);
+    
+    setKernelsArgs();
+}
+
+void KernelRunner::reconfigureArgon(const Argon2Params *newParams, std::uint32_t newBatchSize) {
+    if (bySegment) {
+        printf("reconfigureArgon not supported with bySegment mode !\n");
+        exit(1);
     }
+
+    params = newParams;
+    batchSize = newBatchSize;
+    memorySize = params->getMemorySize() * static_cast<std::size_t>(batchSize);
+
+    allocateAndPrecomputeRefs();
+    
+    setKernelsArgs();
 }
 
 void KernelRunner::precomputeRefs()
@@ -106,36 +149,60 @@ void KernelRunner::precomputeRefs()
 
     cl::NDRange globalRange { THREADS_PER_LANE * segments * segmentAddrBlocks };
     cl::NDRange localRange { THREADS_PER_LANE };
-    queue.enqueueNDRangeKernel(kernel, cl::NullRange, globalRange, localRange);
-    queue.finish();
+
+    queue->enqueueNDRangeKernel(kernel, cl::NullRange, globalRange, localRange);
+    queue->finish();
 }
 
-void *KernelRunner::mapInputMemory(std::uint32_t jobId)
-{
+void KernelRunner::uploadToInputMemoryAsync(std::uint32_t jobId, const void* srcPtr) {
     std::size_t memorySize = params->getMemorySize();
     std::size_t mappedSize = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_WRITE,
-                                  memorySize * jobId, mappedSize);
-}
-
-void KernelRunner::unmapInputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
+    bool blocking = false;
+    size_t offset = memorySize * jobId;
+    size_t size = mappedSize;
+#ifndef SKIP_MEM_TRANSFERS
+    {
+        PerfScope p("enqueueWriteBuffer");
+        cl_int res = queue->enqueueWriteBuffer(
+            memoryBuffer,
+            blocking,
+            offset,
+            size,
+            srcPtr,
+            NULL,
+            NULL);
+    }
+#endif
+#ifdef FLUSH_ALL
+    queue->flush();
+#endif
 }
 
-void *KernelRunner::mapOutputMemory(std::uint32_t jobId)
-{
+void KernelRunner::fetchOutputMemoryAsync(std::uint32_t jobId, void* dstPtr) {
     std::size_t memorySize = params->getMemorySize();
-    std::size_t mappedSize = static_cast<std::size_t>(params->getLanes())
-            * ARGON2_BLOCK_SIZE;
+    std::size_t mappedSize = static_cast<std::size_t>(params->getLanes()) * ARGON2_BLOCK_SIZE;
     std::size_t mappedOffset = memorySize * (jobId + 1) - mappedSize;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_READ,
-                                  mappedOffset, mappedSize);
-}
-
-void KernelRunner::unmapOutputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
+    bool blocking = false;
+#ifndef SKIP_MEM_TRANSFERS
+    {
+        PerfScope p("enqueueReadBuffer");
+        cl_int res = queue->enqueueReadBuffer(
+            memoryBuffer,
+            blocking,
+            mappedOffset,
+            mappedSize,
+            dstPtr,
+            NULL,
+            NULL);
+    }
+#endif
+    {
+        PerfScope p("enqueueMarker");
+        queue->enqueueMarker(&end);
+    }
+#ifdef FLUSH_ALL
+    queue->flush();
+#endif
 }
 
 void KernelRunner::run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock)
@@ -160,36 +227,51 @@ void KernelRunner::run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock)
     cl::NDRange globalRange { THREADS_PER_LANE * lanes, batchSize };
     cl::NDRange localRange { THREADS_PER_LANE * lanesPerBlock, jobsPerBlock };
 
-    queue.enqueueMarker(&start);
-
-    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * jobsPerBlock
-            * sizeof(cl_uint) * 2;
+    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * jobsPerBlock * sizeof(cl_uint) * 2;
     kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
     if (bySegment) {
         for (std::uint32_t pass = 0; pass < passes; pass++) {
             for (std::uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
                 kernel.setArg<cl_uint>(precompute ? 6 : 5, pass);
                 kernel.setArg<cl_uint>(precompute ? 7 : 6, slice);
-                queue.enqueueNDRangeKernel(kernel, cl::NullRange,
+                queue->enqueueNDRangeKernel(kernel, cl::NullRange,
                                            globalRange, localRange);
             }
         }
     } else {
-        queue.enqueueNDRangeKernel(kernel, cl::NullRange,
-                                   globalRange, localRange);
+        {
+            PerfScope p("enqueueNDRangeKernel");
+            queue->enqueueNDRangeKernel(kernel, cl::NullRange,
+                globalRange, localRange);
+        }
     }
 
-    queue.enqueueMarker(&end);
+#ifdef FLUSH_ALL
+   queue->flush();
+#endif
 }
 
-float KernelRunner::finish()
-{
-    end.wait();
+void KernelRunner::waitForResults() {
+    {
+      PerfScope p("end.wait()");
+        end.wait();
+    }
+}
+
+bool KernelRunner::resultsReady() {
+    // seems like some platforms (ex: Intel Integrated) need the flush call
+    cl::detail::errHandler(queue->flush(), "KernelRunner::resultsReady, flushing queue");
 
-    cl_ulong nsStart = start.getProfilingInfo<CL_PROFILING_COMMAND_END>();
-    cl_ulong nsEnd   = end.getProfilingInfo<CL_PROFILING_COMMAND_END>();
+    cl_int err = NULL;
+    auto status = end.getInfo<CL_EVENT_COMMAND_EXECUTION_STATUS>(&err);
+    cl::detail::errHandler(err, "KernelRunner::resultsReady");
 
-    return (nsEnd - nsStart) / (1000.0F * 1000.0F);
+    if (status == CL_COMPLETE) {
+        return true;
+    }
+    else {
+        return false;
+    }
 }
 
 } // namespace opencl
diff --git a/lib/argon2-opencl/processingunit.cpp b/lib/argon2-opencl/processingunit.cpp
index 598ced7..fb34a0a 100644
--- a/lib/argon2-opencl/processingunit.cpp
+++ b/lib/argon2-opencl/processingunit.cpp
@@ -13,6 +13,9 @@ static bool isPowerOfTwo(std::uint32_t x)
     return (x & (x - 1)) == 0;
 }
 
+#pragma warning(disable:4267)
+#pragma warning(disable:4101)
+
 ProcessingUnit::ProcessingUnit(
         const ProgramContext *programContext, const Argon2Params *params,
         const Device *device, std::size_t batchSize,
@@ -23,115 +26,139 @@ ProcessingUnit::ProcessingUnit(
       bestLanesPerBlock(runner.getMinLanesPerBlock()),
       bestJobsPerBlock(runner.getMinJobsPerBlock())
 {
+    // preallocate buffers used by ProcessingUnit::setPassword
+    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
+    for (int i = 0; i < batchSize; i++) {
+        uint8_t* ptrPinnedMem = new uint8_t[size];
+        setPasswordBuffers.push_back(ptrPinnedMem);
+    }
+
     /* pre-fill first blocks with pseudo-random data: */
     for (std::size_t i = 0; i < batchSize; i++) {
         setPassword(i, NULL, 0);
     }
 
+#if 0
     if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
-            && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
+        && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
+        //#ifndef NDEBUG
+        std::cout << "[INFO] Tuning lanes per block..." << std::endl;
+        //#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
-             lpb *= 2)
+            lpb *= 2)
         {
             float time;
             try {
-                runner.run(lpb, bestJobsPerBlock);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   OpenCL error on " << lpb
-                          << " lanes per block: " << ex.what() << std::endl;
-#endif
+                //runner.run(lpb, bestJobsPerBlock);
+                //time = runner.finish();
+            }
+            catch (cl::Error &ex) {
+                //#ifndef NDEBUG
+                std::cout << "[WARN]   OpenCL error on " << lpb
+                    << " lanes per block: " << ex.what() << std::endl;
+                //#endif
                 break;
             }
 
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << lpb << " lanes per block: "
-                      << time << " ms" << std::endl;
-#endif
+            //#ifndef NDEBUG
+            std::cout << "[INFO]   " << lpb << " lanes per block: "
+                << time << " ms" << std::endl;
+            //#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestLanesPerBlock = lpb;
             }
         }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestLanesPerBlock
-                  << " lanes per block." << std::endl;
-#endif
+        //#ifndef NDEBUG
+        std::cout << "[INFO] Picked " << bestLanesPerBlock
+            << " lanes per block." << std::endl;
+        //#endif
     }
 
     /* Only tune jobs per block if we hit maximum lanes per block: */
     if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
-            && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
-            && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
+        && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
+        && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
+
+        //#ifndef NDEBUG
         std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
+        //#endif
 
         float bestTime = std::numeric_limits<float>::infinity();
         for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
-             jpb *= 2)
+            jpb *= 2)
         {
             float time;
             try {
-                runner.run(bestLanesPerBlock, jpb);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
+                //runner.run(bestLanesPerBlock, jpb);
+                //time = runner.finish();
+            }
+            catch (cl::Error &ex) {
+                //#ifndef NDEBUG
                 std::cerr << "[WARN]   OpenCL error on " << jpb
-                          << " jobs per block: " << ex.what() << std::endl;
-#endif
+                    << " jobs per block: " << ex.what() << std::endl;
+                //#endif
                 break;
             }
 
-#ifndef NDEBUG
+            //#ifndef NDEBUG
             std::cerr << "[INFO]   " << jpb << " jobs per block: "
-                      << time << " ms" << std::endl;
-#endif
+                << time << " ms" << std::endl;
+            //#endif
 
             if (time < bestTime) {
                 bestTime = time;
                 bestJobsPerBlock = jpb;
             }
         }
-#ifndef NDEBUG
+
+
+        //bestJobsPerBlock = 128;
+
+
+        //#ifndef NDEBUG
         std::cerr << "[INFO] Picked " << bestJobsPerBlock
-                  << " jobs per block." << std::endl;
-#endif
+            << " jobs per block." << std::endl;
+        //#endif
     }
+#endif
 }
 
 void ProcessingUnit::setPassword(std::size_t index, const void *pw,
                                  std::size_t pwSize)
 {
-    void *memory = runner.mapInputMemory(index);
-    params->fillFirstBlocks(memory, pw, pwSize,
+    params->fillFirstBlocks(setPasswordBuffers[index], pw, pwSize,
                             programContext->getArgon2Type(),
                             programContext->getArgon2Version());
-    runner.unmapInputMemory(memory);
+
+    runner.uploadToInputMemoryAsync(index, setPasswordBuffers[index]);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    void *memory = runner.mapOutputMemory(index);
-    params->finalize(hash, memory);
-    runner.unmapOutputMemory(memory);
+void ProcessingUnit::fetchResultAsync(std::size_t index, void *dest) {
+    runner.fetchOutputMemoryAsync(index, dest);
 }
 
-void ProcessingUnit::beginProcessing()
+void ProcessingUnit::runKernelAsync()
 {
     runner.run(bestLanesPerBlock, bestJobsPerBlock);
 }
 
-void ProcessingUnit::endProcessing()
-{
-    runner.finish();
+void ProcessingUnit::waitForResults() {
+    runner.waitForResults();
+}
+
+bool ProcessingUnit::resultsReady() {
+    return runner.resultsReady();
+}
+
+void ProcessingUnit::reconfigureArgon(const Argon2Params *newParams, std::uint32_t batchSize) {
+    params = newParams;
+    runner.reconfigureArgon(params, batchSize);
+    bestLanesPerBlock = runner.getMinLanesPerBlock();
+    bestJobsPerBlock = runner.getMinJobsPerBlock();
 }
 
 } // namespace opencl
