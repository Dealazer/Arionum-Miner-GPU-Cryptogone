diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3dd95ba..d8e67d9 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1,4 +1,4 @@
-cmake_minimum_required(VERSION 3.7)
+cmake_minimum_required(VERSION 3.5)
 
 project(argon2-gpu CXX)
 set(BINARY_INSTALL_DIR /usr/local/bin)
@@ -28,9 +28,17 @@ if(CUDA_FOUND)
     )
 endif()
 
+FIND_PACKAGE(OpenCL)
+INCLUDE_DIRECTORIES(${OPENCL_INCLUDE_DIR})
+if (OPENCL_FOUND)
+    message("INFO: Using OPENCL version ${OpenCL_VERSION_MAJOR}.${OpenCL_VERSION_MINOR}")
+else()
+    message("INFO: OPENCL not found")
+endif()
+
 add_subdirectory(ext/argon2)
 
-add_library(argon2-gpu-common SHARED
+add_library(argon2-gpu-common STATIC
     lib/argon2-gpu-common/argon2params.cpp
     lib/argon2-gpu-common/blake2b.cpp
 )
@@ -44,15 +52,19 @@ target_include_directories(argon2-gpu-common PRIVATE
 )
 
 if(CUDA_FOUND)
-    cuda_add_library(argon2-cuda SHARED
+    cuda_compile_ptx (ptxfiles
+	lib/argon2-cuda/kernels.cu)
+
+    cuda_add_library(argon2-cuda STATIC
         lib/argon2-cuda/device.cpp
         lib/argon2-cuda/globalcontext.cpp
         lib/argon2-cuda/programcontext.cpp
         lib/argon2-cuda/processingunit.cpp
         lib/argon2-cuda/kernels.cu
+        ${ptxfiles}
     )
 else()
-    add_library(argon2-cuda SHARED
+    add_library(argon2-cuda STATIC
         lib/argon2-cuda/nocuda.cpp
     )
 endif()
@@ -67,7 +79,7 @@ target_include_directories(argon2-cuda INTERFACE
 )
 target_link_libraries(argon2-cuda argon2-gpu-common)
 
-add_library(argon2-opencl SHARED
+add_library(argon2-opencl STATIC
     lib/argon2-opencl/device.cpp
     lib/argon2-opencl/globalcontext.cpp
     lib/argon2-opencl/kernelloader.cpp
@@ -84,56 +96,5 @@ target_include_directories(argon2-opencl PRIVATE
     lib/argon2-opencl
 )
 target_link_libraries(argon2-opencl
-    argon2-gpu-common -lOpenCL
-)
-
-add_executable(argon2-gpu-test
-    src/argon2-gpu-test/main.cpp
-    src/argon2-gpu-test/testcase.cpp
-)
-target_include_directories(argon2-gpu-test PRIVATE src/argon2-gpu-test)
-target_link_libraries(argon2-gpu-test
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_executable(argon2-gpu-bench
-    src/argon2-gpu-bench/cpuexecutive.cpp
-    src/argon2-gpu-bench/cudaexecutive.cpp
-    src/argon2-gpu-bench/openclexecutive.cpp
-    src/argon2-gpu-bench/benchmark.cpp
-    src/argon2-gpu-bench/main.cpp
-)
-target_include_directories(argon2-gpu-bench PRIVATE src/argon2-gpu-bench)
-target_link_libraries(argon2-gpu-bench
-    argon2-cuda argon2-opencl argon2 -lOpenCL
-)
-
-add_test(argon2-gpu-test-opencl argon2-gpu-test -m opencl)
-add_test(argon2-gpu-test-cuda argon2-gpu-test -m cuda)
-
-install(
-    TARGETS argon2-gpu-common argon2-opencl argon2-cuda
-    DESTINATION ${LIBRARY_INSTALL_DIR}
-)
-install(FILES
-    include/argon2-gpu-common/argon2-common.h
-    include/argon2-gpu-common/argon2params.h
-    include/argon2-opencl/cl.hpp
-    include/argon2-opencl/opencl.h
-    include/argon2-opencl/device.h
-    include/argon2-opencl/globalcontext.h
-    include/argon2-opencl/programcontext.h
-    include/argon2-opencl/processingunit.h
-    include/argon2-opencl/kernelrunner.h
-    include/argon2-cuda/cudaexception.h
-    include/argon2-cuda/kernels.h
-    include/argon2-cuda/device.h
-    include/argon2-cuda/globalcontext.h
-    include/argon2-cuda/programcontext.h
-    include/argon2-cuda/processingunit.h
-    DESTINATION ${INCLUDE_INSTALL_DIR}
-)
-install(
-    TARGETS argon2-gpu-bench argon2-gpu-test
-    DESTINATION ${BINARY_INSTALL_DIR}
+    argon2-gpu-common ${OpenCL_LIBRARY}
 )
diff --git a/data/kernels/argon2_kernel.cl b/data/kernels/argon2_kernel.cl
index fa5e12a..b466102 100644
--- a/data/kernels/argon2_kernel.cl
+++ b/data/kernels/argon2_kernel.cl
@@ -35,6 +35,8 @@ typedef ptrdiff_t intptr_t;
 #define ARGON2_VERSION_10 0x10
 #define ARGON2_VERSION_13 0x13
 
+#define BLOCK_SIZE_ULONG (1024/8)
+
 #define ARGON2_BLOCK_SIZE 1024
 #define ARGON2_QWORDS_IN_BLOCK (ARGON2_BLOCK_SIZE / 8)
 #define ARGON2_SYNC_POINTS 4
@@ -50,6 +52,8 @@ typedef ptrdiff_t intptr_t;
 #define ARGON2_TYPE ARGON2_I
 #endif
 
+#define BLOCK_MEM(block_id) (__global ulong*)(memory + block_id)
+
 ulong u64_build(uint hi, uint lo)
 {
     return upsample(hi, lo);
@@ -132,6 +136,15 @@ void xor_block(struct block_th *dst, const struct block_th *src)
     dst->d ^= src->d;
 }
 
+void load_block_loc(local struct block_th *dst, __global const struct block_g *src,
+    uint thread)
+{
+    dst->a = src->data[0 * THREADS_PER_LANE + thread];
+    dst->b = src->data[1 * THREADS_PER_LANE + thread];
+    dst->c = src->data[2 * THREADS_PER_LANE + thread];
+    dst->d = src->data[3 * THREADS_PER_LANE + thread];
+}
+
 void load_block(struct block_th *dst, __global const struct block_g *src,
                 uint thread)
 {
@@ -150,6 +163,15 @@ void load_block_xor(struct block_th *dst, __global const struct block_g *src,
     dst->d ^= src->data[3 * THREADS_PER_LANE + thread];
 }
 
+void load_block_xor_loc(local struct block_th *dst, __global const struct block_g *src,
+    uint thread)
+{
+    dst->a ^= src->data[0 * THREADS_PER_LANE + thread];
+    dst->b ^= src->data[1 * THREADS_PER_LANE + thread];
+    dst->c ^= src->data[2 * THREADS_PER_LANE + thread];
+    dst->d ^= src->data[3 * THREADS_PER_LANE + thread];
+}
+
 void store_block(__global struct block_g *dst, const struct block_th *src,
                  uint thread)
 {
@@ -159,29 +181,10 @@ void store_block(__global struct block_g *dst, const struct block_th *src,
     dst->data[3 * THREADS_PER_LANE + thread] = src->d;
 }
 
-#ifdef cl_amd_media_ops
-#pragma OPENCL EXTENSION cl_amd_media_ops : enable
-
-ulong rotr64(ulong x, ulong n)
-{
-    uint lo = u64_lo(x);
-    uint hi = u64_hi(x);
-    uint r_lo, r_hi;
-    if (n < 32) {
-        r_lo = amd_bitalign(hi, lo, (uint)n);
-        r_hi = amd_bitalign(lo, hi, (uint)n);
-    } else {
-        r_lo = amd_bitalign(lo, hi, (uint)n - 32);
-        r_hi = amd_bitalign(hi, lo, (uint)n - 32);
-    }
-    return u64_build(r_hi, r_lo);
-}
-#else
 ulong rotr64(ulong x, ulong n)
 {
     return rotate(x, 64 - n);
 }
-#endif
 
 ulong f(ulong x, ulong y)
 {
@@ -407,244 +410,320 @@ void next_addresses(struct block_th *addr, struct block_th *tmp,
     xor_block(addr, tmp);
 }
 
-#if ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID
+#if (1) //ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID
 struct ref {
     uint ref_lane;
     uint ref_index;
 };
 
-/*
- * Refs hierarchy:
- * lanes -> passes -> slices -> blocks
- */
-__kernel void argon2_precompute_kernel(
-        __local struct u64_shuffle_buf *shuffle_bufs, __global struct ref *refs,
-        uint passes, uint lanes, uint segment_blocks)
-{
-    uint block_id = get_global_id(0) / THREADS_PER_LANE;
-    uint warp = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
+struct index {
+    uint refSlot;
+    uint storeSlot;
+};
 
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+void g_alt(local ulong *pA, local ulong *pB, local ulong *pC, local ulong *pD)
+{
+    ulong a = *pA;
+    ulong b = *pB;
+    ulong c = *pC;
+    ulong d = *pD;
 
-    uint segment_addr_blocks = (segment_blocks + ARGON2_QWORDS_IN_BLOCK - 1)
-            / ARGON2_QWORDS_IN_BLOCK;
-    uint block = block_id % segment_addr_blocks;
-    uint segment = block_id / segment_addr_blocks;
+    a = f(a, b);
+    d = rotr64(d ^ a, 32);
+    c = f(c, d);
+    b = rotr64(b ^ c, 24);
 
-    uint slice, pass, lane;
-#if ARGON2_TYPE == ARGON2_ID
-    slice = segment % (ARGON2_SYNC_POINTS / 2);
-    lane = segment / (ARGON2_SYNC_POINTS / 2);
-    pass = 0;
-#else
-    uint pass_id;
+    a = f(a, b);
+    d = rotr64(d ^ a, 16);
+    c = f(c, d);
+    b = rotr64(b ^ c, 63);
 
-    slice = segment % ARGON2_SYNC_POINTS;
-    pass_id = segment / ARGON2_SYNC_POINTS;
+    *pA = a;
+    *pB = b;
+    *pC = c;
+    *pD = d;
+}
+
+#define G_ALT_1(p, offA, offB, offC, offD) \
+{ \
+a = p[offA]; \
+b = p[offB]; \
+c = p[offC]; \
+d = p[offD]; \
+a = f(a, b); \
+d = rotr64(d ^ a, 32); \
+c = f(c, d); \
+b = rotr64(b ^ c, 24); \
+a = f(a, b); \
+d = rotr64(d ^ a, 16); \
+c = f(c, d); \
+b = rotr64(b ^ c, 63); \
+p[offB] = b; \
+p[offC] = c; \
+p[offD] = d; \
+}
+
+#define G_ALT_2(p, offA, offB, offC, offD) \
+{ \
+b = p[offB]; \
+c = p[offC]; \
+d = p[offD]; \
+a = f(a, b); \
+d = rotr64(d ^ a, 32); \
+c = f(c, d); \
+b = rotr64(b ^ c, 24); \
+a = f(a, b); \
+d = rotr64(d ^ a, 16); \
+c = f(c, d); \
+b = rotr64(b ^ c, 63); \
+p[offA] = a; \
+p[offB] = b; \
+p[offC] = c; \
+p[offD] = d; \
+}
+
+__constant int OFFS[4][16] = {
+    {
+        0, 4, 8, 12,
+        0, 5, 10, 15,
+        0, 32, 64, 96,
+        0, 33, 80, 113
+    },
+    {
+        1, 5, 9, 13,
+        1, 6, 11, 12,
+        1, 33, 65, 97,
+        1, 48, 81, 96
+    },
+    {
+        2, 6, 10, 14,
+        2, 7, 8, 13,
+        16, 48, 80, 112,
+        16, 49, 64, 97,
+    },
+    {
+        3, 7, 11, 15,
+        3, 4, 9, 14,
+        17, 49, 81, 113,
+        17, 32, 65, 112,
+    }
+};
 
-    pass = pass_id % passes;
-    lane = pass_id / passes;
-#endif
+__global struct block_g *selectMemoryRegion(
+    int batch_id,
+    __global struct block_g *memory,
+    uint memory_0_count,
+    __global struct block_g *memory_1,
+    uint memory_1_count,
+    __global struct block_g *memory_2,
+    uint memory_2_count,
+    __global struct block_g *memory_3,
+    uint memory_3_count,
+    int blocksPerBatch)
+{
+    if (batch_id >= 0 && batch_id < memory_0_count)
+        memory = memory + (size_t)(batch_id * blocksPerBatch);
+    batch_id -= memory_0_count;
+    if (batch_id >= 0 && batch_id < memory_1_count)
+        memory = memory_1 + (size_t)(batch_id * blocksPerBatch);
+    batch_id -= memory_1_count;
+    if (batch_id >= 0 && batch_id < memory_2_count)
+        memory = memory_2 + (size_t)(size_t)(batch_id * blocksPerBatch);
+    batch_id -= memory_2_count;
+    if (batch_id >= 0 && batch_id < memory_3_count)
+        memory = memory_3 + (size_t)(size_t)(batch_id * blocksPerBatch);
+    return memory;
+}
+
+__kernel void argon2_kernel_oneshot_precomputedIndex_localState(
+    __global struct block_g *memory,
+    uint memory_0_count,
+    __global struct block_g *memory_1,
+    uint memory_1_count,
+    __global struct block_g *memory_2,
+    uint memory_2_count,
+    __global struct block_g *memory_3,
+    uint memory_3_count,
+    __global const struct index *indexs,
+    uint nSteps,
+    uint blocksPerBatch)
+{
+    uint thread = get_local_id(0) % THREADS_PER_LANE; // [0-31]
+    int batch_id = get_global_id(1);
+
+    memory = selectMemoryRegion(
+        batch_id, memory, memory_0_count, memory_1, memory_1_count,
+        memory_2, memory_2_count, memory_3, memory_3_count, blocksPerBatch);
+
+#if 1 // CPU BLOCK, LOCAL_STATE, INDEX CACHE
+    uint thGroup = thread / 4;
+    uint subId = thread % 4;
+
+    __local ulong full_state[BLOCK_SIZE_ULONG];
+    __local ulong* p1 = full_state + 16 * thGroup;
+    __local ulong* p2 = full_state + 2 * thGroup;
+    __local struct index indexCache[THREADS_PER_LANE];
+    __constant int* offs = OFFS[subId];
+    __global const struct index * pIndex = indexs;
+    __global const struct index * pIndexEnd = indexs + (nSteps - 2);
+
+    ulong4 th_state = vload4(thread, BLOCK_MEM(1));
+
+    while (pIndex < pIndexEnd) {
+        uint nBlocks = min((uint)(pIndexEnd - pIndex), (uint)THREADS_PER_LANE);
+        indexCache[thread] = pIndex[thread];
+        barrier(CLK_LOCAL_MEM_FENCE);
+
+        for (uint i = 0; i < nBlocks; i++) {
+            ulong4 th_ref = vload4(thread, BLOCK_MEM(indexCache[i].refSlot));
+
+            th_state ^= th_ref;
+
+            ulong4 th_tmp = th_state;
+
+            vstore4(th_state, thread, full_state);
+            barrier(CLK_LOCAL_MEM_FENCE);
 
-    struct block_th addr, tmp;
+            ulong a, b, c, d;
+            G_ALT_1(p1, offs[0], offs[1], offs[2], offs[3]);
+            G_ALT_2(p1, offs[4], offs[5], offs[6], offs[7]);
+            barrier(CLK_LOCAL_MEM_FENCE);
 
-    uint thread_input;
-    switch (thread) {
-    case 0:
-        thread_input = pass;
-        break;
-    case 1:
-        thread_input = lane;
-        break;
-    case 2:
-        thread_input = slice;
-        break;
-    case 3:
-        thread_input = lanes * segment_blocks * ARGON2_SYNC_POINTS;
-        break;
-    case 4:
-        thread_input = passes;
-        break;
-    case 5:
-        thread_input = ARGON2_TYPE;
-        break;
-    case 6:
-        thread_input = block + 1;
-        break;
-    default:
-        thread_input = 0;
-        break;
-    }
+            G_ALT_1(p2, offs[8], offs[9], offs[10], offs[11]);
+            G_ALT_2(p2, offs[12], offs[13], offs[14], offs[15]);
+            barrier(CLK_LOCAL_MEM_FENCE);
 
-    next_addresses(&addr, &tmp, thread_input, thread, shuffle_buf);
+            th_state = vload4(thread, full_state) ^ th_tmp;
 
-    refs += segment * segment_blocks;
+            uint slot = indexCache[i].storeSlot;
+            if (slot != UINT_MAX)
+                vstore4(th_state, thread, BLOCK_MEM(slot));
 
-    for (uint i = 0; i < QWORDS_PER_THREAD; i++) {
-        uint pos = i * THREADS_PER_LANE + thread;
-        uint offset = block * ARGON2_QWORDS_IN_BLOCK + pos;
-        if (offset < segment_blocks) {
-            ulong v = block_th_get(&addr, i);
-            uint ref_index = u64_lo(v);
-            uint ref_lane  = u64_hi(v);
-
-            compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
-                            &ref_lane, &ref_index);
-
-            refs[offset].ref_index = ref_index;
-            refs[offset].ref_lane  = ref_lane;
+            pIndex++;
         }
     }
-}
 
-void argon2_step_precompute(
-        __global struct block_g *memory, __global struct block_g *mem_curr,
-        struct block_th *prev, struct block_th *tmp,
-        __local struct u64_shuffle_buf *shuffle_buf,
-        __global const struct ref **refs,
-        uint lanes, uint segment_blocks, uint thread,
-        uint lane, uint pass, uint slice, uint offset)
-{
-    uint ref_index, ref_lane;
-    bool data_independent;
-#if ARGON2_TYPE == ARGON2_I
-    data_independent = true;
-#elif ARGON2_TYPE == ARGON2_ID
-    data_independent = pass == 0 && slice < ARGON2_SYNC_POINTS / 2;
-#else
-    data_independent = false;
-#endif
-    if (data_independent) {
-        ref_index = (*refs)->ref_index;
-        ref_lane = (*refs)->ref_lane;
-        (*refs)++;
-    } else {
-        ulong v = u64_shuffle(prev->a, 0, thread, shuffle_buf);
-        ref_index = u64_lo(v);
-        ref_lane  = u64_hi(v);
+    vstore4(th_state, thread, BLOCK_MEM(blocksPerBatch - 1));
+#else // CPU BLOCK, LOCAL_STATE, 1.5
+    uint thGroup = thread / 4;
+    uint subId = thread % 4;
+    __local ulong full_state[1024 / 8];
+    __local ulong* p1 = full_state + 16 * thGroup;
+    __local ulong* p2 = full_state + 2 * thGroup;
+    __constant int* offs = OFFS[subId];
 
-        compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
-                        &ref_lane, &ref_index);
-    }
+    ulong4 th_state = vload4(thread, (__global ulong*)(memory + 1));
 
-    argon2_core(memory, mem_curr, prev, tmp, shuffle_buf, lanes, thread, pass,
-                ref_index, ref_lane);
-}
+    for (uint i = 2; i < nSteps; ++i)
+    {
+        ulong4 th_ref = vload4(thread, (global ulong*)(memory + indexs->refSlot));
 
-__kernel void argon2_kernel_segment_precompute(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, __global const struct ref *refs,
-        uint passes, uint lanes, uint segment_blocks,
-        uint pass, uint slice)
-{
-    uint job_id = get_global_id(1);
-    uint lane   = get_global_id(0) / THREADS_PER_LANE;
-    uint warp   = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
+        th_state ^= th_ref;
 
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+        ulong4 th_tmp = th_state;
 
-    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+        vstore4(th_state, thread, full_state);
+        barrier(CLK_LOCAL_MEM_FENCE);
 
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+        g_alt(p1 + offs[0], p1 + offs[1], p1 + offs[2], p1 + offs[3]);
+        g_alt(p1 + offs[4], p1 + offs[5], p1 + offs[6], p1 + offs[7]);
+        barrier(CLK_LOCAL_MEM_FENCE);
 
-    struct block_th prev, tmp;
+        g_alt(p2 + offs[8], p2 + offs[9], p2 + offs[10], p2 + offs[11]);
+        g_alt(p2 + offs[12], p2 + offs[13], p2 + offs[14], p2 + offs[15]);
+        barrier(CLK_LOCAL_MEM_FENCE);
 
-    __global struct block_g *mem_segment =
-            memory + slice * segment_blocks * lanes + lane;
-    __global struct block_g *mem_prev, *mem_curr;
-    uint start_offset = 0;
-    if (pass == 0) {
-        if (slice == 0) {
-            mem_prev = mem_segment + 1 * lanes;
-            mem_curr = mem_segment + 2 * lanes;
-            start_offset = 2;
-        } else {
-            mem_prev = mem_segment - lanes;
-            mem_curr = mem_segment;
-        }
-    } else {
-        mem_prev = mem_segment + (slice == 0 ? lane_blocks * lanes : 0) - lanes;
-        mem_curr = mem_segment;
-    }
+        th_state = vload4(thread, full_state) ^ th_tmp;
+        //barrier(CLK_LOCAL_MEM_FENCE);
 
-    load_block(&prev, mem_prev, thread);
+        uint slot = indexs->storeSlot;
+        if (slot != UINT_MAX)
+            vstore4(th_state, thread, (global ulong*)(memory + slot));
 
-#if ARGON2_TYPE == ARGON2_ID
-        if (pass == 0 && slice < ARGON2_SYNC_POINTS / 2) {
-            refs += lane * (lane_blocks / 2) + slice * segment_blocks;
-            refs += start_offset;
-        }
-#else
-        refs += (lane * passes + pass) * lane_blocks + slice * segment_blocks;
-        refs += start_offset;
-#endif
-
-    for (uint offset = start_offset; offset < segment_blocks; ++offset) {
-        argon2_step_precompute(
-                    memory, mem_curr, &prev, &tmp, shuffle_buf, &refs, lanes,
-                    segment_blocks, thread, lane, pass, slice, offset);
-
-        mem_curr += lanes;
+        indexs++;
     }
+
+    vstore4(th_state, thread, (global ulong*)(memory + blocksPerBatch - 1));
+#endif
 }
 
-__kernel void argon2_kernel_oneshot_precompute(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, __global const struct ref *refs,
-        uint passes, uint lanes, uint segment_blocks)
+__kernel void argon2_kernel_oneshot_precomputedIndex_shuffleBuf(
+    __local struct u64_shuffle_buf *shuffle_bufs,   // 0
+    __global struct block_g *memory,                // 1
+    uint memory_0_count,                            // 2
+    __global struct block_g *memory_1,              // 3
+    uint memory_1_count,                            // 4
+    __global struct block_g *memory_2,              // 5
+    uint memory_2_count,                            // 6
+    __global struct block_g *memory_3,              // 7
+    uint memory_3_count,                            // 8
+    __global const struct index *indexs,            // 9
+    uint nSteps,                                    // 10
+    uint blocksPerBatch)                            // 11
 {
-    uint job_id = get_global_id(1);
-    uint lane   = get_global_id(0) / THREADS_PER_LANE;
-    uint warp   = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
+    uint thread = get_local_id(0) % THREADS_PER_LANE; // [0-31]
+    int batch_id = get_global_id(1);
 
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
+    memory = selectMemoryRegion(
+        batch_id, memory, memory_0_count, memory_1, memory_1_count,
+        memory_2, memory_2_count, memory_3, memory_3_count, blocksPerBatch);
 
-    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+    struct block_th prev, tmp;
+    load_block(&prev, memory + 1, thread);
 
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[0];
 
-    struct block_th prev, tmp;
+#if 1 // CPU BLOCK, SHUFFLE, INDEX CACHE
+    __local struct index indexCache[THREADS_PER_LANE];
+    __global const struct index * pIndex = indexs;
+    __global const struct index * pIndexEnd = indexs + (nSteps - 2);
 
-    __global struct block_g *mem_lane = memory + lane;
-    __global struct block_g *mem_prev = mem_lane + 1 * lanes;
-    __global struct block_g *mem_curr = mem_lane + 2 * lanes;
+    while (pIndex < pIndexEnd) {
+        uint nBlocks = min((uint)(pIndexEnd - pIndex), (uint)THREADS_PER_LANE);
+        indexCache[thread] = pIndex[thread];
+        barrier(CLK_LOCAL_MEM_FENCE);
 
-    load_block(&prev, mem_prev, thread);
+        for (uint i = 0; i < nBlocks; i++) {
+            uint ref_index = indexCache[i].refSlot;
+            load_block_xor(&prev, BLOCK_MEM(ref_index), thread);
 
-#if ARGON2_TYPE == ARGON2_ID
-    refs += lane * (lane_blocks / 2) + 2;
-#else
-    refs += lane * passes * lane_blocks + 2;
-#endif
+            move_block(&tmp, &prev);
+            shuffle_block(&prev, thread, shuffle_buf);
+            xor_block(&prev, &tmp);
 
-    uint skip = 2;
-    for (uint pass = 0; pass < passes; ++pass) {
-        for (uint slice = 0; slice < ARGON2_SYNC_POINTS; ++slice) {
-            for (uint offset = 0; offset < segment_blocks; ++offset) {
-                if (skip > 0) {
-                    --skip;
-                    continue;
-                }
+            uint slot = indexCache[i].storeSlot;
+            if (slot != UINT_MAX)
+                store_block(BLOCK_MEM(slot), &prev, thread);
 
-                argon2_step_precompute(
-                            memory, mem_curr, &prev, &tmp, shuffle_buf, &refs,
-                            lanes, segment_blocks, thread,
-                            lane, pass, slice, offset);
+            pIndex++;
+        }
+    }
 
-                mem_curr += lanes;
-            }
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
+#else // CPU BLOCK, SHUFFLE, LEGACY
+    for (uint i = 2; i < nSteps; ++i) {
+        uint ref_index = indexs->refSlot;
+        __global struct block_g *mem_ref = memory + ref_index;
 
-            barrier(CLK_LOCAL_MEM_FENCE);
+        load_block_xor(&prev, mem_ref, thread);
+
+        move_block(&tmp, &prev);
+        shuffle_block(&prev, thread, shuffle_buf);
+        xor_block(&prev, &tmp);
+
+        uint slot = indexs->storeSlot;
+        if (slot != UINT_MAX) {
+            __global struct block_g *mem_curr = memory + slot;
+            store_block(mem_curr, &prev, thread);
         }
 
-        mem_curr = mem_lane;
+        indexs++;
     }
+
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
+#endif
 }
+
 #endif /* ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID */
 
 void argon2_step(
@@ -692,94 +771,21 @@ void argon2_step(
                 ref_index, ref_lane);
 }
 
-__kernel void argon2_kernel_segment(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, uint passes, uint lanes,
-        uint segment_blocks, uint pass, uint slice)
-{
-    uint job_id = get_global_id(1);
-    uint lane   = get_global_id(0) / THREADS_PER_LANE;
-    uint warp   = get_local_id(0) / THREADS_PER_LANE;
-    uint thread = get_local_id(0) % THREADS_PER_LANE;
-
-    __local struct u64_shuffle_buf *shuffle_buf = &shuffle_bufs[warp];
-
-    uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
-
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
-
-    struct block_th prev, addr, tmp;
-    uint thread_input;
-
-#if ARGON2_TYPE == ARGON2_I || ARGON2_TYPE == ARGON2_ID
-    switch (thread) {
-    case 0:
-        thread_input = pass;
-        break;
-    case 1:
-        thread_input = lane;
-        break;
-    case 2:
-        thread_input = slice;
-        break;
-    case 3:
-        thread_input = lanes * lane_blocks;
-        break;
-    case 4:
-        thread_input = passes;
-        break;
-    case 5:
-        thread_input = ARGON2_TYPE;
-        break;
-    default:
-        thread_input = 0;
-        break;
-    }
-
-    if (pass == 0 && slice == 0 && segment_blocks > 2) {
-        if (thread == 6) {
-            ++thread_input;
-        }
-        next_addresses(&addr, &tmp, thread_input, thread, shuffle_buf);
-    }
-#endif
-
-    __global struct block_g *mem_segment =
-            memory + slice * segment_blocks * lanes + lane;
-    __global struct block_g *mem_prev, *mem_curr;
-    uint start_offset = 0;
-    if (pass == 0) {
-        if (slice == 0) {
-            mem_prev = mem_segment + 1 * lanes;
-            mem_curr = mem_segment + 2 * lanes;
-            start_offset = 2;
-        } else {
-            mem_prev = mem_segment - lanes;
-            mem_curr = mem_segment;
-        }
-    } else {
-        mem_prev = mem_segment + (slice == 0 ? lane_blocks * lanes : 0) - lanes;
-        mem_curr = mem_segment;
-    }
-
-    load_block(&prev, mem_prev, thread);
-
-    for (uint offset = start_offset; offset < segment_blocks; ++offset) {
-        argon2_step(memory, mem_curr, &prev, &tmp, &addr, shuffle_buf,
-                    lanes, segment_blocks, thread, &thread_input,
-                    lane, pass, slice, offset);
-
-        mem_curr += lanes;
-    }
-}
-
 __kernel void argon2_kernel_oneshot(
-        __local struct u64_shuffle_buf *shuffle_bufs,
-        __global struct block_g *memory, uint passes, uint lanes,
-        uint segment_blocks)
-{
-    uint job_id = get_global_id(1);
+        __local struct u64_shuffle_buf *shuffle_bufs,   // 0
+        __global struct block_g *memory,                // 1
+        uint memory_0_count,                            // 2
+        __global struct block_g *memory_1,              // 3
+        uint memory_1_count,                            // 4
+        __global struct block_g *memory_2,              // 5
+        uint memory_2_count,                            // 6
+        __global struct block_g *memory_3,              // 7
+        uint memory_3_count,                            // 8
+        uint passes,                                    // 9
+        uint lanes,                                     // 10
+        uint segment_blocks)                            // 11
+{
+    int batch_id = get_global_id(1);
     uint lane   = get_global_id(0) / THREADS_PER_LANE;
     uint warp   = get_local_id(0) / THREADS_PER_LANE;
     uint thread = get_local_id(0) % THREADS_PER_LANE;
@@ -788,9 +794,11 @@ __kernel void argon2_kernel_oneshot(
 
     uint lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
 
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+    memory = selectMemoryRegion(
+        batch_id, memory, memory_0_count, memory_1, memory_1_count,
+        memory_2, memory_2_count, memory_3, memory_3_count, lanes * lane_blocks);
 
+    /* hash */
     struct block_th prev, addr, tmp;
     uint thread_input;
 
diff --git a/include/argon2-cuda/cudaexception.h b/include/argon2-cuda/cudaexception.h
index ebc8460..92d44b6 100644
--- a/include/argon2-cuda/cudaexception.h
+++ b/include/argon2-cuda/cudaexception.h
@@ -6,6 +6,8 @@
 #endif
 
 #include <exception>
+#include <stdio.h>
+#include <iostream>
 
 namespace argon2 {
 namespace cuda {
@@ -27,6 +29,7 @@ public:
     static void check(cudaError_t res)
     {
         if (res != cudaSuccess) {
+            printf("CUDA exception => |%s|\n", cudaGetErrorString(res));
             throw CudaException(res);
         }
     }
diff --git a/include/argon2-cuda/device.h b/include/argon2-cuda/device.h
index 59eab4c..7fa67bb 100644
--- a/include/argon2-cuda/device.h
+++ b/include/argon2-cuda/device.h
@@ -19,6 +19,7 @@
 #define ARGON2_CUDA_DEVICE_H
 
 #include <string>
+#include <cuda_runtime.h>
 
 namespace argon2 {
 namespace cuda {
@@ -31,26 +32,27 @@ private:
     int deviceIndex;
 
 public:
+    Device() { }
+    Device(int deviceIndex) : deviceIndex(deviceIndex) {}
+    Device(const Device &) = default;
+    Device(Device &&) = default;
+    Device &operator=(const Device &) = default;
     std::string getName() const;
     std::string getInfo() const;
-
     int getDeviceIndex() const { return deviceIndex; }
+    void setAsCurrent() const;
+};
 
-    /**
-     * @brief Empty constructor.
-     * NOTE: Calling methods other than the destructor on an instance initialized
-     * with empty constructor results in undefined behavior.
-     */
-    Device() { }
-
-    Device(int deviceIndex) : deviceIndex(deviceIndex)
-    {
-    }
-
-    Device(const Device &) = default;
-    Device(Device &&) = default;
+struct QueueWrapper {
+    QueueWrapper();
+    ~QueueWrapper();
+    cudaStream_t stream;
+};
 
-    Device &operator=(const Device &) = default;
+struct BufferWrapper {
+    BufferWrapper(size_t size);
+    ~BufferWrapper();
+    void* buf;
 };
 
 #else
@@ -58,19 +60,19 @@ public:
 class Device
 {
 public:
-    std::string getName() const { return {}; }
-    std::string getInfo() const { return {}; }
-
-    int getDeviceIndex() const { return 0; }
-
     Device() { }
-
     Device(const Device &) = default;
     Device(Device &&) = default;
-
     Device &operator=(const Device &) = default;
+
+    std::string getName() const { return {}; }
+    std::string getInfo() const { return {}; }
+    int getDeviceIndex() const { return 0; }
 };
 
+struct QueueWrapper {};
+struct BufferWrapper {};
+
 #endif /* HAVE_CUDA */
 
 } // namespace cuda
diff --git a/include/argon2-cuda/kernels.h b/include/argon2-cuda/kernels.h
index 16418b4..35723c6 100644
--- a/include/argon2-cuda/kernels.h
+++ b/include/argon2-cuda/kernels.h
@@ -5,6 +5,8 @@
 
 #include <cuda_runtime.h>
 #include <cstdint>
+#include "programcontext.h"
+#include "../../include/argon2-gpu-common/argon2params.h"
 
 /* workaround weird CMake/CUDA bug: */
 #ifdef argon2
@@ -14,48 +16,29 @@
 namespace argon2 {
 namespace cuda {
 
-class KernelRunner
+class KernelRunner : public KernelRunnerBase<ProgramContext>
 {
-private:
-    std::uint32_t type, version;
-    std::uint32_t passes, lanes, segmentBlocks;
-    std::uint32_t batchSize;
-    bool bySegment;
-    bool precompute;
-
-    cudaEvent_t start, end;
-    cudaStream_t stream;
-    void *memory;
-    void *refs;
-
-    void precomputeRefs();
-
-    void runKernelSegment(std::uint32_t lanesPerBlock,
-                          std::uint32_t jobsPerBlock,
-                          std::uint32_t pass, std::uint32_t slice);
-    void runKernelOneshot(std::uint32_t lanesPerBlock,
-                          std::uint32_t jobsPerBlock);
-
 public:
-    std::uint32_t getMinLanesPerBlock() const { return bySegment ? 1 : lanes; }
-    std::uint32_t getMaxLanesPerBlock() const { return lanes; }
-
-    std::uint32_t getMinJobsPerBlock() const { return 1; }
-    std::uint32_t getMaxJobsPerBlock() const { return batchSize; }
-
-    std::uint32_t getBatchSize() const { return batchSize; }
-
-    KernelRunner(std::uint32_t type, std::uint32_t version,
-                 std::uint32_t passes, std::uint32_t lanes,
-                 std::uint32_t segmentBlocks, std::uint32_t batchSize,
-                 bool bySegment, bool precompute);
-    ~KernelRunner();
+    struct MiningContext {
+        const cudaStream_t &stream;
+        const ProgramContext& programContext;
+    };
+
+    KernelRunner(const Argon2iMiningConfig &, const MiningContext &);
+    void configure();
+    void writeInputMemory(const void* srcPtr);
+    void readOutputMemory(std::uint8_t *dstPtr);
+    void syncStream();
+    bool streamOperationsComplete();
+    void run(std::uint32_t lanesPerBlock);
+    float finish();
 
-    void writeInputMemory(std::uint32_t jobId, const void *buffer);
-    void readOutputMemory(std::uint32_t jobId, void *buffer);
+private:
+    MiningContext ctx;
+    cudaEvent_t start, end;
+    void *blocksBuffers[MAX_BLOCKS_BUFFERS], *indexBuffer;
 
-    void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
-    float finish();
+    void runKernelOneshot(std::uint32_t lanesPerBlock);
 };
 
 } // cuda
diff --git a/include/argon2-cuda/processingunit.h b/include/argon2-cuda/processingunit.h
index 2109154..74501ec 100644
--- a/include/argon2-cuda/processingunit.h
+++ b/include/argon2-cuda/processingunit.h
@@ -3,8 +3,6 @@
 
 #if HAVE_CUDA
 
-#include <memory>
-
 #include "programcontext.h"
 #include "kernels.h"
 #include "argon2-gpu-common/argon2params.h"
@@ -14,28 +12,27 @@ namespace cuda {
 
 class ProcessingUnit
 {
-private:
-    const ProgramContext *programContext;
-    const Argon2Params *params;
-    const Device *device;
-
-    KernelRunner runner;
-    std::uint32_t bestLanesPerBlock;
-    std::uint32_t bestJobsPerBlock;
-
 public:
-    std::size_t getBatchSize() const { return runner.getBatchSize(); }
-
     ProcessingUnit(
-            const ProgramContext *programContext, const Argon2Params *params,
-            const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false);
-
-    void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
+        const Argon2iMiningConfig & miningConfig,
+        const KernelRunner::MiningContext & miningCtx);
+    ~ProcessingUnit();
 
+    void configure();
+    void uploadInputDataAsync(const std::vector<std::string>& bases);
     void beginProcessing();
-    void endProcessing();
+    bool streamOperationsComplete();
+    void fetchResultsAsync();
+    uint8_t* results();
+    time_point asyncStartTime() const {
+        return asyncTp;
+    }
+
+private:
+    KernelRunner runner;
+    uint8_t* inBuffer;
+    uint8_t* outBuffer;
+    time_point asyncTp;
 };
 
 } // namespace cuda
@@ -44,7 +41,6 @@ public:
 #else
 
 #include <cstddef>
-
 #include "programcontext.h"
 #include "argon2-gpu-common/argon2params.h"
 
@@ -54,21 +50,17 @@ namespace cuda {
 class ProcessingUnit
 {
 public:
-    std::size_t getBatchSize() const { return 0; }
-
     ProcessingUnit(
-            const ProgramContext *programContext, const Argon2Params *params,
-            const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false)
-    {
-    }
-
-    void setPassword(std::size_t index, const void *pw, std::size_t pwSize) { }
-
-    void getHash(std::size_t index, void *hash) { }
-
-    void beginProcessing() { }
-    void endProcessing() { }
+        const Device *device,
+        const Argon2iMiningConfig & miningConfig,
+        const KernelRunner::MiningContext & miningCtx) {};
+
+    void configure() {};
+    void uploadInputDataAsync(const std::vector<std::string>& bases) {};
+    void beginProcessing() {};
+    bool streamOperationsComplete() { return true; };
+    void fetchResultsAsync() {};
+    uint8_t* results() { return nullptr; };
 };
 
 } // namespace cuda
diff --git a/include/argon2-cuda/programcontext.h b/include/argon2-cuda/programcontext.h
index 9808669..4a4205f 100644
--- a/include/argon2-cuda/programcontext.h
+++ b/include/argon2-cuda/programcontext.h
@@ -2,7 +2,7 @@
 #define ARGON2_CUDA_PROGRAMCONTEXT_H
 
 #include "globalcontext.h"
-#include "argon2-gpu-common/argon2-common.h"
+#include "../argon2-gpu-common/argon2-common.h"
 
 namespace argon2 {
 namespace cuda {
@@ -11,47 +11,26 @@ namespace cuda {
 
 class ProgramContext
 {
-private:
-    const GlobalContext *globalContext;
-
-    Type type;
-    Version version;
-
 public:
-    const GlobalContext *getGlobalContext() const { return globalContext; }
-
+    ProgramContext(const Device & d, Type type, Version version);
     Type getArgon2Type() const { return type; }
     Version getArgon2Version() const { return version; }
+    const Device & getDevice() const { return device; }
 
-    ProgramContext(
-            const GlobalContext *globalContext,
-            const std::vector<Device> &devices,
-            Type type, Version version);
+private:
+    const Device & device;
+    Type type;
+    Version version;
 };
 
 #else
 
 class ProgramContext
 {
-private:
-    const GlobalContext *globalContext;
-
-    Type type;
-    Version version;
-
 public:
-    const GlobalContext *getGlobalContext() const { return globalContext; }
-
-    Type getArgon2Type() const { return type; }
-    Version getArgon2Version() const { return version; }
-
-    ProgramContext(
-            const GlobalContext *globalContext,
-            const std::vector<Device> &devices,
-            Type type, Version version)
-        : globalContext(globalContext), type(type), version(version)
-    {
-    }
+    ProgramContext(const Device & d, Type type, Version version) {};
+    Type getArgon2Type() const { return{}; }
+    Version getArgon2Version() const { return{}; }
 };
 
 #endif /* HAVE_CUDA */
diff --git a/include/argon2-gpu-common/argon2-common.h b/include/argon2-gpu-common/argon2-common.h
index fbcf67c..09195a4 100644
--- a/include/argon2-gpu-common/argon2-common.h
+++ b/include/argon2-gpu-common/argon2-common.h
@@ -1,8 +1,14 @@
 #ifndef ARGON2COMMON_H
 #define ARGON2COMMON_H
 
+#include <cstddef>
+#include <cstdint>
+#include <chrono>
+
 namespace argon2 {
 
+typedef std::chrono::time_point<std::chrono::high_resolution_clock> time_point;
+
 enum {
     ARGON2_BLOCK_SIZE = 1024,
     ARGON2_SYNC_POINTS = 4,
@@ -21,8 +27,54 @@ enum Version {
     ARGON2_VERSION_13 = 0x13,
 };
 
-} // namespace argon2
+enum OPT_MODE {
+    BASELINE = 0,
+    PRECOMPUTE_SHUFFLE = 1,
+    PRECOMPUTE_LOCAL_STATE = 2
+};
+
+struct OptParams {
+    OPT_MODE mode;
+    std::uint32_t customBlockCount;
+    const std::uint32_t* customIndex;
+    std::uint32_t customIndexNbSteps;
+
+    OptParams() :
+        mode(BASELINE), customBlockCount(0),
+        customIndex(nullptr), customIndexNbSteps(0) {
+    }
+};
+
+#define BLOCK_TYPE_COUNT (2)
+#define MAX_BLOCKS_BUFFERS (4)
+
+struct MemConfig {
+    MemConfig() {
+        for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+            for (int j = 0; j < BLOCK_TYPE_COUNT; j++) {
+                batchSizes[j][i] = 0;
+                blocksBuffers[j][i] = nullptr;
+            }
+        }
+        indexBuffer = nullptr;
+        in = out = 0;
+    }
 
+    size_t batchSizes[BLOCK_TYPE_COUNT][MAX_BLOCKS_BUFFERS];
+    void* blocksBuffers[BLOCK_TYPE_COUNT][MAX_BLOCKS_BUFFERS];
+    void* indexBuffer;
+    size_t in;
+    size_t out;
+
+    size_t getTotalHashes(int blockType) const {
+        size_t total = 0;
+        for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+            total += batchSizes[blockType][i];
+        }
+        return total;
+    }
+};
+} // namespace argon2
 
 #endif // ARGON2COMMON_H
 
diff --git a/include/argon2-gpu-common/argon2params.h b/include/argon2-gpu-common/argon2params.h
index bf2ca75..d517bcb 100644
--- a/include/argon2-gpu-common/argon2params.h
+++ b/include/argon2-gpu-common/argon2params.h
@@ -2,8 +2,15 @@
 #define ARGON2_ARGON2PARAMS_H
 
 #include <cstdint>
+#include <string>
+#include <vector>
 
 #include "argon2-common.h"
+#include "../../../include/perfscope.h"
+
+#include <chrono>
+
+#define OPEN_CL_SKIP_MEM_TRANSFERS (0)
 
 namespace argon2 {
 
@@ -19,7 +26,7 @@ private:
     static void digestLong(void *out, std::size_t outLen,
                            const void *in, std::size_t inLen);
 
-    void initialHash(void *out, const void *pwd, std::size_t pwdLen,
+    void initialHash(void *out, const void *pwd, std::uint32_t pwdLen,
                      Type type, Version version) const;
 
 public:
@@ -47,19 +54,96 @@ public:
         return static_cast<std::size_t>(getMemoryBlocks()) * ARGON2_BLOCK_SIZE;
     }
 
-    Argon2Params(
-            std::size_t outLen,
-            const void *salt, std::size_t saltLen,
-            const void *secret, std::size_t secretLen,
-            const void *ad, std::size_t adLen,
-            std::size_t t_cost, std::size_t m_cost, std::size_t lanes);
+    Argon2Params(std::uint32_t outLen,
+            const void *salt, std::uint32_t saltLen,
+            const void *secret, std::uint32_t secretLen,
+            const void *ad, std::uint32_t adLen,
+            std::uint32_t t_cost, std::uint32_t m_cost, std::uint32_t lanes);
 
-    void fillFirstBlocks(void *memory, const void *pwd, std::size_t pwdLen,
+    void fillFirstBlocks(void *memory, const void *pwd, std::uint32_t pwdLen,
                          Type type, Version version) const;
 
     void finalize(void *out, const void *memory) const;
 };
 
+struct Argon2iMiningConfig {
+    explicit Argon2iMiningConfig(const MemConfig & memConfig,
+        const Argon2Params & argon2, const OptParams & opt, int blockType) :
+        mem(&memConfig), argon2(&argon2), opt(&opt), blockType(blockType) {};
+
+    const MemConfig * mem{};
+    const Argon2Params * argon2{};
+    const OptParams * opt{};
+    int blockType{};
+};
+
+class KernelRunnerConfig
+{
+public:
+    std::size_t startBlockSize() const { return cfg.argon2->getLanes() * 2 * ARGON2_BLOCK_SIZE; };
+    std::uint32_t minLanesPerBlock() const { return cfg.argon2->getLanes(); };
+    std::uint32_t maxLanesPerBlock() const { return cfg.argon2->getLanes(); };
+    std::uint32_t minJobsPerBlock() const { return 1; };
+    int currentBlockType() const { return cfg.blockType; };
+
+protected:
+    KernelRunnerConfig(const Argon2iMiningConfig & miningConfig) :
+        cfg(miningConfig) {
+    }
+
+    uint32_t blockCountPerHash() const {
+        auto & customBlockCount = cfg.opt->customBlockCount;
+        return (customBlockCount > 0) ?
+            customBlockCount :
+            cfg.argon2->getLanes() * cfg.argon2->getSegmentBlocks() * ARGON2_SYNC_POINTS;
+    }
+
+    std::uint32_t hashesPerRun() const {
+        return (std::uint32_t)cfg.mem->getTotalHashes(cfg.blockType);
+    }
+
+    const Argon2iMiningConfig &cfg;
+};
+
+template<class CONTEXT>
+class KernelRunnerBase : public KernelRunnerConfig
+{
+public:
+    static void prepareInputData(
+        KernelRunnerBase<CONTEXT> runner,
+        uint8_t* inPtr,
+        const std::vector<std::string>& bases) {
+
+        PerfScope p("KernelRunnerBase::prepareInputData()", true);
+
+        std::vector<std::string>::const_iterator it = bases.begin();
+        auto blockType = runner.currentBlockType();
+        uint8_t* pIn = inPtr;
+        for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+            auto nHashes = runner.cfg.mem->batchSizes[blockType][i];
+            for (size_t j = 0; j < nHashes; j++) {
+                runner.cfg.argon2->fillFirstBlocks(
+                    pIn,
+                    it->data(),
+                    (std::uint32_t)it->length(),
+                    runner.programContext->getArgon2Type(),
+                    runner.programContext->getArgon2Version());
+                pIn += runner.startBlockSize();
+                it++;
+            }
+        }
+    }
+
+protected:
+    KernelRunnerBase(
+        const Argon2iMiningConfig & miningConfig,
+        const CONTEXT * programContext) :
+        KernelRunnerConfig(miningConfig),
+        programContext(programContext) {
+    }
+    const CONTEXT *programContext;
+};
+
 } // namespace argon2
 
 #endif // ARGON2_ARGON2PARAMS_H
diff --git a/include/argon2-opencl/cl.hpp b/include/argon2-opencl/cl.hpp
index ced34f5..3f0e6c8 100644
--- a/include/argon2-opencl/cl.hpp
+++ b/include/argon2-opencl/cl.hpp
@@ -142,6 +142,14 @@
  * \endcode
  *
  */
+
+#ifdef _WIN32
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+#include <iostream>
+
 #ifndef CL_HPP_
 #define CL_HPP_
 
@@ -227,6 +235,7 @@
 
 #include <cstring>
 
+extern bool s_openCL_logErrors;
 
 /*! \namespace cl
  *
@@ -318,7 +327,6 @@ public:
 #define __ERR_STR(x) NULL
 #endif // __CL_ENABLE_EXCEPTIONS
 
-
 namespace detail
 {
 #if defined(__CL_ENABLE_EXCEPTIONS)
@@ -327,6 +335,8 @@ static inline cl_int errHandler (
     const char * errStr = NULL)
 {
     if (err != CL_SUCCESS) {
+        if (s_openCL_logErrors)
+            printf("OpenCL error: errCode=%d str=%s\n", err, errStr ? errStr: "none");
         throw Error(err, errStr);
     }
     return err;
diff --git a/include/argon2-opencl/device.h b/include/argon2-opencl/device.h
index 40e1f5e..bfc2750 100644
--- a/include/argon2-opencl/device.h
+++ b/include/argon2-opencl/device.h
@@ -25,31 +25,20 @@ namespace opencl {
 
 class Device
 {
-private:
-    cl::Device device;
-
 public:
-    std::string getName() const;
-    std::string getInfo() const;
-
-    const cl::Device &getCLDevice() const { return device; }
-
-    /**
-     * @brief Empty constructor.
-     * NOTE: Calling methods other than the destructor on an instance initialized
-     * with empty constructor results in undefined behavior.
-     */
     Device() { }
-
     Device(const cl::Device &device)
-        : device(device)
-    {
-    }
-
+        : device(device) {}
     Device(const Device &) = default;
     Device(Device &&) = default;
-
     Device &operator=(const Device &) = default;
+
+    std::string getName() const;
+    std::string getInfo() const;
+    const cl::Device &getCLDevice() const { return device; }
+
+private:
+    cl::Device device;
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/globalcontext.h b/include/argon2-opencl/globalcontext.h
index fed66eb..175b9f1 100644
--- a/include/argon2-opencl/globalcontext.h
+++ b/include/argon2-opencl/globalcontext.h
@@ -28,13 +28,12 @@ namespace opencl {
 
 class GlobalContext
 {
-private:
-    std::vector<Device> devices;
-
 public:
+    GlobalContext();
     const std::vector<Device> &getAllDevices() const { return devices; }
 
-    GlobalContext();
+private:
+    std::vector<Device> devices;
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/kernelrunner.h b/include/argon2-opencl/kernelrunner.h
index a1bba8b..5ea2e3f 100644
--- a/include/argon2-opencl/kernelrunner.h
+++ b/include/argon2-opencl/kernelrunner.h
@@ -3,56 +3,41 @@
 
 #include "programcontext.h"
 #include "argon2-gpu-common/argon2params.h"
+#include <vector>
 
 namespace argon2 {
 namespace opencl {
 
-class KernelRunner
+class KernelRunner : public KernelRunnerBase<ProgramContext>
 {
-private:
-    const ProgramContext *programContext;
-    const Argon2Params *params;
-
-    std::uint32_t batchSize;
-    bool bySegment;
-    bool precompute;
-
-    cl::CommandQueue queue;
-    cl::Kernel kernel;
-    cl::Buffer memoryBuffer, refsBuffer;
-    cl::Event start, end;
-
-    std::size_t memorySize;
-
-    void precomputeRefs();
-
 public:
-    std::uint32_t getMinLanesPerBlock() const
-    {
-        return bySegment ? 1 : params->getLanes();
-    }
-    std::uint32_t getMaxLanesPerBlock() const { return params->getLanes(); }
-
-    std::uint32_t getMinJobsPerBlock() const { return 1; }
-    std::uint32_t getMaxJobsPerBlock() const { return batchSize; }
-
-    std::uint32_t getBatchSize() const { return batchSize; }
+    struct MiningContext {
+        cl::CommandQueue & queue;
+        const ProgramContext & programContext;
+    };
 
-    KernelRunner(const ProgramContext *programContext,
-                 const Argon2Params *params, const Device *device,
-                 std::uint32_t batchSize, bool bySegment, bool precompute);
+public:
+    KernelRunner(const Argon2iMiningConfig &, MiningContext &);
 
-    void *mapInputMemory(std::uint32_t jobId);
-    void unmapInputMemory(void *memory);
+    void configure();
+    void uploadToInputMemoryAsync(const void* srcPtr) const;
+    void fetchOutputMemoryAsync(uint8_t* dstPtr) const;
+    void run(std::uint32_t lanesPerBlock);
+    void insertEndEventAndFlush();
+    void waitForResults() const;
+    bool resultsReady() const;
 
-    void *mapOutputMemory(std::uint32_t jobId);
-    void unmapOutputMemory(void *memory);
+private:
+    MiningContext &ctx;
+    cl::Kernel kernel;
+    cl::Buffer blocksBuffers[MAX_BLOCKS_BUFFERS], indexBuffer;
+    cl::Event end;
 
-    void run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock);
-    float finish();
+    void setKernelsArgs();
+    void setupKernel();
 };
 
-} // namespace opencl
-} // namespace argon2
+}
+}
 
 #endif // ARGON2_OPENCL_KERNELRUNNER_H
diff --git a/include/argon2-opencl/processingunit.h b/include/argon2-opencl/processingunit.h
index 277272d..c28bcfa 100644
--- a/include/argon2-opencl/processingunit.h
+++ b/include/argon2-opencl/processingunit.h
@@ -2,6 +2,7 @@
 #define ARGON2_OPENCL_PROCESSINGUNIT_H
 
 #include <memory>
+#include <vector>
 
 #include "kernelrunner.h"
 
@@ -10,28 +11,31 @@ namespace opencl {
 
 class ProcessingUnit
 {
-private:
-    const ProgramContext *programContext;
-    const Argon2Params *params;
-    const Device *device;
+public:
+    ProcessingUnit(const Argon2iMiningConfig & miningConfig,
+        KernelRunner::MiningContext & miningCtx);
+
+    void configure();
+    void uploadInputDataAsync(const std::vector<std::string>& bases);
+    void runKernelAsync();
+    void waitForResults() const;
+    bool resultsReady() const;
+    void fetchResultsAsync();
+    uint8_t* results();
+    time_point asyncStartTime() const {
+        return asyncTp;
+    }
 
+private:
     KernelRunner runner;
-    std::uint32_t bestLanesPerBlock;
-    std::uint32_t bestJobsPerBlock;
-
-public:
-    std::size_t getBatchSize() const { return runner.getBatchSize(); }
 
-    ProcessingUnit(
-            const ProgramContext *programContext, const Argon2Params *params,
-            const Device *device, std::size_t batchSize,
-            bool bySegment = true, bool precomputeRefs = false);
+    cl::Buffer inBuffer;
+    uint8_t* inPtr;
 
-    void setPassword(std::size_t index, const void *pw, std::size_t pwSize);
-    void getHash(std::size_t index, void *hash);
+    cl::Buffer outBuffer;
+    uint8_t* outPtr;
 
-    void beginProcessing();
-    void endProcessing();
+    time_point asyncTp;
 };
 
 } // namespace opencl
diff --git a/include/argon2-opencl/programcontext.h b/include/argon2-opencl/programcontext.h
index 45d7ab3..a7fd166 100644
--- a/include/argon2-opencl/programcontext.h
+++ b/include/argon2-opencl/programcontext.h
@@ -37,7 +37,7 @@ public:
     ProgramContext(
             const GlobalContext *globalContext,
             const std::vector<Device> &devices,
-            Type type, Version version, char *pathToKernel);
+            Type type, Version version, const char *pathToKernel);
 };
 
 } // namespace opencl
diff --git a/lib/argon2-cuda/device.cpp b/lib/argon2-cuda/device.cpp
index 76a300b..8697d6d 100644
--- a/lib/argon2-cuda/device.cpp
+++ b/lib/argon2-cuda/device.cpp
@@ -27,15 +27,37 @@ std::string Device::getName() const
 {
     cudaDeviceProp prop;
     CudaException::check(cudaGetDeviceProperties(&prop, deviceIndex));
-    return "CUDA Device '" + std::string(prop.name) + "'";
+    return prop.name;
 }
 
 std::string Device::getInfo() const
 {
     /* FIXME: show some more stuff here: */
-    cudaDeviceProp prop;
-    CudaException::check(cudaGetDeviceProperties(&prop, deviceIndex));
-    return "CUDA Device '" + std::string(prop.name) + "'";
+    return getName();
+}
+
+void Device::setAsCurrent() const {
+    int currentIndex = -1;
+    CudaException::check(cudaGetDevice(&currentIndex));
+    if (currentIndex != deviceIndex)
+        CudaException::check(cudaSetDevice(deviceIndex));
+}
+
+QueueWrapper::QueueWrapper() : stream{} {
+    argon2::cuda::CudaException::check(cudaStreamCreate(&stream));
+}
+
+QueueWrapper::~QueueWrapper() {
+    cudaStreamDestroy(stream);
+}
+
+BufferWrapper::BufferWrapper(size_t size) : buf{} {
+    argon2::cuda::CudaException::check(cudaMalloc(&buf, size));
+    cudaMemset(buf, 0, size); // warm up the buffer
+}
+
+BufferWrapper::~BufferWrapper() {
+    cudaFree(buf);
 }
 
 } // namespace cuda
diff --git a/lib/argon2-cuda/globalcontext.cpp b/lib/argon2-cuda/globalcontext.cpp
index eb52e02..56040c9 100644
--- a/lib/argon2-cuda/globalcontext.cpp
+++ b/lib/argon2-cuda/globalcontext.cpp
@@ -25,7 +25,11 @@ GlobalContext::GlobalContext()
     : devices()
 {
     int count;
-    CudaException::check(cudaGetDeviceCount(&count));
+    cudaError_t res = cudaGetDeviceCount(&count);
+    if (res != cudaSuccess) {
+        std::cout << "Cannot get CUDA device count, aborting...." << std::endl;
+        exit(1);
+    }
 
     devices.reserve(count);
     for (int i = 0; i < count; i++) {
diff --git a/lib/argon2-cuda/kernels.cu b/lib/argon2-cuda/kernels.cu
index 79fb767..20f580f 100644
--- a/lib/argon2-cuda/kernels.cu
+++ b/lib/argon2-cuda/kernels.cu
@@ -3,8 +3,11 @@
 #define __CUDACC__
 #endif
 
-#include "kernels.h"
-#include "cudaexception.h"
+#include <cuda_runtime.h>
+
+#include "../../include/argon2-cuda/kernels.h"
+#include "../../include/argon2-cuda/cudaexception.h"
+#include "../../../include/perfscope.h"
 
 #include <stdexcept>
 #ifndef NDEBUG
@@ -49,8 +52,8 @@ __device__ uint64_t u64_shuffle(uint64_t v, uint32_t thread)
 {
     uint32_t lo = u64_lo(v);
     uint32_t hi = u64_hi(v);
-    lo = __shfl(lo, thread);
-    hi = __shfl(hi, thread);
+    lo = __shfl_sync(0xFFFFFFFF, lo, thread);
+    hi = __shfl_sync(0xFFFFFFFF, hi, thread);
     return u64_build(hi, lo);
 }
 
@@ -269,6 +272,7 @@ __device__ void next_addresses(struct block_th *addr, struct block_th *tmp,
     xor_block(addr, tmp);
 }
 
+// ~= index_alpha in ref code ...
 __device__ void compute_ref_pos(
         uint32_t lanes, uint32_t segment_blocks,
         uint32_t pass, uint32_t lane, uint32_t slice, uint32_t offset,
@@ -304,91 +308,41 @@ __device__ void compute_ref_pos(
     }
 }
 
+__device__ struct block_g* selectMemoryRegion(
+    int batch_id,
+    struct block_g *memory,
+    uint32_t memory_0_count,
+    struct block_g *memory_1,
+    uint32_t memory_1_count,
+    struct block_g *memory_2,
+    uint32_t memory_2_count,
+    struct block_g *memory_3,
+    uint32_t memory_3_count,
+    uint32_t blocksPerBatch)
+{
+    if (batch_id >= 0 && batch_id < memory_0_count)
+        memory = memory + (size_t)(batch_id * blocksPerBatch);
+    batch_id -= memory_0_count;
+    if (batch_id >= 0 && batch_id < memory_1_count)
+        memory = memory_1 + (size_t)(batch_id * blocksPerBatch);
+    batch_id -= memory_1_count;
+    if (batch_id >= 0 && batch_id < memory_2_count)
+        memory = memory_2 + (size_t)(size_t)(batch_id * blocksPerBatch);
+    batch_id -= memory_2_count;
+    if (batch_id >= 0 && batch_id < memory_3_count)
+        memory = memory_3 + (size_t)(size_t)(batch_id * blocksPerBatch);
+    return memory;
+}
+
 struct ref {
     uint32_t ref_lane;
     uint32_t ref_index;
 };
 
-/*
- * Refs hierarchy:
- * lanes -> passes -> slices -> blocks
- */
-template<uint32_t type>
-__global__ void argon2_precompute_kernel(
-        struct ref *refs, uint32_t passes, uint32_t lanes,
-        uint32_t segment_blocks)
-{
-    uint32_t block_id = blockIdx.y * blockDim.y + threadIdx.y;
-    uint32_t thread = threadIdx.x;
-
-    uint32_t segment_addr_blocks = (segment_blocks + ARGON2_QWORDS_IN_BLOCK - 1)
-            / ARGON2_QWORDS_IN_BLOCK;
-    uint32_t block = block_id % segment_addr_blocks;
-    uint32_t segment = block_id / segment_addr_blocks;
-
-    uint32_t slice, pass, pass_id, lane;
-    if (type == ARGON2_ID) {
-        slice = segment % (ARGON2_SYNC_POINTS / 2);
-        lane = segment / (ARGON2_SYNC_POINTS / 2);
-        pass_id = pass = 0;
-    } else {
-        slice = segment % ARGON2_SYNC_POINTS;
-        pass_id = segment / ARGON2_SYNC_POINTS;
-
-        pass = pass_id % passes;
-        lane = pass_id / passes;
-    }
-
-    struct block_th addr, tmp;
-
-    uint32_t thread_input;
-    switch (thread) {
-    case 0:
-        thread_input = pass;
-        break;
-    case 1:
-        thread_input = lane;
-        break;
-    case 2:
-        thread_input = slice;
-        break;
-    case 3:
-        thread_input = lanes * segment_blocks * ARGON2_SYNC_POINTS;
-        break;
-    case 4:
-        thread_input = passes;
-        break;
-    case 5:
-        thread_input = type;
-        break;
-    case 6:
-        thread_input = block + 1;
-        break;
-    default:
-        thread_input = 0;
-        break;
-    }
-
-    next_addresses(&addr, &tmp, thread_input, thread);
-
-    refs += segment * segment_blocks;
-
-    for (uint32_t i = 0; i < QWORDS_PER_THREAD; i++) {
-        uint32_t pos = i * THREADS_PER_LANE + thread;
-        uint32_t offset = block * ARGON2_QWORDS_IN_BLOCK + pos;
-        if (offset < segment_blocks) {
-            uint64_t v = block_th_get(&addr, i);
-            uint32_t ref_index = u64_lo(v);
-            uint32_t ref_lane  = u64_hi(v);
-
-            compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
-                            &ref_lane, &ref_index);
-
-            refs[offset].ref_index = ref_index;
-            refs[offset].ref_lane  = ref_lane;
-        }
-    }
-}
+struct index {
+    uint32_t refSlot;
+    uint32_t storeSlot;
+};
 
 template<uint32_t version>
 __device__ void argon2_core(
@@ -415,137 +369,284 @@ __device__ void argon2_core(
     store_block(mem_curr, prev, thread);
 }
 
-template<uint32_t type, uint32_t version>
-__device__ void argon2_step_precompute(
-        struct block_g *memory, struct block_g *mem_curr,
-        struct block_th *prev, struct block_th *tmp, const struct ref **refs,
-        uint32_t lanes, uint32_t segment_blocks, uint32_t thread,
-        uint32_t lane, uint32_t pass, uint32_t slice, uint32_t offset)
+__device__ void argon2_core_precomputedIndex(
+    struct block_g *memory, struct block_g *mem_curr,
+    struct block_th *prev, struct block_th *tmp,
+    uint32_t thread, uint32_t ref_index)
 {
-    uint32_t ref_index, ref_lane;
-    if (type == ARGON2_I || (type == ARGON2_ID && pass == 0 &&
-            slice < ARGON2_SYNC_POINTS / 2)) {
-        ref_index = (*refs)->ref_index;
-        ref_lane = (*refs)->ref_lane;
-        (*refs)++;
-    } else {
-        uint64_t v = u64_shuffle(prev->a, 0);
-        ref_index = u64_lo(v);
-        ref_lane  = u64_hi(v);
+    struct block_g *mem_ref = memory + ref_index;
 
-        compute_ref_pos(lanes, segment_blocks, pass, lane, slice, offset,
-                        &ref_lane, &ref_index);
-    }
+    load_block_xor(prev, mem_ref, thread);
+    move_block(tmp, prev);
 
-    argon2_core<version>(memory, mem_curr, prev, tmp, lanes, thread, pass,
-                         ref_index, ref_lane);
+    shuffle_block(prev, thread);
+
+    xor_block(prev, tmp);
+
+    if (mem_curr != 0)
+        store_block(mem_curr, prev, thread);
 }
 
-template<uint32_t type, uint32_t version>
-__global__ void argon2_kernel_segment_precompute(
-        struct block_g *memory, const struct ref *refs,
-        uint32_t passes, uint32_t lanes, uint32_t segment_blocks,
-        uint32_t pass, uint32_t slice)
-{
-    uint32_t job_id = blockIdx.z * blockDim.z + threadIdx.z;
-    uint32_t lane   = blockIdx.y * blockDim.y + threadIdx.y;
-    uint32_t thread = threadIdx.x;
+extern __shared__ uint64_t sharedMem[];
 
-    uint32_t lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+#define INDEX_CACHE_DECLS() \
+struct index * index_cache = (struct index *)(sharedMem + (ARGON2_BLOCK_SIZE / sizeof(uint64_t))); \
+const struct index * pIndex = indexs; \
+const struct index * pIndexEnd = indexs + (nSteps - 2);
 
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+#define INDEX_CACHE_LOOP_BEGIN() \
+while (pIndex < pIndexEnd) { \
+    uint32_t nBlocks = min((uint32_t)(pIndexEnd - pIndex), THREADS_PER_LANE); \
+    index_cache[thread] = pIndex[thread]; \
+    for (uint32_t i = 0; i < nBlocks; i++) {
 
-    struct block_th prev, tmp;
+#define INDEX_CACHE_LOOP_END() \
+}}
 
-    struct block_g *mem_segment =
-            memory + slice * segment_blocks * lanes + lane;
-    struct block_g *mem_prev, *mem_curr;
-    uint32_t start_offset = 0;
-    if (pass == 0) {
-        if (slice == 0) {
-            mem_prev = mem_segment + 1 * lanes;
-            mem_curr = mem_segment + 2 * lanes;
-            start_offset = 2;
-        } else {
-            mem_prev = mem_segment - lanes;
-            mem_curr = mem_segment;
-        }
-    } else {
-        mem_prev = mem_segment + (slice == 0 ? lane_blocks * lanes : 0) - lanes;
-        mem_curr = mem_segment;
-    }
+__device__ void g_alt(uint32_t offA, uint32_t offB, uint32_t offC, uint32_t offD)
+{
+    uint64_t a = sharedMem[offA];
+    uint64_t b = sharedMem[offB];
+    uint64_t c = sharedMem[offC];
+    uint64_t d = sharedMem[offD];
 
-    load_block(&prev, mem_prev, thread);
+    a = f(a, b);
+    d = rotr64(d ^ a, 32);
+    c = f(c, d);
+    b = rotr64(b ^ c, 24);
 
-    if (type == ARGON2_ID) {
-        if (pass == 0 && slice < ARGON2_SYNC_POINTS / 2) {
-            refs += lane * (lane_blocks / 2) + slice * segment_blocks;
-            refs += start_offset;
-        }
-    } else {
-        refs += (lane * passes + pass) * lane_blocks + slice * segment_blocks;
-        refs += start_offset;
+    a = f(a, b);
+    d = rotr64(d ^ a, 16);
+    c = f(c, d);
+    b = rotr64(b ^ c, 63);
+
+    sharedMem[offA] = a;
+    sharedMem[offB] = b;
+    sharedMem[offC] = c;
+    sharedMem[offD] = d;
+}
+
+#define G_ALT_1(p, offA, offB, offC, offD) \
+a = sharedMem[p + offA]; \
+b = sharedMem[p + offB]; \
+c = sharedMem[p + offC]; \
+d = sharedMem[p + offD]; \
+a = f(a, b); \
+d = rotr64(d ^ a, 32); \
+c = f(c, d); \
+b = rotr64(b ^ c, 24); \
+a = f(a, b); \
+d = rotr64(d ^ a, 16); \
+c = f(c, d); \
+b = rotr64(b ^ c, 63); \
+sharedMem[p + offB] = b; \
+sharedMem[p + offC] = c; \
+sharedMem[p + offD] = d;
+
+#define G_ALT_2(p, offA, offB, offC, offD) \
+b = sharedMem[p + offB]; \
+c = sharedMem[p + offC]; \
+d = sharedMem[p + offD]; \
+a = f(a, b); \
+d = rotr64(d ^ a, 32); \
+c = f(c, d); \
+b = rotr64(b ^ c, 24); \
+a = f(a, b); \
+d = rotr64(d ^ a, 16); \
+c = f(c, d); \
+b = rotr64(b ^ c, 63); \
+sharedMem[p + offA] = a; \
+sharedMem[p + offB] = b; \
+sharedMem[p + offC] = c; \
+sharedMem[p + offD] = d;
+
+__constant__ uint8_t OFFS[4][16] = {
+    {
+        0, 4, 8, 12,
+        0, 5, 10, 15,
+        0, 32, 64, 96,
+        0, 33, 80, 113
+    },
+    {
+        1, 5, 9, 13,
+        1, 6, 11, 12,
+        1, 33, 65, 97,
+        1, 48, 81, 96
+    },
+    {
+        2, 6, 10, 14,
+        2, 7, 8, 13,
+        16, 48, 80, 112,
+        16, 49, 64, 97,
+    },
+    {
+        3, 7, 11, 15,
+        3, 4, 9, 14,
+        17, 49, 81, 113,
+        17, 32, 65, 112,
     }
+};
 
-    for (uint32_t offset = start_offset; offset < segment_blocks; ++offset) {
-        argon2_step_precompute<type, version>(
-                    memory, mem_curr, &prev, &tmp, &refs, lanes, segment_blocks,
-                    thread, lane, pass, slice, offset);
+__global__ void argon2_kernel_oneshot_precomputedIndex(
+    struct block_g *memory,
+    uint32_t memory_0_count,
+    struct block_g *memory_1,
+    uint32_t memory_1_count,
+    struct block_g *memory_2,
+    uint32_t memory_2_count,
+    struct block_g *memory_3,
+    uint32_t memory_3_count,
+    const struct index *indexs,
+    uint32_t nSteps, uint32_t blocksPerBatch)
+{
+    struct block_th prev, tmp;
+    uint32_t batch_id = blockIdx.z; // nBatches
+    uint32_t thread = threadIdx.x; // 32
+
+    memory = selectMemoryRegion(
+        batch_id,
+        memory, memory_0_count, memory_1, memory_1_count,
+        memory_2, memory_2_count, memory_3, memory_3_count,
+        blocksPerBatch);
+
+    load_block(&prev, memory + 1, thread);
+
+#if (1) // CPU BLOCK, SHUFFLE, INDEX CACHE
+    INDEX_CACHE_DECLS();
+    INDEX_CACHE_LOOP_BEGIN()
+        uint32_t storeSlot = index_cache[i].storeSlot;
+        struct block_g *mem_curr =
+            (storeSlot != UINT32_MAX) ? (memory + storeSlot) : 0;
+
+        argon2_core_precomputedIndex(
+            memory, mem_curr, &prev, &tmp, thread, index_cache[i].refSlot);
+
+        pIndex++;
+    INDEX_CACHE_LOOP_END()
+#else // CPU BLOCK, SHUFFLE, LEGACY
+    for (uint32_t i = 2; i < nSteps; ++i)
+    {
+        uint32_t storeSlot = indexs->storeSlot;
+        uint32_t ref_index = indexs->refSlot;
 
-        mem_curr += lanes;
+        struct block_g *mem_curr = (storeSlot != UINT32_MAX) ? (memory + storeSlot) : 0;
+        argon2_core_precomputedIndex(
+            memory, mem_curr, &prev, &tmp, thread, ref_index);
+
+        indexs++;
     }
+#endif
+
+    store_block(memory + (blocksPerBatch - 1), &prev, thread);
 }
 
-template<uint32_t type, uint32_t version>
-__global__ void argon2_kernel_oneshot_precompute(
-        struct block_g *memory, const struct ref *refs, uint32_t passes,
-        uint32_t lanes, uint32_t segment_blocks)
+__global__ void argon2_kernel_index_local_state(
+    struct block_g *memory,
+    uint32_t memory_0_count,
+    struct block_g *memory_1,
+    uint32_t memory_1_count,
+    struct block_g *memory_2,
+    uint32_t memory_2_count,
+    struct block_g *memory_3,
+    uint32_t memory_3_count,
+    const struct index *indexs,
+    uint32_t nSteps, uint32_t blocksPerBatch)
 {
-    uint32_t job_id = blockIdx.z * blockDim.z + threadIdx.z;
-    uint32_t lane   = threadIdx.y;
-    uint32_t thread = threadIdx.x;
+    uint32_t thread = threadIdx.x; // 32
+    uint32_t batch_id = blockIdx.z; // nBatches
 
-    uint32_t lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
+    memory = selectMemoryRegion(
+        batch_id,
+        memory, memory_0_count, memory_1, memory_1_count,
+        memory_2, memory_2_count, memory_3, memory_3_count,
+        blocksPerBatch);
 
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+    uint32_t thGroup = thread / 4;
+    uint32_t subId = thread % 4;
 
-    struct block_th prev, tmp;
+    uint32_t offP1 = 16 * thGroup;
+    uint32_t offP2 = 2 * thGroup;
+    uint8_t* offs = OFFS[subId];
 
-    struct block_g *mem_lane = memory + lane;
-    struct block_g *mem_prev = mem_lane + 1 * lanes;
-    struct block_g *mem_curr = mem_lane + 2 * lanes;
+    ulonglong4 th_state =
+        (reinterpret_cast<ulonglong4*>(memory + 1))[thread];
 
-    load_block(&prev, mem_prev, thread);
+#if 1 // CPU BLOCK, LOCAL STATE, INDEX CACHE
+    INDEX_CACHE_DECLS();
+    INDEX_CACHE_LOOP_BEGIN()
+        ulonglong4 th_ref =
+            (reinterpret_cast<ulonglong4*>(memory + index_cache[i].refSlot))[thread];
 
-    if (type == ARGON2_ID) {
-        refs += lane * (lane_blocks / 2) + 2;
-    } else {
-        refs += lane * passes * lane_blocks + 2;
-    }
+        th_state.x ^= th_ref.x; th_state.y ^= th_ref.y;
+        th_state.z ^= th_ref.z; th_state.w ^= th_ref.w;
 
-    uint32_t skip = 2;
-    for (uint32_t pass = 0; pass < passes; ++pass) {
-        for (uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; ++slice) {
-            for (uint32_t offset = 0; offset < segment_blocks; ++offset) {
-                if (skip > 0) {
-                    --skip;
-                    continue;
-                }
+        ulonglong4 th_tmp = th_state;
 
-                argon2_step_precompute<type, version>(
-                            memory, mem_curr, &prev, &tmp, &refs, lanes,
-                            segment_blocks, thread, lane, pass, slice, offset);
+        (reinterpret_cast<ulonglong4*>(sharedMem))[thread] = th_state;
+        __syncthreads();
 
-                mem_curr += lanes;
-            }
+        uint64_t a, b, c, d;
 
-            __syncthreads();
-        }
+        G_ALT_1(offP1, offs[0], offs[1], offs[2], offs[3]);
+        G_ALT_2(offP1, offs[4], offs[5], offs[6], offs[7]);
+        __syncthreads();
 
-        mem_curr = mem_lane;
+        G_ALT_1(offP2, offs[8], offs[9], offs[10], offs[11]);
+        G_ALT_2(offP2, offs[12], offs[13], offs[14], offs[15]);
+        __syncthreads();
+
+        th_state = (reinterpret_cast<ulonglong4*>(sharedMem))[thread];
+
+        th_state.x ^= th_tmp.x; th_state.y ^= th_tmp.y;
+        th_state.z ^= th_tmp.z; th_state.w ^= th_tmp.w;
+
+        uint32_t slot = index_cache[i].storeSlot;
+        if (slot != UINT32_MAX)
+            (reinterpret_cast<ulonglong4*>(memory + slot))[thread] = th_state;
+
+        pIndex++;
+    INDEX_CACHE_LOOP_END()
+
+    (reinterpret_cast<ulonglong4*>(memory + blocksPerBatch - 1))[thread] = th_state;
+#else // CPU BLOCK, LOCAL_STATE, LEGACY
+    for (uint32_t i = 2; i < nSteps; ++i)
+    {
+        ulonglong4 th_ref =
+            (reinterpret_cast<ulonglong4*>(memory + indexs->refSlot))[thread];
+
+        th_state.x ^= th_ref.x;
+        th_state.y ^= th_ref.y;
+        th_state.z ^= th_ref.z;
+        th_state.w ^= th_ref.w;
+
+        ulonglong4 th_tmp = th_state;
+
+        (reinterpret_cast<ulonglong4*>(sharedMem))[thread] = th_state;
+        __syncthreads();
+
+        g_alt(offP1 + offs[0], offP1 + offs[1], offP1 + offs[2], offP1 + offs[3]);
+        g_alt(offP1 + offs[4], offP1 + offs[5], offP1 + offs[6], offP1 + offs[7]);
+        __syncthreads();
+
+        g_alt(offP2 + offs[8], offP2 + offs[9], offP2 + offs[10], offP2 + offs[11]);
+        g_alt(offP2 + offs[12], offP2 + offs[13], offP2 + offs[14], offP2 + offs[15]);
+        __syncthreads();
+
+        th_state = (reinterpret_cast<ulonglong4*>(sharedMem))[thread];
+
+        th_state.x ^= th_tmp.x;
+        th_state.y ^= th_tmp.y;
+        th_state.z ^= th_tmp.z;
+        th_state.w ^= th_tmp.w;
+
+        if (indexs->storeSlot != UINT32_MAX)
+            (reinterpret_cast<ulonglong4*>(memory + indexs->storeSlot))[thread] = th_state;
+
+        indexs++;
     }
+
+    (reinterpret_cast<ulonglong4*>(memory + blocksPerBatch - 1))[thread] = th_state;
+#endif
 }
 
 template<uint32_t type, uint32_t version>
@@ -588,98 +689,32 @@ __device__ void argon2_step(
                          ref_index, ref_lane);
 }
 
-template<uint32_t type, uint32_t version>
-__global__ void argon2_kernel_segment(
-        struct block_g *memory, uint32_t passes, uint32_t lanes,
-        uint32_t segment_blocks, uint32_t pass, uint32_t slice)
-{
-    uint32_t job_id = blockIdx.z * blockDim.z + threadIdx.z;
-    uint32_t lane   = blockIdx.y * blockDim.y + threadIdx.y;
-    uint32_t thread = threadIdx.x;
-
-    uint32_t lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
-
-    /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
-
-    struct block_th prev, addr, tmp;
-    uint32_t thread_input;
-
-    if (type == ARGON2_I || type == ARGON2_ID) {
-        switch (thread) {
-        case 0:
-            thread_input = pass;
-            break;
-        case 1:
-            thread_input = lane;
-            break;
-        case 2:
-            thread_input = slice;
-            break;
-        case 3:
-            thread_input = lanes * lane_blocks;
-            break;
-        case 4:
-            thread_input = passes;
-            break;
-        case 5:
-            thread_input = type;
-            break;
-        default:
-            thread_input = 0;
-            break;
-        }
-
-        if (pass == 0 && slice == 0 && segment_blocks > 2) {
-            if (thread == 6) {
-                ++thread_input;
-            }
-            next_addresses(&addr, &tmp, thread_input, thread);
-        }
-    }
-
-    struct block_g *mem_segment =
-            memory + slice * segment_blocks * lanes + lane;
-    struct block_g *mem_prev, *mem_curr;
-    uint32_t start_offset = 0;
-    if (pass == 0) {
-        if (slice == 0) {
-            mem_prev = mem_segment + 1 * lanes;
-            mem_curr = mem_segment + 2 * lanes;
-            start_offset = 2;
-        } else {
-            mem_prev = mem_segment - lanes;
-            mem_curr = mem_segment;
-        }
-    } else {
-        mem_prev = mem_segment + (slice == 0 ? lane_blocks * lanes : 0) - lanes;
-        mem_curr = mem_segment;
-    }
-
-    load_block(&prev, mem_prev, thread);
-
-    for (uint32_t offset = start_offset; offset < segment_blocks; ++offset) {
-        argon2_step<type, version>(
-                    memory, mem_curr, &prev, &tmp, &addr, lanes, segment_blocks,
-                    thread, &thread_input, lane, pass, slice, offset);
-
-        mem_curr += lanes;
-    }
-}
-
 template<uint32_t type, uint32_t version>
 __global__ void argon2_kernel_oneshot(
-        struct block_g *memory, uint32_t passes, uint32_t lanes,
+        struct block_g *memory,
+        uint32_t memory_0_count,
+        struct block_g *memory_1,
+        uint32_t memory_1_count,
+        struct block_g *memory_2,
+        uint32_t memory_2_count,
+        struct block_g *memory_3,
+        uint32_t memory_3_count,
+        uint32_t passes, uint32_t lanes,
         uint32_t segment_blocks)
 {
-    uint32_t job_id = blockIdx.z * blockDim.z + threadIdx.z;
+    int job_id = blockIdx.z * blockDim.z + threadIdx.z;
     uint32_t lane   = threadIdx.y;
     uint32_t thread = threadIdx.x;
 
     uint32_t lane_blocks = ARGON2_SYNC_POINTS * segment_blocks;
 
     /* select job's memory region: */
-    memory += (size_t)job_id * lanes * lane_blocks;
+    // memory += (size_t)job_id * lanes * lane_blocks;
+    memory = selectMemoryRegion(
+        job_id,
+        memory, memory_0_count, memory_1, memory_1_count,
+        memory_2, memory_2_count, memory_3, memory_3_count,
+        lanes * lane_blocks);
 
     struct block_th prev, addr, tmp;
     uint32_t thread_input;
@@ -757,301 +792,140 @@ __global__ void argon2_kernel_oneshot(
     }
 }
 
-KernelRunner::KernelRunner(uint32_t type, uint32_t version, uint32_t passes,
-                           uint32_t lanes, uint32_t segmentBlocks,
-                           uint32_t batchSize, bool bySegment, bool precompute)
-    : type(type), version(version), passes(passes), lanes(lanes),
-      segmentBlocks(segmentBlocks), batchSize(batchSize), bySegment(bySegment),
-      precompute(precompute), stream(nullptr), memory(nullptr),
-      refs(nullptr), start(nullptr), end(nullptr)
-{
-    // FIXME: check overflow:
-    size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE * batchSize;
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
-#endif
-
-    CudaException::check(cudaMalloc(&memory, memorySize));
-
-    CudaException::check(cudaEventCreate(&start));
-    CudaException::check(cudaEventCreate(&end));
-
-    CudaException::check(cudaStreamCreate(&stream));
-
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
-        uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
-
-        size_t refsSize = segments * segmentBlocks * sizeof(struct ref);
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
-#endif
-
-        CudaException::check(cudaMalloc(&refs, refsSize));
-
-        precomputeRefs();
-        CudaException::check(cudaStreamSynchronize(stream));
-    }
+void cudaSafeFree(void* &ptr) {
+    if (ptr)
+        CudaException::check(cudaFree(ptr));
+    ptr = nullptr;
 }
 
-void KernelRunner::precomputeRefs()
-{
-    struct ref *refs = (struct ref *)this->refs;
-
-    uint32_t segmentAddrBlocks = (segmentBlocks + ARGON2_QWORDS_IN_BLOCK - 1)
-            / ARGON2_QWORDS_IN_BLOCK;
-    uint32_t segments =
-            type == ARGON2_ID
-            ? lanes * (ARGON2_SYNC_POINTS / 2)
-            : passes * lanes * ARGON2_SYNC_POINTS;
-
-    dim3 blocks = dim3(1, segments * segmentAddrBlocks);
-    dim3 threads = dim3(THREADS_PER_LANE);
-
-    if (type == ARGON2_I) {
-        argon2_precompute_kernel<ARGON2_I>
-            <<<blocks, threads, 0, stream>>>(
-                refs, passes, lanes, segmentBlocks);
-    } else {
-        argon2_precompute_kernel<ARGON2_ID>
-            <<<blocks, threads, 0, stream>>>(
-                refs, passes, lanes, segmentBlocks);
-    }
+KernelRunner::KernelRunner(const Argon2iMiningConfig & cfg,
+    const MiningContext & ctx) :
+    KernelRunnerBase(cfg, &ctx.programContext),
+    ctx(ctx) {
+    configure();
 }
 
-KernelRunner::~KernelRunner()
-{
-    if (start != nullptr) {
-        cudaEventDestroy(start);
-    }
-    if (end != nullptr) {
-        cudaEventDestroy(end);
-    }
-    if (stream != nullptr) {
-        cudaStreamDestroy(stream);
-    }
-    if (memory != nullptr) {
-        cudaFree(memory);
-    }
-    if (refs != nullptr) {
-        cudaFree(refs);
+void KernelRunner::configure() {
+    PerfScope p("KernelRunner::configure()");
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto bufWrapper = (BufferWrapper *)cfg.mem->blocksBuffers[cfg.blockType][i];
+        blocksBuffers[i] = bufWrapper ? bufWrapper->buf : nullptr;
     }
+    auto indexBufWrapper = (BufferWrapper*)cfg.mem->indexBuffer;
+    indexBuffer = indexBufWrapper ? indexBufWrapper->buf : nullptr;
 }
 
-void KernelRunner::writeInputMemory(uint32_t jobId, const void *buffer)
-{
-    std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE;
-    std::size_t size = static_cast<size_t>(lanes) * 2 * ARGON2_BLOCK_SIZE;
-    std::size_t offset = memorySize * jobId;
-    auto mem = static_cast<uint8_t *>(memory) + offset;
-    CudaException::check(cudaMemcpyAsync(mem, buffer, size,
-                                         cudaMemcpyHostToDevice, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
-}
-
-void KernelRunner::readOutputMemory(uint32_t jobId, void *buffer)
-{
-    std::size_t memorySize = static_cast<size_t>(lanes) * segmentBlocks
-            * ARGON2_SYNC_POINTS * ARGON2_BLOCK_SIZE;
-    std::size_t size = static_cast<size_t>(lanes) * ARGON2_BLOCK_SIZE;
-    std::size_t offset = memorySize * (jobId + 1) - size;
-    auto mem = static_cast<uint8_t *>(memory) + offset;
-    CudaException::check(cudaMemcpyAsync(buffer, mem, size,
-                                         cudaMemcpyDeviceToHost, stream));
-    CudaException::check(cudaStreamSynchronize(stream));
-}
-
-void KernelRunner::runKernelSegment(uint32_t lanesPerBlock,
-                                    uint32_t jobsPerBlock,
-                                    uint32_t pass, uint32_t slice)
-{
-    if (lanesPerBlock > lanes || lanes % lanesPerBlock != 0) {
-        throw std::logic_error("Invalid lanesPerBlock!");
-    }
-
-    if (jobsPerBlock > batchSize || batchSize % jobsPerBlock != 0) {
-        throw std::logic_error("Invalid jobsPerBlock!");
-    }
-
-    struct block_g *memory_blocks = (struct block_g *)memory;
-    dim3 blocks = dim3(1, lanes / lanesPerBlock, batchSize / jobsPerBlock);
-    dim3 threads = dim3(THREADS_PER_LANE, lanesPerBlock, jobsPerBlock);
-    if (type == ARGON2_I) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment_precompute<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment_precompute<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        }
-    } else if (type == ARGON2_ID) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment_precompute<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment_precompute<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_segment<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            } else {
-                argon2_kernel_segment<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks,
-                            pass, slice);
-            }
-        }
-    } else {
-        if (version == ARGON2_VERSION_10) {
-            argon2_kernel_segment<ARGON2_D, ARGON2_VERSION_10>
-                    <<<blocks, threads, 0, stream>>>(
-                        memory_blocks, passes, lanes, segmentBlocks,
-                        pass, slice);
-        } else {
-            argon2_kernel_segment<ARGON2_D, ARGON2_VERSION_13>
-                    <<<blocks, threads, 0, stream>>>(
-                        memory_blocks, passes, lanes, segmentBlocks,
-                        pass, slice);
+void KernelRunner::writeInputMemory(const void* srcPtr) {
+    std::size_t startBlocksSize = startBlockSize();
+    std::size_t batchMemSize = blockCountPerHash() * ARGON2_BLOCK_SIZE;
+    uint8_t* pSrc = (uint8_t*)srcPtr;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nHashes = cfg.mem->batchSizes[cfg.blockType][i];
+        for (size_t j = 0; j < nHashes; j++) {
+            size_t offset = batchMemSize * j;
+            auto mem = static_cast<uint8_t *>(blocksBuffers[i]) + offset;
+            CudaException::check(
+                cudaMemcpyAsync(
+                    mem, pSrc, startBlocksSize,
+                    cudaMemcpyHostToDevice, ctx.stream));
+            pSrc += startBlocksSize;
         }
     }
 }
 
-void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock,
-                                    uint32_t jobsPerBlock)
-{
-    if (lanesPerBlock != lanes) {
-        throw std::logic_error("Invalid lanesPerBlock!");
-    }
-
-    if (jobsPerBlock > batchSize || batchSize % jobsPerBlock != 0) {
-        throw std::logic_error("Invalid jobsPerBlock!");
-    }
-
-    struct block_g *memory_blocks = (struct block_g *)memory;
-    dim3 blocks = dim3(1, 1, batchSize / jobsPerBlock);
-    dim3 threads = dim3(THREADS_PER_LANE, lanes, jobsPerBlock);
-    if (type == ARGON2_I) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot_precompute<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot_precompute<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            }
-        }
-    } else if (type == ARGON2_ID) {
-        if (precompute) {
-            struct ref *refs = (struct ref *)this->refs;
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot_precompute<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot_precompute<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, refs, passes, lanes, segmentBlocks);
-            }
-        } else {
-            if (version == ARGON2_VERSION_10) {
-                argon2_kernel_oneshot<ARGON2_ID, ARGON2_VERSION_10>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            } else {
-                argon2_kernel_oneshot<ARGON2_ID, ARGON2_VERSION_13>
-                        <<<blocks, threads, 0, stream>>>(
-                            memory_blocks, passes, lanes, segmentBlocks);
-            }
-        }
-    } else {
-        if (version == ARGON2_VERSION_10) {
-            argon2_kernel_oneshot<ARGON2_D, ARGON2_VERSION_10>
-                    <<<blocks, threads, 0, stream>>>(
-                        memory_blocks, passes, lanes, segmentBlocks);
-        } else {
-            argon2_kernel_oneshot<ARGON2_D, ARGON2_VERSION_13>
-                    <<<blocks, threads, 0, stream>>>(
-                        memory_blocks, passes, lanes, segmentBlocks);
+void KernelRunner::readOutputMemory(uint8_t *dst) {
+    std::size_t batchMemSize = blockCountPerHash() * ARGON2_BLOCK_SIZE;
+    std::size_t resultSize =
+        static_cast<std::size_t>(cfg.argon2->getLanes()) * ARGON2_BLOCK_SIZE;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nHashes = cfg.mem->batchSizes[cfg.blockType][i];
+        for (size_t j = 0; j < nHashes; j++) {
+            std::size_t offset = batchMemSize * (j + 1) - resultSize;
+            auto mem = static_cast<uint8_t *>(blocksBuffers[i]) + offset;
+            CudaException::check(
+                cudaMemcpyAsync(
+                    dst, mem, resultSize, cudaMemcpyDeviceToHost, ctx.stream));
+            dst += resultSize;
         }
     }
 }
 
-void KernelRunner::run(uint32_t lanesPerBlock, uint32_t jobsPerBlock)
-{
-    CudaException::check(cudaEventRecord(start, stream));
-
-    if (bySegment) {
-        for (uint32_t pass = 0; pass < passes; pass++) {
-            for (uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
-                runKernelSegment(lanesPerBlock, jobsPerBlock, pass, slice);
-            }
-        }
-    } else {
-        runKernelOneshot(lanesPerBlock, jobsPerBlock);
-    }
+void KernelRunner::runKernelOneshot(uint32_t lanesPerBlock) {
+    auto nLanes = cfg.argon2->getLanes();
+    if (lanesPerBlock != nLanes)
+        throw std::logic_error("Invalid lanesPerBlock!");
 
+    auto nHashes = (unsigned int)cfg.mem->getTotalHashes(cfg.blockType);
+
+    dim3 blocks = dim3(1, 1, nHashes);
+    dim3 threads = dim3(THREADS_PER_LANE, nLanes, 1);
+
+    auto counts = cfg.mem->batchSizes[cfg.blockType];
+    auto optMode = cfg.opt->mode;
+    struct index *indexs = (struct index *)this->indexBuffer;
+
+    const size_t sharedMemSize = ARGON2_BLOCK_SIZE + THREADS_PER_LANE * sizeof(struct index);
+
+    if (optMode == PRECOMPUTE_SHUFFLE) {
+        argon2_kernel_oneshot_precomputedIndex
+            << <blocks, threads, sharedMemSize, ctx.stream >> > (
+                (struct block_g *)blocksBuffers[0], (uint32_t)counts[0],
+                (struct block_g *)blocksBuffers[1], (uint32_t)counts[1],
+                (struct block_g *)blocksBuffers[2], (uint32_t)counts[2],
+                (struct block_g *)blocksBuffers[3], (uint32_t)counts[3],
+                indexs,
+                ARGON2_SYNC_POINTS * cfg.argon2->getSegmentBlocks(),
+                cfg.opt->customBlockCount);
+    }
+    else if (optMode == PRECOMPUTE_LOCAL_STATE) {
+        argon2_kernel_index_local_state
+            << <blocks, threads, sharedMemSize, ctx.stream>> > (
+                (struct block_g *)blocksBuffers[0], (uint32_t)counts[0],
+                (struct block_g *)blocksBuffers[1], (uint32_t)counts[1],
+                (struct block_g *)blocksBuffers[2], (uint32_t)counts[2],
+                (struct block_g *)blocksBuffers[3], (uint32_t)counts[3],
+                indexs,
+                ARGON2_SYNC_POINTS * cfg.argon2->getSegmentBlocks(),
+                cfg.opt->customBlockCount);
+    }
+    else {
+        argon2_kernel_oneshot<ARGON2_I, ARGON2_VERSION_13>
+            << <blocks, threads, 0, ctx.stream >> >(
+                (struct block_g *)blocksBuffers[0], (uint32_t)counts[0],
+                (struct block_g *)blocksBuffers[1], (uint32_t)counts[1],
+                (struct block_g *)blocksBuffers[2], (uint32_t)counts[2],
+                (struct block_g *)blocksBuffers[3], (uint32_t)counts[3],
+                cfg.argon2->getTimeCost(),
+                cfg.argon2->getLanes(),
+                cfg.argon2->getSegmentBlocks());
+
+        argon2::cuda::CudaException::check(cudaGetLastError());
+    }
+}
+
+void KernelRunner::run(uint32_t lanesPerBlock)
+{
+    ctx.programContext.getDevice().setAsCurrent();
+    runKernelOneshot(lanesPerBlock);
     CudaException::check(cudaGetLastError());
-
-    CudaException::check(cudaEventRecord(end, stream));
 }
 
-float KernelRunner::finish()
-{
-    CudaException::check(cudaStreamSynchronize(stream));
+void KernelRunner::syncStream() {
+    CudaException::check(cudaStreamSynchronize(ctx.stream));
+}
 
-    float time = 0.0;
-    CudaException::check(cudaEventElapsedTime(&time, start, end));
-    return time;
+bool KernelRunner::streamOperationsComplete() {
+    cudaError_t res = cudaStreamQuery(ctx.stream);
+    if (res == cudaSuccess) {
+        return true;
+    }
+    else if (res == cudaErrorNotReady) {
+        return false;
+    }
+    else {
+        CudaException::check(res);
+        return false;
+    }
 }
 
 } // cuda
diff --git a/lib/argon2-cuda/processingunit.cpp b/lib/argon2-cuda/processingunit.cpp
index 7768427..4e7aa59 100644
--- a/lib/argon2-cuda/processingunit.cpp
+++ b/lib/argon2-cuda/processingunit.cpp
@@ -7,149 +7,60 @@
 #include <iostream>
 #endif
 
+#include <chrono>
+
 namespace argon2 {
 namespace cuda {
 
-static void setCudaDevice(int deviceIndex)
-{
-    int currentIndex = -1;
-    CudaException::check(cudaGetDevice(&currentIndex));
-    if (currentIndex != deviceIndex) {
-        CudaException::check(cudaSetDevice(deviceIndex));
-    }
+ProcessingUnit::ProcessingUnit(
+    const Argon2iMiningConfig & cfg,
+    const KernelRunner::MiningContext & miningCtx) :
+    runner(cfg, miningCtx),
+    inBuffer{}, outBuffer{} {
+
+    auto & device = miningCtx.programContext.getDevice();
+    auto createPinnedBuffer = [&device](uint8_t* &dst, size_t size, std::string name) {
+        device.setAsCurrent();
+        cudaError_t status = cudaMallocHost((void**)&(dst), size);
+        if (status != cudaSuccess)
+            throw std::logic_error(std::string("Error allocating ") + name);
+        cudaMemset(dst, 0, size); // warm up the buffer
+    };
+
+    createPinnedBuffer(inBuffer, cfg.mem->in, "pinned input host memory");
+    createPinnedBuffer(outBuffer, cfg.mem->out, "pinned output host memory");
 }
 
-static bool isPowerOfTwo(std::uint32_t x)
-{
-    return (x & (x - 1)) == 0;
+uint8_t* ProcessingUnit::results() {
+    return outBuffer;
 }
 
-ProcessingUnit::ProcessingUnit(
-        const ProgramContext *programContext, const Argon2Params *params,
-        const Device *device, std::size_t batchSize, bool bySegment,
-        bool precomputeRefs)
-    : programContext(programContext), params(params), device(device),
-      runner(programContext->getArgon2Type(),
-             programContext->getArgon2Version(), params->getTimeCost(),
-             params->getLanes(), params->getSegmentBlocks(), batchSize,
-             bySegment, precomputeRefs),
-      bestLanesPerBlock(runner.getMinLanesPerBlock()),
-      bestJobsPerBlock(runner.getMinJobsPerBlock())
-{
-    setCudaDevice(device->getDeviceIndex());
-
-    /* pre-fill first blocks with pseudo-random data: */
-    for (std::size_t i = 0; i < batchSize; i++) {
-        setPassword(i, NULL, 0);
-    }
-
-    if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
-            && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
-             lpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(lpb, bestJobsPerBlock);
-                time = runner.finish();
-            } catch(CudaException &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   CUDA error on " << lpb
-                          << " lanes per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
-
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << lpb << " lanes per block: "
-                      << time << " ms" << std::endl;
-#endif
-
-            if (time < bestTime) {
-                bestTime = time;
-                bestLanesPerBlock = lpb;
-            }
-        }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestLanesPerBlock
-                  << " lanes per block." << std::endl;
-#endif
-    }
-
-    /* Only tune jobs per block if we hit maximum lanes per block: */
-    if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
-            && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
-            && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
-             jpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(bestLanesPerBlock, jpb);
-                time = runner.finish();
-            } catch(CudaException &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   CUDA error on " << jpb
-                          << " jobs per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
-
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << jpb << " jobs per block: "
-                      << time << " ms" << std::endl;
-#endif
+ProcessingUnit::~ProcessingUnit() {
+    CudaException::check(cudaFree(inBuffer));
+    CudaException::check(cudaFree(outBuffer));
+}
 
-            if (time < bestTime) {
-                bestTime = time;
-                bestJobsPerBlock = jpb;
-            }
-        }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestJobsPerBlock
-                  << " jobs per block." << std::endl;
-#endif
-    }
+void ProcessingUnit::configure() {
+    runner.configure();
 }
 
-void ProcessingUnit::setPassword(std::size_t index, const void *pw,
-                                 std::size_t pwSize)
-{
-    std::size_t size = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    params->fillFirstBlocks(buffer.get(), pw, pwSize,
-                            programContext->getArgon2Type(),
-                            programContext->getArgon2Version());
-    runner.writeInputMemory(index, buffer.get());
+void ProcessingUnit::uploadInputDataAsync(const std::vector<std::string>& bases) {
+    KernelRunnerBase<ProgramContext>::prepareInputData(runner, inBuffer, bases);
+
+    asyncTp = std::chrono::high_resolution_clock::now();
+    runner.writeInputMemory(inBuffer);
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    std::size_t size = params->getLanes() * ARGON2_BLOCK_SIZE;
-    auto buffer = std::unique_ptr<uint8_t[]>(new uint8_t[size]);
-    runner.readOutputMemory(index, buffer.get());
-    params->finalize(hash, buffer.get());
+void ProcessingUnit::beginProcessing() {
+    runner.run(runner.minLanesPerBlock());
 }
 
-void ProcessingUnit::beginProcessing()
-{
-    setCudaDevice(device->getDeviceIndex());
-    runner.run(bestLanesPerBlock, bestJobsPerBlock);
+void ProcessingUnit::fetchResultsAsync() {
+    runner.readOutputMemory(outBuffer);
 }
 
-void ProcessingUnit::endProcessing()
-{
-    runner.finish();
+bool ProcessingUnit::streamOperationsComplete() {
+    return runner.streamOperationsComplete();
 }
 
 } // namespace cuda
diff --git a/lib/argon2-cuda/programcontext.cpp b/lib/argon2-cuda/programcontext.cpp
index 106f9df..4d25f6e 100644
--- a/lib/argon2-cuda/programcontext.cpp
+++ b/lib/argon2-cuda/programcontext.cpp
@@ -6,12 +6,8 @@
 namespace argon2 {
 namespace cuda {
 
-ProgramContext::ProgramContext(
-        const GlobalContext *globalContext,
-        const std::vector<Device> &devices,
-        Type type, Version version)
-    : globalContext(globalContext), type(type), version(version)
-{
+ProgramContext::ProgramContext(const Device & d, Type type, Version version) :
+    device(d), type(type), version(version) {
 }
 
 } // namespace cuda
diff --git a/lib/argon2-gpu-common/argon2params.cpp b/lib/argon2-gpu-common/argon2params.cpp
index dd122d8..8dd97bc 100644
--- a/lib/argon2-gpu-common/argon2params.cpp
+++ b/lib/argon2-gpu-common/argon2params.cpp
@@ -21,17 +21,17 @@ static void store32(void *dst, std::uint32_t v)
 }
 
 Argon2Params::Argon2Params(
-        std::size_t outLen,
-        const void *salt, std::size_t saltLen,
-        const void *secret, std::size_t secretLen,
-        const void *ad, std::size_t adLen,
-        std::size_t t_cost, std::size_t m_cost, std::size_t lanes)
+        std::uint32_t outLen,
+        const void *salt, std::uint32_t saltLen,
+        const void *secret, std::uint32_t secretLen,
+        const void *ad, std::uint32_t adLen,
+        std::uint32_t t_cost, std::uint32_t m_cost, std::uint32_t lanes)
     : salt(salt), secret(secret), ad(ad),
       outLen(outLen), saltLen(saltLen), secretLen(secretLen), adLen(adLen),
       t_cost(t_cost), m_cost(m_cost), lanes(lanes)
 {
     // TODO validate inputs
-    std::size_t segments = lanes * ARGON2_SYNC_POINTS;
+    std::uint32_t segments = lanes * ARGON2_SYNC_POINTS;
     segmentBlocks = std::max(m_cost, 2 * segments) / segments;
 }
 
@@ -77,7 +77,7 @@ void Argon2Params::digestLong(void *out, std::size_t outLen,
 }
 
 void Argon2Params::initialHash(
-        void *out, const void *pwd, std::size_t pwdLen,
+        void *out, const void *pwd, std::uint32_t pwdLen,
         Type type, Version version) const
 {
     Blake2b blake;
@@ -104,7 +104,7 @@ void Argon2Params::initialHash(
 }
 
 void Argon2Params::fillFirstBlocks(
-        void *memory, const void *pwd, std::size_t pwdLen,
+        void *memory, const void *pwd, std::uint32_t pwdLen,
         Type type, Version version) const
 {
     std::uint8_t initHash[ARGON2_PREHASH_SEED_LENGTH];
@@ -190,6 +190,5 @@ void Argon2Params::finalize(void *out, const void *memory) const
 
     digestLong(out, outLen, &xored, ARGON2_BLOCK_SIZE);
 }
-
 } // namespace argon2
 
diff --git a/lib/argon2-gpu-common/blake2b.h b/lib/argon2-gpu-common/blake2b.h
index 094fb83..7f438ee 100644
--- a/lib/argon2-gpu-common/blake2b.h
+++ b/lib/argon2-gpu-common/blake2b.h
@@ -2,6 +2,7 @@
 #define ARGON2_BLAKE2B_H
 
 #include <cstdint>
+#include <cstddef>
 
 namespace argon2 {
 
diff --git a/lib/argon2-opencl/device.cpp b/lib/argon2-opencl/device.cpp
index 298b995..bbc00c0 100644
--- a/lib/argon2-opencl/device.cpp
+++ b/lib/argon2-opencl/device.cpp
@@ -26,9 +26,7 @@ namespace opencl {
 
 std::string Device::getName() const
 {
-    return "OpenCL Device '"
-            + device.getInfo<CL_DEVICE_NAME>()
-            + "' (" + device.getInfo<CL_DEVICE_VENDOR>() + ")";
+    return device.getInfo<CL_DEVICE_NAME>();
 }
 
 template<class T>
diff --git a/lib/argon2-opencl/kernelloader.cpp b/lib/argon2-opencl/kernelloader.cpp
index 22949f3..70ed6a3 100644
--- a/lib/argon2-opencl/kernelloader.cpp
+++ b/lib/argon2-opencl/kernelloader.cpp
@@ -12,11 +12,16 @@ cl::Program KernelLoader::loadArgon2Program(
         const std::string &sourceDirectory,
         Type type, Version version, bool debug)
 {
-    std::string sourcePath = sourceDirectory + "/argon2_kernel.cl";
+    std::string sourcePath = sourceDirectory + "argon2_kernel.cl";
     std::string sourceText;
     std::stringstream buildOpts;
     {
         std::ifstream sourceFile { sourcePath };
+
+        if (!sourceFile.is_open()) {
+            throw std::logic_error(std::string("Cannot find kernel source: ") + sourcePath);
+        }
+
         sourceText = {
             std::istreambuf_iterator<char>(sourceFile),
             std::istreambuf_iterator<char>()
@@ -35,6 +40,7 @@ cl::Program KernelLoader::loadArgon2Program(
         prog.build(opts.c_str());
     } catch (const cl::Error &err) {
         std::cerr << "ERROR: Failed to build program:" << std::endl;
+        std::cerr << "cl::Error=" << err.what() << std::endl;
         for (cl::Device &device : context.getInfo<CL_CONTEXT_DEVICES>()) {
             std::cerr << "  Build log from device '" << device.getInfo<CL_DEVICE_NAME>() << "':" << std::endl;
             std::cerr << prog.getBuildInfo<CL_PROGRAM_BUILD_LOG>(device);
diff --git a/lib/argon2-opencl/kernelrunner.cpp b/lib/argon2-opencl/kernelrunner.cpp
index 9fe39b4..ec83717 100644
--- a/lib/argon2-opencl/kernelrunner.cpp
+++ b/lib/argon2-opencl/kernelrunner.cpp
@@ -1,195 +1,190 @@
 #include "kernelrunner.h"
 
 #include <stdexcept>
-
-#ifndef NDEBUG
+#include <thread>
 #include <iostream>
-#endif
+#include <iomanip>
+#include <map>
+#include <sstream>
 
-#define THREADS_PER_LANE 32
+#include "../../include/perfscope.h"
 
 namespace argon2 {
 namespace opencl {
 
-enum {
-    ARGON2_REFS_PER_BLOCK = ARGON2_BLOCK_SIZE / (2 * sizeof(cl_uint)),
-};
-
-KernelRunner::KernelRunner(const ProgramContext *programContext,
-                           const Argon2Params *params, const Device *device,
-                           std::uint32_t batchSize, bool bySegment, bool precompute)
-    : programContext(programContext), params(params), batchSize(batchSize),
-      bySegment(bySegment), precompute(precompute),
-      memorySize(params->getMemorySize() * static_cast<std::size_t>(batchSize))
-{
-    auto context = programContext->getContext();
-    std::uint32_t passes = params->getTimeCost();
-    std::uint32_t lanes = params->getLanes();
-    std::uint32_t segmentBlocks = params->getSegmentBlocks();
-
-    queue = cl::CommandQueue(context, device->getCLDevice(),
-                             CL_QUEUE_PROFILING_ENABLE);
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << memorySize << " bytes for memory..."
-                  << std::endl;
-#endif
-
-    memoryBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, memorySize);
-
-    Type type = programContext->getArgon2Type();
-    if ((type == ARGON2_I || type == ARGON2_ID) && precompute) {
-        std::uint32_t segments =
-                type == ARGON2_ID
-                ? lanes * (ARGON2_SYNC_POINTS / 2)
-                : passes * lanes * ARGON2_SYNC_POINTS;
-
-        std::size_t refsSize = segments * segmentBlocks * sizeof(cl_uint) * 2;
-
-#ifndef NDEBUG
-        std::cerr << "[INFO] Allocating " << refsSize << " bytes for refs..."
-                  << std::endl;
-#endif
-
-        refsBuffer = cl::Buffer(context, CL_MEM_READ_WRITE, refsSize);
-
-        precomputeRefs();
-    }
+KernelRunner::KernelRunner(const Argon2iMiningConfig & miningConfig,
+    MiningContext & ctx) :
+    KernelRunnerBase(miningConfig, &ctx.programContext),
+    ctx(ctx) {
+    configure();
+}
 
-    static const char *KERNEL_NAMES[2][2] = {
-        {
-            "argon2_kernel_oneshot",
-            "argon2_kernel_segment",
-        },
-        {
-            "argon2_kernel_oneshot_precompute",
-            "argon2_kernel_segment_precompute",
-        }
-    };
+void KernelRunner::configure() {
+    PerfScope p("KernelRunner::configure");
 
-    kernel = cl::Kernel(programContext->getProgram(),
-                        KERNEL_NAMES[precompute][bySegment]);
-    kernel.setArg<cl::Buffer>(1, memoryBuffer);
-    if (precompute) {
-        kernel.setArg<cl::Buffer>(2, refsBuffer);
-        kernel.setArg<cl_uint>(3, passes);
-        kernel.setArg<cl_uint>(4, lanes);
-        kernel.setArg<cl_uint>(5, segmentBlocks);
-    } else {
-        kernel.setArg<cl_uint>(2, passes);
-        kernel.setArg<cl_uint>(3, lanes);
-        kernel.setArg<cl_uint>(4, segmentBlocks);
+    auto pBuffers = cfg.mem->blocksBuffers[cfg.blockType];
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto pbuffer = (cl::Buffer*)pBuffers[i];
+        blocksBuffers[i] = pbuffer ? *pbuffer : cl::Buffer();
     }
-}
 
-void KernelRunner::precomputeRefs()
-{
-    std::uint32_t passes = params->getTimeCost();
-    std::uint32_t lanes = params->getLanes();
-    std::uint32_t segmentBlocks = params->getSegmentBlocks();
-    std::uint32_t segmentAddrBlocks =
-            (segmentBlocks + ARGON2_REFS_PER_BLOCK - 1)
-            / ARGON2_REFS_PER_BLOCK;
-    std::uint32_t segments = programContext->getArgon2Type() == ARGON2_ID
-            ? lanes * (ARGON2_SYNC_POINTS / 2)
-            : passes * lanes * ARGON2_SYNC_POINTS;
-
-    std::size_t shmemSize = THREADS_PER_LANE * sizeof(cl_uint) * 2;
-
-    cl::Kernel kernel = cl::Kernel(programContext->getProgram(),
-                                   "argon2_precompute_kernel");
-    kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
-    kernel.setArg<cl::Buffer>(1, refsBuffer);
-    kernel.setArg<cl_uint>(2, passes);
-    kernel.setArg<cl_uint>(3, lanes);
-    kernel.setArg<cl_uint>(4, segmentBlocks);
-
-    cl::NDRange globalRange { THREADS_PER_LANE * segments * segmentAddrBlocks };
-    cl::NDRange localRange { THREADS_PER_LANE };
-    queue.enqueueNDRangeKernel(kernel, cl::NullRange, globalRange, localRange);
-    queue.finish();
-}
+    auto pIndexBuffer = (cl::Buffer*)cfg.mem->indexBuffer;
+    indexBuffer = pIndexBuffer ? *pIndexBuffer : cl::Buffer();
 
-void *KernelRunner::mapInputMemory(std::uint32_t jobId)
-{
-    std::size_t memorySize = params->getMemorySize();
-    std::size_t mappedSize = params->getLanes() * 2 * ARGON2_BLOCK_SIZE;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_WRITE,
-                                  memorySize * jobId, mappedSize);
+    setupKernel();
 }
 
-void KernelRunner::unmapInputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
-}
+void KernelRunner::setKernelsArgs() {
+    PerfScope p("setKernelsArgs");
 
-void *KernelRunner::mapOutputMemory(std::uint32_t jobId)
-{
-    std::size_t memorySize = params->getMemorySize();
-    std::size_t mappedSize = static_cast<std::size_t>(params->getLanes())
-            * ARGON2_BLOCK_SIZE;
-    std::size_t mappedOffset = memorySize * (jobId + 1) - mappedSize;
-    return queue.enqueueMapBuffer(memoryBuffer, true, CL_MAP_READ,
-                                  mappedOffset, mappedSize);
-}
+    auto& params = *cfg.argon2;
+    auto& optParams = *cfg.opt;
 
-void KernelRunner::unmapOutputMemory(void *memory)
-{
-    queue.enqueueUnmapMemObject(memoryBuffer, memory);
+    std::uint32_t passes = params.getTimeCost();
+    std::uint32_t lanes = params.getLanes();
+    std::uint32_t segmentBlocks = params.getSegmentBlocks();
+
+    int bufPrmOffset = 1;
+    if (optParams.mode == PRECOMPUTE_LOCAL_STATE) {
+        bufPrmOffset = 0;
+    }
+
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        kernel.setArg<cl::Buffer>(i * 2 + bufPrmOffset, blocksBuffers[i]);
+        kernel.setArg<cl_uint>(i * 2 + 1 + bufPrmOffset,
+            (cl_uint)cfg.mem->batchSizes[cfg.blockType][i]);
+    }
+
+    if ((optParams.mode == PRECOMPUTE_SHUFFLE) ||
+        (optParams.mode == PRECOMPUTE_LOCAL_STATE))
+    {
+        uint32_t nSteps = ARGON2_SYNC_POINTS * segmentBlocks;
+        kernel.setArg<cl::Buffer>(bufPrmOffset + 8, indexBuffer);
+        kernel.setArg<cl_uint>(bufPrmOffset + 9, nSteps);
+        kernel.setArg<cl_uint>(bufPrmOffset + 10, optParams.customBlockCount);
+    }
+    else {
+        kernel.setArg<cl_uint>(9, passes);
+        kernel.setArg<cl_uint>(10, lanes);
+        kernel.setArg<cl_uint>(11, segmentBlocks);
+    }
 }
 
-void KernelRunner::run(std::uint32_t lanesPerBlock, std::uint32_t jobsPerBlock)
-{
-    std::uint32_t lanes = params->getLanes();
-    std::uint32_t passes = params->getTimeCost();
+void KernelRunner::setupKernel() {
+    static const char *KERNEL_NAMES[3] = {
+        "argon2_kernel_oneshot",
+        "argon2_kernel_oneshot_precomputedIndex_shuffleBuf",
+        "argon2_kernel_oneshot_precomputedIndex_localState"
+    };
 
-    if (bySegment) {
-        if (lanesPerBlock > lanes || lanes % lanesPerBlock != 0) {
-            throw std::logic_error("Invalid lanesPerBlock!");
-        }
-    } else {
-        if (lanesPerBlock != lanes) {
-            throw std::logic_error("Invalid lanesPerBlock!");
+    PerfScope p("setupKernel");
+    auto& optParams = *cfg.opt;
+    const char* kernelName = KERNEL_NAMES[optParams.mode];
+
+    if ((optParams.mode == PRECOMPUTE_LOCAL_STATE) ||
+        (optParams.mode == PRECOMPUTE_SHUFFLE)) {
+        if (cfg.argon2->getLanes() > 1 || cfg.argon2->getTimeCost() > 1) {
+            throw std::logic_error("PRECOMPUTE only supported with 1 lane, 1 pass and oneshot mode");
         }
     }
 
-    if (jobsPerBlock > batchSize || batchSize % jobsPerBlock != 0) {
-        throw std::logic_error("Invalid jobsPerBlock!");
+    kernel = cl::Kernel(programContext->getProgram(), kernelName);
+    setKernelsArgs();
+}
+
+void KernelRunner::uploadToInputMemoryAsync(const void* srcPtr) const {
+    bool blocking = false;
+    std::size_t startblockssize = startBlockSize();
+    std::size_t batchmemsize = blockCountPerHash() * ARGON2_BLOCK_SIZE;
+    uint8_t* pblocks = (uint8_t*)srcPtr;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nhashes = cfg.mem->batchSizes[cfg.blockType][i];
+        for (size_t j = 0; j < nhashes; j++) {
+            size_t offset = batchmemsize * j;
+            {
+                PerfScope p("enqueuewritebuffer (uploadtoinputmemoryasync)");
+                ctx.queue.enqueueWriteBuffer(
+                    blocksBuffers[i],
+                    blocking,
+                    offset,
+                    startblockssize,
+                    pblocks,
+                    NULL,
+                    NULL);
+            }
+            pblocks += startblockssize;
+        }
     }
+}
 
-    cl::NDRange globalRange { THREADS_PER_LANE * lanes, batchSize };
-    cl::NDRange localRange { THREADS_PER_LANE * lanesPerBlock, jobsPerBlock };
-
-    queue.enqueueMarker(&start);
-
-    std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * jobsPerBlock
-            * sizeof(cl_uint) * 2;
-    kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
-    if (bySegment) {
-        for (std::uint32_t pass = 0; pass < passes; pass++) {
-            for (std::uint32_t slice = 0; slice < ARGON2_SYNC_POINTS; slice++) {
-                kernel.setArg<cl_uint>(precompute ? 6 : 5, pass);
-                kernel.setArg<cl_uint>(precompute ? 7 : 6, slice);
-                queue.enqueueNDRangeKernel(kernel, cl::NullRange,
-                                           globalRange, localRange);
+void KernelRunner::fetchOutputMemoryAsync(uint8_t* dstPtr) const {
+    std::size_t batchMemSize = blockCountPerHash() * ARGON2_BLOCK_SIZE;
+    std::size_t resultSize = static_cast<std::size_t>(
+        cfg.argon2->getLanes()) * ARGON2_BLOCK_SIZE;
+    bool blocking = false;
+    for (int i = 0; i < MAX_BLOCKS_BUFFERS; i++) {
+        auto nHashes = cfg.mem->batchSizes[cfg.blockType][i];
+        for (size_t j = 0; j < nHashes; j++) {
+            std::size_t offset = batchMemSize * (j + 1) - resultSize;
+            {
+                PerfScope p("enqueueReadBuffer (fetchOutputMemoryAsync)");
+                ctx.queue.enqueueReadBuffer(
+                    blocksBuffers[i],
+                    blocking,
+                    offset,
+                    resultSize,
+                    dstPtr,
+                    NULL,
+                    NULL);
             }
+            dstPtr += resultSize;
         }
-    } else {
-        queue.enqueueNDRangeKernel(kernel, cl::NullRange,
-                                   globalRange, localRange);
     }
+}
 
-    queue.enqueueMarker(&end);
+void KernelRunner::insertEndEventAndFlush() {
+    PerfScope p("insertEndEventAndFlush");
+    ctx.queue.enqueueMarker(&end);
+    cl::detail::errHandler(ctx.queue.flush(),
+        "KernelRunner::resultsReady, flushing queue");
 }
 
-float KernelRunner::finish()
-{
-    end.wait();
+void KernelRunner::run(std::uint32_t lanesPerBlock) {
+    std::uint32_t lanes = cfg.argon2->getLanes();
+
+    if (lanesPerBlock != lanes)
+        throw std::logic_error("Invalid lanesPerBlock");
 
-    cl_ulong nsStart = start.getProfilingInfo<CL_PROFILING_COMMAND_END>();
-    cl_ulong nsEnd   = end.getProfilingInfo<CL_PROFILING_COMMAND_END>();
+    auto nHashes = hashesPerRun();
+    if (nHashes <= 0)
+        throw std::logic_error("Invalid Kernel nHashes");
+
+    const uint32_t THREADS_PER_LANE = 32;
+    cl::NDRange globalRange { THREADS_PER_LANE * lanes, nHashes };
+    cl::NDRange localRange { THREADS_PER_LANE * lanesPerBlock, 1 };
+
+    if (cfg.opt->mode != PRECOMPUTE_LOCAL_STATE) {
+        std::size_t shmemSize = THREADS_PER_LANE * lanesPerBlock * sizeof(cl_uint) * 2;
+        kernel.setArg<cl::LocalSpaceArg>(0, { shmemSize });
+    }
+
+    {
+        PerfScope p("enqueueNDRangeKernel");
+        ctx.queue.enqueueNDRangeKernel(kernel, cl::NullRange,
+            globalRange, localRange);
+    }
+}
+
+void KernelRunner::waitForResults() const {
+    PerfScope p("end.wait()");
+    end.wait();
+}
 
-    return (nsEnd - nsStart) / (1000.0F * 1000.0F);
+bool KernelRunner::resultsReady() const {
+    cl_int err = 0;
+    auto status = end.getInfo<CL_EVENT_COMMAND_EXECUTION_STATUS>(&err);
+    cl::detail::errHandler(err, "KernelRunner::resultsReady");
+    return (status == CL_COMPLETE);
 }
 
 } // namespace opencl
diff --git a/lib/argon2-opencl/processingunit.cpp b/lib/argon2-opencl/processingunit.cpp
index 598ced7..8cfbd89 100644
--- a/lib/argon2-opencl/processingunit.cpp
+++ b/lib/argon2-opencl/processingunit.cpp
@@ -1,6 +1,7 @@
 #include "processingunit.h"
 
 #include <limits>
+#include <tuple>
 #ifndef NDEBUG
 #include <iostream>
 #endif
@@ -8,130 +9,75 @@
 namespace argon2 {
 namespace opencl {
 
-static bool isPowerOfTwo(std::uint32_t x)
-{
-    return (x & (x - 1)) == 0;
+enum class PinnedMemType {
+    INPUT,
+    OUTPUT
+};
+
+std::pair<cl::Buffer, uint8_t*> newPinnedMemBuffer(
+    KernelRunner::MiningContext & miningCtx,
+    size_t size, PinnedMemType type) {
+    auto context = miningCtx.programContext.getContext();
+    bool isInput = (type == PinnedMemType::INPUT);
+    cl::Buffer buffer = cl::Buffer(
+        context,
+        (isInput ? CL_MEM_READ_ONLY : CL_MEM_WRITE_ONLY) |
+        CL_MEM_ALLOC_HOST_PTR,
+        size);
+    auto mapPtr = (uint8_t*)miningCtx.queue.enqueueMapBuffer(
+        buffer,
+        true,
+        isInput ? CL_MAP_WRITE : CL_MAP_READ,
+        0, size);
+
+    uint8_t dummy = 0xFF;
+    miningCtx.queue.enqueueWriteBuffer(
+        buffer, true, size - 1, 1, &dummy);
+
+    return{ buffer, mapPtr };
 }
 
-ProcessingUnit::ProcessingUnit(
-        const ProgramContext *programContext, const Argon2Params *params,
-        const Device *device, std::size_t batchSize,
-        bool bySegment, bool precomputeRefs)
-    : programContext(programContext), params(params), device(device),
-      runner(programContext, params, device, batchSize, bySegment,
-             precomputeRefs),
-      bestLanesPerBlock(runner.getMinLanesPerBlock()),
-      bestJobsPerBlock(runner.getMinJobsPerBlock())
+ProcessingUnit::ProcessingUnit(const Argon2iMiningConfig & miningConfig,
+        KernelRunner::MiningContext & miningCtx) :
+    runner(miningConfig, miningCtx)
 {
-    /* pre-fill first blocks with pseudo-random data: */
-    for (std::size_t i = 0; i < batchSize; i++) {
-        setPassword(i, NULL, 0);
-    }
-
-    if (runner.getMaxLanesPerBlock() > runner.getMinLanesPerBlock()
-            && isPowerOfTwo(runner.getMaxLanesPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning lanes per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t lpb = 1; lpb <= runner.getMaxLanesPerBlock();
-             lpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(lpb, bestJobsPerBlock);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   OpenCL error on " << lpb
-                          << " lanes per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
+    std::tie(inBuffer, inPtr) = newPinnedMemBuffer(
+        miningCtx, miningConfig.mem->in, PinnedMemType::INPUT);
 
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << lpb << " lanes per block: "
-                      << time << " ms" << std::endl;
-#endif
+    std::tie(outBuffer, outPtr) = newPinnedMemBuffer(
+        miningCtx, miningConfig.mem->out, PinnedMemType::OUTPUT);
+}
 
-            if (time < bestTime) {
-                bestTime = time;
-                bestLanesPerBlock = lpb;
-            }
-        }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestLanesPerBlock
-                  << " lanes per block." << std::endl;
-#endif
-    }
+void ProcessingUnit::uploadInputDataAsync(const std::vector<std::string>& bases) {
+    KernelRunner::prepareInputData(runner, inPtr, bases);
 
-    /* Only tune jobs per block if we hit maximum lanes per block: */
-    if (bestLanesPerBlock == runner.getMaxLanesPerBlock()
-            && runner.getMaxJobsPerBlock() > runner.getMinJobsPerBlock()
-            && isPowerOfTwo(runner.getMaxJobsPerBlock())) {
-#ifndef NDEBUG
-        std::cerr << "[INFO] Tuning jobs per block..." << std::endl;
-#endif
-
-        float bestTime = std::numeric_limits<float>::infinity();
-        for (std::uint32_t jpb = 1; jpb <= runner.getMaxJobsPerBlock();
-             jpb *= 2)
-        {
-            float time;
-            try {
-                runner.run(bestLanesPerBlock, jpb);
-                time = runner.finish();
-            } catch(cl::Error &ex) {
-#ifndef NDEBUG
-                std::cerr << "[WARN]   OpenCL error on " << jpb
-                          << " jobs per block: " << ex.what() << std::endl;
-#endif
-                break;
-            }
+    asyncTp = std::chrono::high_resolution_clock::now();
+    runner.uploadToInputMemoryAsync(inPtr);
+}
 
-#ifndef NDEBUG
-            std::cerr << "[INFO]   " << jpb << " jobs per block: "
-                      << time << " ms" << std::endl;
-#endif
+void ProcessingUnit::fetchResultsAsync() {
+    runner.fetchOutputMemoryAsync(outPtr);
+    runner.insertEndEventAndFlush();
+}
 
-            if (time < bestTime) {
-                bestTime = time;
-                bestJobsPerBlock = jpb;
-            }
-        }
-#ifndef NDEBUG
-        std::cerr << "[INFO] Picked " << bestJobsPerBlock
-                  << " jobs per block." << std::endl;
-#endif
-    }
+void ProcessingUnit::runKernelAsync() {
+    runner.run(runner.minLanesPerBlock());
 }
 
-void ProcessingUnit::setPassword(std::size_t index, const void *pw,
-                                 std::size_t pwSize)
-{
-    void *memory = runner.mapInputMemory(index);
-    params->fillFirstBlocks(memory, pw, pwSize,
-                            programContext->getArgon2Type(),
-                            programContext->getArgon2Version());
-    runner.unmapInputMemory(memory);
+void ProcessingUnit::waitForResults() const {
+    runner.waitForResults();
 }
 
-void ProcessingUnit::getHash(std::size_t index, void *hash)
-{
-    void *memory = runner.mapOutputMemory(index);
-    params->finalize(hash, memory);
-    runner.unmapOutputMemory(memory);
+bool ProcessingUnit::resultsReady() const {
+    return runner.resultsReady();
 }
 
-void ProcessingUnit::beginProcessing()
-{
-    runner.run(bestLanesPerBlock, bestJobsPerBlock);
+uint8_t* ProcessingUnit::results() {
+    return outPtr;
 }
 
-void ProcessingUnit::endProcessing()
-{
-    runner.finish();
+void ProcessingUnit::configure() {
+    runner.configure();
 }
 
 } // namespace opencl
diff --git a/lib/argon2-opencl/programcontext.cpp b/lib/argon2-opencl/programcontext.cpp
index 67c2dc1..d1e1dc4 100644
--- a/lib/argon2-opencl/programcontext.cpp
+++ b/lib/argon2-opencl/programcontext.cpp
@@ -8,7 +8,7 @@ namespace opencl {
 ProgramContext::ProgramContext(
         const GlobalContext *globalContext,
         const std::vector<Device> &devices,
-        Type type, Version version, char* pathToKernel)
+        Type type, Version version, const char* pathToKernel)
     : globalContext(globalContext), devices(), type(type), version(version)
 {
     this->devices.reserve(devices.size());
